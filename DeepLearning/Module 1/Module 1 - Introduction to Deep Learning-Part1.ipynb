{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79c7728b",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning\n",
    "\n",
    "Deep Learning is a subset of machine learning that utilizes artificial neural networks with multiple layers to model complex patterns in data. It has achieved remarkable success in fields such as computer vision, natural language processing, and speech recognition.\n",
    "\n",
    "Key characteristics of deep learning include:\n",
    "\n",
    "- **Hierarchical Feature Learning:** Deep neural networks automatically learn representations from raw data through multiple layers of abstraction.\n",
    "- **Large-Scale Data Utilization:** Deep learning models excel with large datasets, leveraging vast amounts of data to improve performance.\n",
    "- **End-to-End Learning:** These models can learn directly from input to output, reducing the need for manual feature engineering.\n",
    "\n",
    "In this notebook, we will explore the fundamental concepts and practical applications of deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d60086",
   "metadata": {},
   "source": [
    "## Types of Deep Learning\n",
    "\n",
    "Deep learning techniques can be categorized based on the type of data and supervision involved in the learning process:\n",
    "\n",
    "### 1. Supervised Learning\n",
    "- **Definition:** The model is trained on labeled data, where each input has a corresponding output label.\n",
    "- **Examples:** Image classification, speech recognition, sentiment analysis.\n",
    "\n",
    "### 2. Unsupervised Learning\n",
    "- **Definition:** The model learns patterns from unlabeled data, discovering hidden structures without explicit output labels.\n",
    "- **Examples:** Clustering, dimensionality reduction, anomaly detection.\n",
    "\n",
    "### 3. Semi-supervised Learning\n",
    "- **Definition:** Combines a small amount of labeled data with a large amount of unlabeled data during training.\n",
    "- **Examples:** Text classification with limited labeled samples, image recognition with few annotated images.\n",
    "\n",
    "These approaches enable deep learning models to tackle a wide range of real-world problems, even when labeled data is scarce."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036bb14f",
   "metadata": {},
   "source": [
    "## Core Concepts of Deep Learning\n",
    "\n",
    "Deep learning is built upon several foundational concepts that enable neural networks to learn complex patterns from data:\n",
    "\n",
    "- **Artificial Neural Networks (ANNs):** Computational models inspired by the human brain, consisting of interconnected nodes (neurons) organized in layers.\n",
    "\n",
    "- **Layers:**\n",
    "    - **Input Layer:** Receives raw data.\n",
    "    - **Hidden Layers:** Perform feature extraction and transformation through learned weights.\n",
    "    - **Output Layer:** Produces the final prediction or classification.\n",
    "\n",
    "- **Activation Functions:** Non-linear functions (e.g., ReLU, sigmoid, tanh) applied to neuron outputs, allowing networks to model complex relationships.\n",
    "\n",
    "- **Forward Propagation:** The process of passing input data through the network to generate predictions.\n",
    "\n",
    "- **Loss Function:** Measures the difference between predicted and actual values (e.g., mean squared error, cross-entropy).\n",
    "\n",
    "- **Backpropagation:** An algorithm for computing gradients of the loss function with respect to network weights, enabling learning.\n",
    "\n",
    "- **Optimization Algorithms:** Methods like stochastic gradient descent (SGD), Adam, or RMSprop that update weights to minimize the loss.\n",
    "\n",
    "- **Overfitting and Regularization:** Techniques such as dropout, weight decay, and data augmentation help prevent the model from memorizing training data and improve generalization.\n",
    "\n",
    "- **Batch and Epoch:** Training is performed in batches (subsets of data), and one complete pass through the dataset is called an epoch.\n",
    "\n",
    "Understanding these core concepts is essential for designing, training, and evaluating deep learning models effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3625d3a",
   "metadata": {},
   "source": [
    "# Forward Propagation Example – Student Marks Prediction\n",
    "\n",
    "We want to predict a student's **marks** based on two features:  \n",
    "\n",
    "- `x1` = hours studied per day  \n",
    "- `x2` = number of days studied  \n",
    "\n",
    "The student scored **16 marks**.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Define weights and bias\n",
    "Assume the model gives importance like this:\n",
    "\n",
    "- `w1 = 2` (weight for hours/day)  \n",
    "- `w2 = 3` (weight for days)  \n",
    "- `b = 0` (bias term)\n",
    "\n",
    "Prediction rule:\n",
    "\n",
    "ŷ = (w1 * x1) + (w2 * x2) + b\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Step 2: Feed-forward example\n",
    "Suppose the student:  \n",
    "\n",
    "- studied **2 hours/day** (`x1 = 2`)  \n",
    "- studied for **4 days** (`x2 = 4`)  \n",
    "\n",
    "Compute step by step:\n",
    "\n",
    "- z = (2 * 2) + (3 * 4) + 0\n",
    "- z = 4 + 12 = 16\n",
    "- ŷ = 16\n",
    "\n",
    "\n",
    "✅ Predicted marks = **16**, matches actual marks.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 3: Neural network interpretation\n",
    "- **Inputs (`x1, x2`)** → student features  \n",
    "- **Weights (`w1, w2`)** → importance of each feature  \n",
    "- **Bias (`b`)** → extra adjustment  \n",
    "- **Computation (forward propagation)** → weighted sum  \n",
    "- **Output (`ŷ`)** → predicted marks  \n",
    "\n",
    "---\n",
    "\n",
    "⚡ Forward propagation = features → multiply by weights → add bias → output.\n",
    "\n",
    "---\n",
    "\n",
    "![Student Marks Prediction Neural Network](./Images/StudentMarks.png)\n",
    "\n",
    "*Figure: Simple neural network diagram for student marks prediction using forward propagation.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4a28e8",
   "metadata": {},
   "source": [
    "## Understanding the Loss Function in Neural Networks\n",
    "\n",
    "In the student marks prediction example, the **loss function** measures how well the model's predictions match the actual marks. It quantifies the difference between the predicted output (`ŷ`) and the true value (`y`).\n",
    "\n",
    "### Why is the Loss Function Important?\n",
    "- It provides feedback to the model about its performance.\n",
    "- During training, the model adjusts its weights and bias to minimize this loss, improving prediction accuracy.\n",
    "\n",
    "### Common Loss Functions\n",
    "- **Mean Squared Error (MSE):** Used for regression tasks. It calculates the average of the squared differences between predicted and actual values.\n",
    "- **Cross-Entropy Loss:** Used for classification tasks.\n",
    "\n",
    "### Example Calculation\n",
    "\n",
    "Suppose the actual marks are **16** and the model predicts **15**:\n",
    "\n",
    "### Mean Squared Error (MSE) Formula\n",
    "\n",
    "The general formula for Mean Squared Error is:\n",
    "\n",
    "$$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "Where:\n",
    "- $n$ = number of samples\n",
    "- $y_i$ = actual value for sample $i$\n",
    "- $\\hat{y}_i$ = predicted value for sample $i$\n",
    "\n",
    "---\n",
    "\n",
    "### Example Calculations\n",
    "\n",
    "**For one sample with actual marks = 16, predicted marks = 15:**\n",
    "\n",
    "$$\\text{MSE} = (16 - 15)^2 = 1$$\n",
    "\n",
    "**If the prediction is perfect (predicted = 16):**\n",
    "\n",
    "$$\\text{MSE} = (16 - 16)^2 = 0$$\n",
    "\n",
    "✅ **Key Insight:** Perfect predictions give zero loss!\n",
    "\n",
    "### In the Neural Network\n",
    "\n",
    "- **Forward propagation** computes the prediction (`ŷ`).\n",
    "- The **loss function** (e.g., MSE) measures the error.\n",
    "- **Backpropagation** uses this error to update weights and bias, reducing the loss in future predictions.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary:**  \n",
    "The loss function is a critical component that guides the learning process in neural networks by quantifying prediction errors and enabling the model to improve through training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34752a77",
   "metadata": {},
   "source": [
    "## Understanding Backpropagation in Neural Networks\n",
    "\n",
    "Backpropagation is the key algorithm that enables neural networks to learn from data. It is used to update the model's weights and bias based on the error (loss) between the predicted output and the actual value.\n",
    "\n",
    "### How Backpropagation Works\n",
    "\n",
    "1. **Forward Propagation:**  \n",
    "    - Compute the output (`ŷ`) using the current weights and bias.\n",
    "    - Calculate the loss (e.g., mean squared error).\n",
    "\n",
    "2. **Backward Propagation:**  \n",
    "    - Compute the gradient of the loss with respect to each parameter (weights and bias).\n",
    "    - These gradients indicate how much each parameter contributed to the error.\n",
    "\n",
    "3. **Parameter Update:**  \n",
    "    - Adjust the weights and bias in the direction that reduces the loss, typically using gradient descent.\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Backpropagation for Student Marks Prediction\n",
    "\n",
    "Suppose we have:\n",
    "- Inputs: `x1 = 2` (hours/day), `x2 = 4` (days)\n",
    "- Weights: `w1 = 2`, `w2 = 3`\n",
    "- Bias: `b = 0`\n",
    "- Actual marks: `y = 16`\n",
    "- Predicted marks: `ŷ = (w1 * x1) + (w2 * x2) + b = 16`\n",
    "\n",
    "If the prediction is perfect, the loss is zero, and no update is needed.  \n",
    "But if the prediction is not perfect (e.g., `ŷ = 15`), we need to update the weights and bias.\n",
    "\n",
    "#### Steps:\n",
    "\n",
    "1. **Compute the loss:**  \n",
    "   $$\\text{Loss} = (y - \\hat{y})^2$$\n",
    "\n",
    "2. **Compute gradients:**  \n",
    "   - For each parameter (e.g., `w1`), calculate how the loss changes if we change that parameter:\n",
    "     $$\\frac{\\partial \\text{Loss}}{\\partial w_1} = 2 \\cdot (y - \\hat{y}) \\cdot (-x_1)$$\n",
    "   - Similarly for `w2` and `b`.\n",
    "\n",
    "3. **Update parameters:**  \n",
    "   - Using a learning rate (`lr`), update each parameter:\n",
    "     $$w_1 = w_1 - lr \\cdot \\frac{\\partial \\text{Loss}}{\\partial w_1}$$\n",
    "   - Repeat for `w2` and `b`.\n",
    "---\n",
    "\n",
    "#### Steps with Actual Values:\n",
    "\n",
    "**Given:**\n",
    "- Actual marks (y) = 16\n",
    "- Predicted marks (ŷ) = 15\n",
    "- Input values: x₁ = 2 (hours), x₂ = 4 (days)\n",
    "- Current weights: w₁ = 1.5, w₂ = 2.5 (example values that give ŷ = 15)\n",
    "\n",
    "1. **Compute the loss:**  \n",
    "   $$\\text{Loss} = (y - \\hat{y})^2 = (16 - 15)^2 = 1$$\n",
    "\n",
    "2. **Compute gradients:**  \n",
    "   - For w₁:\n",
    "     $$\\frac{\\partial \\text{Loss}}{\\partial w_1} = 2 \\cdot (y - \\hat{y}) \\cdot (-x_1) = 2 \\cdot (16 - 15) \\cdot (-2) = 2 \\cdot 1 \\cdot (-2) = -4$$\n",
    "   \n",
    "   - For w₂:\n",
    "     $$\\frac{\\partial \\text{Loss}}{\\partial w_2} = 2 \\cdot (y - \\hat{y}) \\cdot (-x_2) = 2 \\cdot (16 - 15) \\cdot (-4) = 2 \\cdot 1 \\cdot (-4) = -8$$\n",
    "\n",
    "3. **Update parameters (assuming learning rate = 0.1):**  \n",
    "   - Update w₁:\n",
    "     $$w_1 = w_1 - lr \\cdot \\frac{\\partial \\text{Loss}}{\\partial w_1} = w_1 - 0.1 \\cdot (-4) = w_1 + 0.4$$\n",
    "   \n",
    "   - Update w₂:\n",
    "     $$w_2 = w_2 - lr \\cdot \\frac{\\partial \\text{Loss}}{\\partial w_2} = w_2 - 0.1 \\cdot (-8) = w_2 + 0.8$$\n",
    "     \n",
    "4. **Revised prediction with new weights:**\n",
    "   $$\\hat{y}^{new} = w_1^{new} \\cdot x_1 + w_2^{new} \\cdot x_2 = 1.9 \\cdot 2 + 3.3 \\cdot 4 = 3.8 + 13.2 = 17$$\n",
    "\n",
    "\n",
    "**Result:** Both weights increase, which will increase the prediction (MAY BE closer to 16).\n",
    "---\n",
    "\n",
    "**Summary:**  \n",
    "Backpropagation calculates how to adjust each weight and bias to reduce the error. By repeating this process over many examples, the neural network learns to make better predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c49d10",
   "metadata": {},
   "source": [
    "## Linearity vs. Non-Linearity in Neural Networks\n",
    "\n",
    "### What is Linearity?\n",
    "\n",
    "In neural networks, a linear model computes outputs as a weighted sum of inputs plus a bias:\n",
    "$$\n",
    "y = w_1 x_1 + w_2 x_2 + \\ldots + w_n x_n + b\n",
    "$$\n",
    "\n",
    "**Example:**  \n",
    "A single-layer perceptron without activation functions is a linear model. It can only learn to separate data that is linearly separable (e.g., a straight line in 2D).\n",
    "\n",
    "---\n",
    "\n",
    "### What is Non-Linearity?\n",
    "\n",
    "A function is **non-linear** if it does not satisfy the properties above. Non-linear functions can model complex relationships and boundaries.\n",
    "\n",
    "In neural networks, **non-linearity** is introduced using activation functions like ReLU, sigmoid, or tanh. These functions allow the network to learn and represent complex, non-linear patterns in data.\n",
    "\n",
    "**Example:**  \n",
    "The XOR problem cannot be solved by a linear model, but a neural network with non-linear activation functions can learn the XOR relationship.\n",
    "\n",
    "---\n",
    "\n",
    "### Why is Non-Linearity Important?\n",
    "\n",
    "- **Expressive Power:** Non-linear activation functions enable neural networks to approximate any function, not just straight lines or planes.\n",
    "- **Solving Complex Problems:** Many real-world problems (like image recognition, language understanding) require modeling non-linear relationships.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary:**  \n",
    "- **Linear models** are limited to simple, straight-line relationships.\n",
    "- **Non-linear models** (with activation functions) can capture complex patterns, making deep learning powerful and flexible.\n",
    "\n",
    "![Linearity vs Non-Linearity in Neural Networks](./Images/LinearityVsNonLinearity.png)\n",
    "\n",
    "*Figure: Linear models can only separate data with straight lines, while non-linear models (with activation functions) can capture complex boundaries.*\n",
    "\n",
    "| Study Hours | Linear Prediction | Reality (Non-Linear) | Difference |\n",
    "|-------------|-------------------|---------------------|------------|\n",
    "| 2 hours     | 40 marks          | 35 marks            | -5 (slow start) |\n",
    "| 4 hours     | 60 marks          | 65 marks            | +5 (sweet spot) |\n",
    "| 6 hours     | 80 marks          | 85 marks            | +5 (peak efficiency) |\n",
    "| 8 hours     | 100 marks         | 90 marks            | -10 (diminishing returns) |\n",
    "| 10 hours    | 120 marks         | 85 marks            | -35 (burnout effect) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1686d5ac",
   "metadata": {},
   "source": [
    "## Activation Functions in Neural Networks\n",
    "\n",
    "Activation functions introduce non-linearity into neural networks, enabling them to learn complex patterns and relationships in data. Without activation functions, a neural network would behave like a simple linear model, regardless of its depth.\n",
    "\n",
    "### Why Are Activation Functions Important?\n",
    "- **Non-linearity:** Allow networks to approximate complex, non-linear functions.\n",
    "- **Decision Boundaries:** Enable the network to learn intricate decision boundaries for classification and regression tasks.\n",
    "- **Gradient Flow:** Affect how gradients are propagated during backpropagation, influencing learning efficiency.\n",
    "\n",
    "### Common Activation Functions\n",
    "\n",
    "| Name      | Formula                                  | Range         | Typical Use                |\n",
    "|-----------|------------------------------------------|---------------|----------------------------|\n",
    "| Sigmoid   | $\\sigma(x) = \\frac{1}{1 + e^{-x}}$       | (0, 1)        | Output layer (binary)      |\n",
    "| Tanh      | $\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$ | (-1, 1)   | Hidden layers              |\n",
    "| ReLU      | $f(x) = \\max(0, x)$                      | [0, ∞)        | Hidden layers (default)    |\n",
    "| Leaky ReLU| $f(x) = \\max(\\alpha x, x)$, $\\alpha \\ll 1$ | (-∞, ∞)    | Hidden layers (avoids dying ReLU) |\n",
    "| Softmax   | $f(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}$ | (0, 1), sum=1 | Output layer (multi-class) |\n",
    "\n",
    "---\n",
    "\n",
    "### Visual Comparison\n",
    "\n",
    "- **Sigmoid:** S-shaped curve, squashes input to (0, 1). Good for probabilities, but can cause vanishing gradients.\n",
    "- **Tanh:** Similar to sigmoid but outputs between -1 and 1. Zero-centered, often preferred over sigmoid in hidden layers.\n",
    "- **ReLU (Rectified Linear Unit):** Outputs zero for negative inputs, linear for positive. Fast and effective, but can suffer from \"dying ReLU\" (neurons stuck at zero).\n",
    "- **Leaky ReLU:** Like ReLU, but allows a small, non-zero gradient when input is negative.\n",
    "- **Softmax:** Converts a vector of values into probabilities that sum to 1. Used for multi-class classification.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "![Activation Functions Comparison](./Images/ActivationFunctions.png)\n",
    "**Summary:**  \n",
    "Activation functions are essential for deep learning. They enable neural networks to model complex, non-linear relationships and make deep architectures powerful and expressive. The choice of activation function can significantly impact model performance and training dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b70d0db",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Example Table:**\n",
    "| Input ($x$) | Sigmoid($x$) | Tanh($x$) | ReLU($x$) | Leaky ReLU($x$) ($\\alpha=0.01$) | Softmax($x$)\\* |\n",
    "|-------------|--------------|-----------|-----------|-------------------|--------------|\n",
    "| -200        | 0.00         | -1.00     | 0         | -2.00             | ~0.00        |\n",
    "| -100        | 0.00         | -1.00     | 0         | -1.00             | ~0.00        |\n",
    "| -10         | 0.00         | -1.00     | 0         | -0.10             | ~0.00        |\n",
    "| -5          | 0.01         | -1.00     | 0         | -0.05             | ~0.00        |\n",
    "| -2          | 0.12         | -0.96     | 0         | -0.02             | ~0.00        |\n",
    "| 0           | 0.50         | 0         | 0         | 0                 | ~0.09        |\n",
    "| 2           | 0.88         | 0.96      | 2         | 2                 | ~0.67        |\n",
    "| 5           | 0.99         | 1.00      | 5         | 5                 | ~0.99        |\n",
    "| 10          | 1.00         | 1.00      | 10        | 10                | ~1.00        |\n",
    "| 100         | 1.00         | 1.00      | 100       | 100               | ~1.00        |\n",
    "| 200         | 1.00         | 1.00      | 200       | 200               | ~1.00        |\n",
    "\n",
    "\\*Softmax($x$) shown as the probability for $x$ in the set [$x$, 0], i.e., $\\text{softmax}(x) = \\frac{e^{x}}{e^{x} + e^{0}}$. For large negative $x$, softmax approaches 0; for large positive $x$, it approaches 1.\n",
    "\n",
    "These activation functions are essential for enabling deep neural networks to model complex, non-linear relationships in data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d0c7f5",
   "metadata": {},
   "source": [
    "### Guidelines for Selecting an Activation Function\n",
    "\n",
    "Choosing the right activation function is crucial for neural network performance. Here are some practical guidelines:\n",
    "\n",
    "- **Hidden Layers (Default):**\n",
    "    - **ReLU (Rectified Linear Unit):**  \n",
    "        - Most commonly used for hidden layers in deep networks.\n",
    "        - Pros: Fast, reduces vanishing gradient problem, simple to compute.\n",
    "        - Cons: Can suffer from \"dying ReLU\" (neurons stuck at zero).\n",
    "    - **Leaky ReLU / Parametric ReLU:**  \n",
    "        - Use if you observe many dead neurons with standard ReLU.\n",
    "        - Allows a small gradient when input is negative.\n",
    "\n",
    "- **Output Layer:**\n",
    "    - **Sigmoid:**  \n",
    "        - Use for binary classification (output between 0 and 1).\n",
    "    - **Softmax:**  \n",
    "        - Use for multi-class classification (outputs probabilities that sum to 1).\n",
    "    - **Linear:**  \n",
    "        - Use for regression tasks (predicting continuous values).\n",
    "\n",
    "- **Other Considerations:**\n",
    "    - **Tanh:**  \n",
    "        - Sometimes preferred over sigmoid in hidden layers (zero-centered output).\n",
    "        - Can still suffer from vanishing gradients in deep networks.\n",
    "    - **Swish, GELU, ELU:**  \n",
    "        - Advanced activations that may offer improvements in some architectures.\n",
    "\n",
    "- **Empirical Testing:**  \n",
    "    - Try different activation functions and compare validation performance.\n",
    "    - Monitor for issues like vanishing/exploding gradients or dead neurons.\n",
    "\n",
    "**Summary Table:**\n",
    "\n",
    "| Task Type                | Recommended Activation Function |\n",
    "|--------------------------|-------------------------------|\n",
    "| Hidden layers (default)  | ReLU, Leaky ReLU              |\n",
    "| Binary classification    | Sigmoid (output layer)        |\n",
    "| Multi-class classification | Softmax (output layer)      |\n",
    "| Regression               | Linear (output layer)         |\n",
    "\n",
    "> **Tip:** Start with ReLU for hidden layers and the appropriate output activation for your task. Experiment if you encounter training issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ad088c",
   "metadata": {},
   "source": [
    "## Batch vs. Epoch in Neural Network Training\n",
    "\n",
    "**Batch** and **epoch** are key terms in the training process of neural networks:\n",
    "\n",
    "### Batch\n",
    "- A **batch** is a subset of the training dataset used to compute one forward and backward pass.\n",
    "- Instead of updating weights after every single sample (stochastic) or after the entire dataset (full batch), data is split into smaller batches.\n",
    "- **Batch size** is the number of samples processed before the model's parameters are updated.\n",
    "\n",
    "### Epoch\n",
    "- An **epoch** is one complete pass through the entire training dataset.\n",
    "- During an epoch, the model sees every sample in the training set once (typically, in shuffled order).\n",
    "- Training usually involves multiple epochs.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Differences\n",
    "\n",
    "| Term   | Definition | Example (Dataset of 1000 samples, batch size = 100) |\n",
    "|--------|------------|-----------------------------------------------------|\n",
    "| Batch  | Subset of data used for one update | 10 batches per epoch (100 samples each) |\n",
    "| Epoch  | One full pass through all data     | 1 epoch = 10 batches (all 1000 samples seen once) |\n",
    "\n",
    "- **Batch size** controls how many samples are processed before updating the model.\n",
    "- **Epoch** counts how many times the model has seen the entire dataset.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary:**  \n",
    "- **Batch:** Number of samples processed before updating weights.\n",
    "- **Epoch:** One full pass through the entire training data.\n",
    "- Multiple batches make up one epoch; multiple epochs are used to train the model for better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54561a2a",
   "metadata": {},
   "source": [
    "### Example: Understanding Batch and Epoch in Neural Network Training\n",
    "\n",
    "Suppose you have a dataset with **12 samples**:\n",
    "\n",
    "| Sample | Data |\n",
    "|--------|------|\n",
    "| 1      | ...  |\n",
    "| 2      | ...  |\n",
    "| 3      | ...  |\n",
    "| ...    | ...  |\n",
    "| 12     | ...  |\n",
    "\n",
    "Let's say you set **batch size = 4** and train for **3 epochs**.\n",
    "\n",
    "### How Training Proceeds\n",
    "\n",
    "- **Batch:** Each batch contains 4 samples.\n",
    "- **Epoch:** One epoch means the model sees all 12 samples once.\n",
    "\n",
    "#### Breakdown\n",
    "\n",
    "- **Number of batches per epoch:**  \n",
    "    $12 \\text{ samples} \\div 4 \\text{ (batch size)} = 3 \\text{ batches per epoch}$\n",
    "\n",
    "- **Epoch 1:**  \n",
    "    - Batch 1: samples 1–4  \n",
    "    - Batch 2: samples 5–8  \n",
    "    - Batch 3: samples 9–12\n",
    "\n",
    "- **Epoch 2:**  \n",
    "    - Batch 1: samples 1–4 (possibly shuffled)  \n",
    "    - Batch 2: samples 5–8  \n",
    "    - Batch 3: samples 9–12\n",
    "\n",
    "- **Epoch 3:**  \n",
    "    - Repeat as above\n",
    "\n",
    "### Visualization\n",
    "\n",
    "| Epoch | Batch 1      | Batch 2      | Batch 3      |\n",
    "|-------|--------------|--------------|--------------|\n",
    "| 1     | 1, 2, 3, 4   | 5, 6, 7, 8   | 9, 10, 11, 12|\n",
    "| 2     | 1, 2, 3, 4   | 5, 6, 7, 8   | 9, 10, 11, 12|\n",
    "| 3     | 1, 2, 3, 4   | 5, 6, 7, 8   | 9, 10, 11, 12|\n",
    "\n",
    "- **After each batch:** Model updates its weights.\n",
    "- **After each epoch:** Model has seen all data once.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary:**  \n",
    "- **Batch:** Subset of data used for one update (e.g., 4 samples).\n",
    "- **Epoch:** One full pass through the entire dataset (e.g., all 12 samples).  \n",
    "- Training for multiple epochs means the model sees the data multiple times, improving learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6fa07e",
   "metadata": {},
   "source": [
    "In neural network training:\n",
    "\n",
    "- **One batch** involves **one forward propagation** and **one backward propagation** (i.e., one update step) for that batch of samples.\n",
    "- **One epoch** consists of as many forward and backward propagations as there are batches in the dataset.\n",
    "\n",
    "**Example:**  \n",
    "If your dataset has 1000 samples and batch size is 100:\n",
    "- Number of batches per epoch = 1000 / 100 = **10**\n",
    "- So, **one epoch = 10 forward + 10 backward propagations** (one per batch)\n",
    "\n",
    "**Summary Table:**\n",
    "\n",
    "| Term   | Forward Propagations | Backward Propagations |\n",
    "|--------|----------------------|-----------------------|\n",
    "| 1 Batch | 1                    | 1                     |\n",
    "| 1 Epoch | Number of batches    | Number of batches     |\n",
    "\n",
    "Thus, **one epoch = (number of batches) × (one forward + one backward propagation)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5180bc",
   "metadata": {},
   "source": [
    "When using **batch training** in neural networks, all 4 samples in the batch are processed **together** in a single forward and backward pass:\n",
    "\n",
    "- **Forward pass:**  \n",
    "    The model computes predictions for all 4 samples at once (using vectorized operations).  \n",
    "    Example: If `X` is shape `(4, 2)`, the network computes outputs for all 4 rows in one go.\n",
    "\n",
    "- **Loss calculation:**  \n",
    "    The loss is computed for each sample, then averaged (or summed) across the batch.\n",
    "\n",
    "- **Backward pass:**  \n",
    "    Gradients are calculated for all 4 samples together (again, using vectorized math), and the average gradient is used to update the weights.\n",
    "\n",
    "**Summary:**  \n",
    "All samples in the batch are processed in parallel using matrix operations. The model updates its parameters once per batch, based on the average effect of all samples in that batch. This is much faster and more efficient than processing each sample one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe51d2d7",
   "metadata": {},
   "source": [
    "## Determining the Size of Input and Output Layers in Neural Networks\n",
    "\n",
    "**Input Layer:**\n",
    "- The number of neurons in the input layer equals the number of features in your data.\n",
    "    - **Example:** For an image of size 28×28 pixels, the input layer has 784 neurons (one per pixel).\n",
    "    - For tabular data with 2 features (like `x1`, `x2` in the XOR example), the input layer has 2 neurons.\n",
    "\n",
    "**Output Layer:**\n",
    "- The number of neurons in the output layer depends on the prediction task:\n",
    "    - **Regression:** 1 neuron (predicts a continuous value).\n",
    "    - **Binary Classification:** 1 neuron (outputs probability or class label).\n",
    "    - **Multi-class Classification:** Number of neurons equals the number of classes (e.g., 10 for digit classification 0–9).\n",
    "\n",
    "**Summary Table:**\n",
    "\n",
    "| Task Type               | Input Layer Size         | Output Layer Size         |\n",
    "|-------------------------|-------------------------|--------------------------|\n",
    "| Regression (1 target)   | # features              | 1                        |\n",
    "| Binary classification   | # features              | 1                        |\n",
    "| Multi-class classification | # features           | # classes                |\n",
    "| Image (28×28 pixels)    | 784                     | Depends on task          |\n",
    "\n",
    "**Key Point:**  \n",
    "- **Input layer:** Matches the number of input features.\n",
    "- **Output layer:** Matches the number of prediction targets or classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744c9723",
   "metadata": {},
   "source": [
    "## Optimization Approaches in Neural Networks\n",
    "\n",
    "**Optimization** in neural networks refers to the process of adjusting model parameters (weights and biases) to minimize the loss function and improve performance. The choice of optimization algorithm can significantly impact training speed, convergence, and final accuracy.\n",
    "\n",
    "### Common Optimization Algorithms\n",
    "\n",
    "- **Stochastic Gradient Descent (SGD):**\n",
    "    - Updates parameters using the gradient of the loss with respect to a random subset (batch) of data.\n",
    "    - Simple and widely used, but can be slow to converge and sensitive to learning rate.\n",
    "\n",
    "- **Momentum:**\n",
    "    - Enhances SGD by adding a fraction of the previous update to the current update.\n",
    "    - Helps accelerate convergence and escape local minima.\n",
    "\n",
    "- **RMSprop:**\n",
    "    - Adapts the learning rate for each parameter by dividing by a running average of recent gradients' magnitudes.\n",
    "    - Works well for recurrent neural networks and non-stationary objectives.\n",
    "\n",
    "- **Adam (Adaptive Moment Estimation):**\n",
    "    - Combines ideas from Momentum and RMSprop.\n",
    "    - Maintains running averages of both gradients and their squares.\n",
    "    - Generally provides fast convergence and is robust to hyperparameter choices.\n",
    "\n",
    "### Comparison Table\n",
    "\n",
    "| Optimizer | Pros | Cons | Typical Use |\n",
    "|-----------|------|------|-------------|\n",
    "| SGD       | Simple, memory efficient | Slow, sensitive to learning rate | General use, baseline |\n",
    "| Momentum  | Faster convergence | Adds extra parameter (momentum) | Deep networks, escaping local minima |\n",
    "| RMSprop   | Adapts learning rate | May not generalize as well | RNNs, non-stationary problems |\n",
    "| Adam      | Fast, adaptive, robust | Slightly more memory | Most deep learning tasks |\n",
    "\n",
    "### Applicability\n",
    "\n",
    "- **SGD/Momentum:** Good for large datasets and when you want more control over learning dynamics.\n",
    "- **RMSprop:** Often used for recurrent networks and time-series data.\n",
    "- **Adam:** Default choice for most deep learning tasks due to its speed and reliability.\n",
    "\n",
    "**Summary:**  \n",
    "Optimization algorithms are crucial for effective neural network training. Adam is often a safe and efficient default, but understanding and experimenting with different optimizers can lead to better results for specific problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a515ce",
   "metadata": {},
   "source": [
    "## Setting Up a Virtual Environment with Python 3.12 for Deep Learning Experiments\n",
    "\n",
    "To ensure a clean and manageable workspace for your deep learning projects, it is recommended to use a virtual environment. This allows you to isolate dependencies and avoid conflicts with other Python projects.\n",
    "\n",
    "### Steps to Install Python 3.12\n",
    "\n",
    "1. **Download Python 3.12:**\n",
    "    - Visit the [official Python downloads page](https://www.python.org/downloads/) and select Python 3.12 for your operating system.\n",
    "\n",
    "2. **Install Python 3.12:**\n",
    "    - **Windows:** Run the installer and follow the prompts. Make sure to check \"Add Python to PATH\" during installation.\n",
    "    - **macOS:** Download the macOS installer and run it, or use Homebrew:\n",
    "      ```bash\n",
    "      brew install python@3.12\n",
    "      ```\n",
    "    - **Linux (Ubuntu/Debian):**\n",
    "      ```bash\n",
    "      sudo apt update\n",
    "      sudo apt install python3.12 python3.12-venv python3.12-dev\n",
    "      ```\n",
    "\n",
    "3. **If `python3.12` is not recognized:**\n",
    "    - Check the installation path:\n",
    "      ```bash\n",
    "      which python3.12\n",
    "      ```\n",
    "    - If the path is not in your `PATH` environment variable, add it (replace `/opt/homebrew/bin` with your actual path if different):\n",
    "      ```bash\n",
    "      export PATH=\"/opt/homebrew/bin:$PATH\"\n",
    "      ```\n",
    "    - On Windows, ensure the Python installation directory is added to your system's PATH environment variable.(replace `C:\\Python312;C:\\Python312\\Scripts` with your actual path if different):\n",
    "      ```bash\n",
    "        setx PATH \"%PATH%;C:\\Python312;C:\\Python312\\Scripts\" /M\n",
    "      ```\n",
    "\n",
    "4. **Verify the installation:**\n",
    "    ```bash\n",
    "    python3.12 --version\n",
    "    ```\n",
    "\n",
    "### Steps to Create a Virtual Environment with Python 3.12\n",
    "\n",
    "1. **Install `virtualenv` (if not already installed):**\n",
    "    ```bash\n",
    "    pip install virtualenv\n",
    "    ```\n",
    "\n",
    "2. **Create a new virtual environment using Python 3.12:**\n",
    "    ```bash\n",
    "    virtualenv -p python3.12 module1_dl_env\n",
    "    ```\n",
    "\n",
    "3. **Activate the virtual environment:**\n",
    "    - On Windows:\n",
    "      ```bash\n",
    "      module1_dl_env\\Scripts\\activate\n",
    "      ```\n",
    "    - On macOS/Linux:\n",
    "      ```bash\n",
    "      source module1_dl_env/bin/activate\n",
    "      ```\n",
    "\n",
    "4. **Install required packages (e.g., TensorFlow, Keras, scikit-learn):**\n",
    "    ```bash\n",
    "    pip install tensorflow==2.17.0 scikit-learn==1.5.0 matplotlib==3.9.0 torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0\n",
    "    ```\n",
    "    \n",
    "    Note: Keras is now included with TensorFlow 2.17.0, so no separate installation is needed.\n",
    "\n",
    "5. **Deactivate the environment when done with all the below experiments:**\n",
    "    ```bash\n",
    "    deactivate\n",
    "    ```\n",
    "\n",
    "Using a virtual environment with Python 3.12 helps you manage dependencies and ensures reproducibility for your deep learning experiments.\n",
    "\n",
    "### Steps to Install Git and Download a Repository\n",
    "\n",
    "1. **Install Git:**\n",
    "  - **Windows:** Download and run the installer from [git-scm.com](https://git-scm.com/download/win).\n",
    "  - **macOS:**  \n",
    "    ```bash\n",
    "    brew install git\n",
    "    ```\n",
    "  - **Linux (Ubuntu/Debian):**  \n",
    "    ```bash\n",
    "    sudo apt update\n",
    "    sudo apt install git\n",
    "    ```\n",
    "\n",
    "2. **Verify Git Installation:**\n",
    "  ```bash\n",
    "  git --version\n",
    "  ```\n",
    "\n",
    "3. **Clone the Repository (including all subfolders):**\n",
    "  ```bash\n",
    "  git clone https://github.com/<reponame>/DeepLearningCourse.git\n",
    "  ```\n",
    "\n",
    "  This command will download the entire repository and its subfolders into a local directory named `DeepLearningCourse`.\n",
    "\n",
    "4. **Navigate into the Downloaded Repository:**\n",
    "  ```bash\n",
    "  cd DeepLearningCourse\n",
    "  ```\n",
    "\n",
    "You can now access all files and subfolders from the repository on your local machine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75342a83",
   "metadata": {},
   "source": [
    "## Using Python Notebooks in VS Code with a Custom Virtual Environment\n",
    "\n",
    "To work with Jupyter Notebooks in Visual Studio Code and use your custom Python 3.12 virtual environment, follow these steps:\n",
    "\n",
    "### 1. Install VS Code and Extensions\n",
    "\n",
    "- Download and install [Visual Studio Code](https://code.visualstudio.com/).\n",
    "- Open VS Code and go to the Extensions view (`Ctrl+Shift+X`).\n",
    "- Search for and install the **Python** extension (by Microsoft).\n",
    "- Search for and install the **Jupyter** extension (by Microsoft).\n",
    "\n",
    "### 2. Open Your Project Folder\n",
    "\n",
    "- Open the folder containing your Jupyter Notebook (`.ipynb`) and virtual environment.\n",
    "\n",
    "### 3. Select the Python Interpreter\n",
    "\n",
    "- Press `Ctrl+Shift+P` to open the Command Palette.\n",
    "- Type `Python: Select Interpreter` and select it.\n",
    "- Choose the interpreter from your virtual environment (e.g., `./module1_dl_env/bin/python` or `.\\module1_dl_env\\Scripts\\python.exe`).\n",
    "\n",
    "### 4. Enable and Use Jupyter Notebooks\n",
    "\n",
    "- Open or create a `.ipynb` notebook file in VS Code.\n",
    "- At the top right of the notebook, click on the **kernel selector** (shows the current Python version).\n",
    "- Select your virtual environment as the Jupyter kernel.\n",
    "\n",
    "### 5. Install Jupyter in the Virtual Environment (if needed)\n",
    "\n",
    "If you haven't already installed Jupyter in your virtual environment, activate the environment and run:\n",
    "\n",
    "```bash\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "### 6. Start Coding\n",
    "\n",
    "- You can now run notebook cells using your selected virtual environment and installed packages.\n",
    "\n",
    "---\n",
    "\n",
    "**Tip:**  \n",
    "If your virtual environment does not appear in the kernel list, restart VS Code after activating the environment and installing the Jupyter extension. Make sure the environment is properly set up and recognized by VS Code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34d8fda",
   "metadata": {},
   "source": [
    "## Popular Deep Learning Frameworks\n",
    "\n",
    "Several open-source frameworks make it easier to build, train, and deploy deep learning models. Here are some of the most widely used:\n",
    "\n",
    "- **TensorFlow:** Developed by Google, TensorFlow is a flexible and scalable framework for building deep learning models. It supports both high-level APIs (like Keras) and low-level operations for advanced research.\n",
    "\n",
    "- **PyTorch:** Developed by Facebook's AI Research lab, PyTorch is known for its dynamic computation graph and intuitive interface, making it popular in both academia and industry.\n",
    "\n",
    "- **Keras:** Initially a standalone high-level API, Keras is now integrated with TensorFlow. It provides a user-friendly interface for quickly prototyping and building neural networks.\n",
    "\n",
    "- **MXNet:** Backed by Apache, MXNet is designed for efficiency and scalability, supporting both symbolic and imperative programming.\n",
    "\n",
    "- **JAX:** Developed by Google, JAX enables high-performance machine learning research with automatic differentiation and GPU/TPU acceleration.\n",
    "\n",
    "These frameworks provide tools and abstractions to accelerate deep learning research and production deployment. Choosing the right framework depends on your project requirements, familiarity, and ecosystem support."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "module1_dl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
