{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experiment 1: Neural Word Embeddings\n",
        "\n",
        "**Course:** Introduction to Deep Learning | **Module:** Natural Language Processing\n",
        "\n",
        "---\n",
        "\n",
        "## Objective\n",
        "\n",
        "Implement neural word embedding models (Skip-gram and CBOW) using PyTorch to learn distributed representations of words from text corpora.\n",
        "\n",
        "## Learning Outcomes\n",
        "\n",
        "By the end of this experiment, you will:\n",
        "\n",
        "1. Understand the theory behind word embeddings and distributional semantics\n",
        "2. Implement Skip-gram and CBOW neural network architectures\n",
        "3. Generate training data using sliding window approaches\n",
        "4. Train word embedding models with negative sampling\n",
        "5. Analyze and visualize learned word representations\n",
        "\n",
        "## Background & Theory\n",
        "\n",
        "**Word Embeddings** are dense vector representations of words that capture semantic and syntactic relationships. Unlike one-hot encodings, embeddings place similar words close together in vector space.\n",
        "\n",
        "**Key Concepts:**\n",
        "\n",
        "- **Distributional Hypothesis:** Words in similar contexts have similar meanings\n",
        "- **Skip-gram:** Predicts context words given a center word\n",
        "- **CBOW:** Predicts center word given context words\n",
        "- **Negative Sampling:** Efficient training technique avoiding full softmax\n",
        "- **Word Similarity:** Measured using cosine similarity in embedding space\n",
        "\n",
        "**Mathematical Foundation:**\n",
        "\n",
        "- Skip-gram objective: maximize P(w_context | w_center)\n",
        "- CBOW objective: maximize P(w_center | w_context)\n",
        "- Negative sampling: σ(v_w^T v_c) + Σ σ(-v_n^T v_c)\n",
        "- Where σ is sigmoid, v_w word vectors, v_c context vectors\n",
        "\n",
        "**Applications:**\n",
        "\n",
        "- Machine translation and cross-lingual tasks\n",
        "- Information retrieval and document similarity\n",
        "- Sentiment analysis and text classification\n",
        "- Named entity recognition and part-of-speech tagging\n",
        "\n",
        "\n",
        "### Skip-gram vs. CBOW: Intuitive Example\n",
        "\n",
        "Suppose we have the sentence:  \n",
        "**\"Oil exploration involves seismic surveys and geological analysis.\"**\n",
        "\n",
        "#### Skip-gram\n",
        "\n",
        "- **Goal:** Given a center word, predict its surrounding context words.\n",
        "- **Example:**  \n",
        "    - Center word: **\"seismic\"**  \n",
        "    - Training pairs generated:  \n",
        "        - (\"seismic\", \"involves\")  \n",
        "        - (\"seismic\", \"surveys\")  \n",
        "        - (\"seismic\", \"and\")  \n",
        "\n",
        "    Here, the context window of size 2 means we look at up to 2 words before and after the center word (\"seismic\"). For each center word, the Skip-gram model tries to predict each of its context words within this window. In this example, \"involves\" is before \"seismic\", and \"surveys\" and \"and\" are after it, so these become the context words for the center word \"seismic\".    - (\"seismic\", \"involves\")  \n",
        "        - (\"seismic\", \"surveys\")  \n",
        "        - (\"seismic\", \"and\")\n",
        "\n",
        "#### CBOW (Continuous Bag of Words)\n",
        "\n",
        "- **Goal:** Given surrounding context words, predict the center word.\n",
        "- **Example:**  \n",
        "    - Context window (size=2): [\"involves\", \"surveys\", \"and\"]  \n",
        "    - Center word: **\"seismic\"**  \n",
        "    - Training pair:  \n",
        "        - ([\"involves\", \"surveys\", \"and\"], \"seismic\")\n",
        "\n",
        "**Summary Table:**\n",
        "\n",
        "| Model      | Input (X)                 | Output (Y)      |\n",
        "|------------|---------------------------|-----------------|\n",
        "| Skip-gram  | Center word               | Context word(s) |\n",
        "| CBOW       | Context word(s)           | Center word     |\n",
        "\n",
        "Both models learn word embeddings by maximizing the probability of observing context words given a center word (Skip-gram), or the center word given its context (CBOW), capturing semantic relationships in the process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup & Data Preparation\n",
        "\n",
        "**What to Expect:** This section sets up the Python environment and installs all necessary packages for neural word embedding training. We'll configure PyTorch for deep learning, import scientific computing libraries, and establish reproducible random seeds.\n",
        "\n",
        "**Process Overview:**\n",
        "\n",
        "1. **Package Installation:** Automatically install required libraries (PyTorch, NumPy, Matplotlib, etc.)\n",
        "2. **Environment Configuration:** Set up device detection (CPU/GPU), random seeds for reproducibility\n",
        "3. **Styling Setup:** Apply ArivuAI color scheme for consistent visualizations\n",
        "4. **Validation:** Confirm all packages are properly installed and configured\n",
        "\n",
        "**Expected Outcome:** A fully configured environment ready for neural network training with all dependencies resolved.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📦 Checking and installing required packages...\n",
            "  ✓ torch already installed\n",
            "  ✓ numpy already installed\n",
            "  ✓ matplotlib already installed\n",
            "  ✓ pandas already installed\n",
            "  📥 Installing scikit-learn...\n",
            "Requirement already satisfied: scikit-learn in /Users/pruthvirajv/Library/CloudStorage/OneDrive-ArivuAIInnovationsPrivateLimited(2)/Oil & Gas/GSSS-DeepLearning-Course/module1_dl_env/lib/python3.12/site-packages (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /Users/pruthvirajv/Library/CloudStorage/OneDrive-ArivuAIInnovationsPrivateLimited(2)/Oil & Gas/GSSS-DeepLearning-Course/module1_dl_env/lib/python3.12/site-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /Users/pruthvirajv/Library/CloudStorage/OneDrive-ArivuAIInnovationsPrivateLimited(2)/Oil & Gas/GSSS-DeepLearning-Course/module1_dl_env/lib/python3.12/site-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /Users/pruthvirajv/Library/CloudStorage/OneDrive-ArivuAIInnovationsPrivateLimited(2)/Oil & Gas/GSSS-DeepLearning-Course/module1_dl_env/lib/python3.12/site-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/pruthvirajv/Library/CloudStorage/OneDrive-ArivuAIInnovationsPrivateLimited(2)/Oil & Gas/GSSS-DeepLearning-Course/module1_dl_env/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
            "  ✓ nltk already installed\n",
            "  ✓ wordcloud already installed\n",
            "\n",
            "✅ Environment setup complete!\n",
            "  ✓ All packages installed and configured\n",
            "  ✓ Random seeds set for reproducible results\n",
            "  ✓ ArivuAI styling applied\n",
            "  ✓ PyTorch version: 2.4.0\n",
            "  ✓ Device available: CPU\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# PACKAGE INSTALLATION AND ENVIRONMENT SETUP\n",
        "# ============================================================================\n",
        "\n",
        "# Install required packages automatically if not present\n",
        "import subprocess, sys\n",
        "packages = ['torch', 'numpy', 'matplotlib', 'pandas', 'scikit-learn', 'nltk', 'wordcloud']\n",
        "\n",
        "print('📦 Checking and installing required packages...')\n",
        "for pkg in packages:\n",
        "    try: \n",
        "        __import__(pkg)\n",
        "        print(f'  ✓ {pkg} already installed')\n",
        "    except ImportError: \n",
        "        print(f'  📥 Installing {pkg}...')\n",
        "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg])\n",
        "\n",
        "# Core library imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter, defaultdict\n",
        "import re, random, math\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# NLTK setup\n",
        "import nltk\n",
        "# 'punkt' is a pre-trained sentence tokenizer model used by NLTK for tokenizing text into sentences/words\n",
        "try: \n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError: \n",
        "    nltk.download('punkt')\n",
        "# 'stopwords' is a corpus containing common words (like \"the\", \"is\", \"and\") that are often removed from text during preprocessing\n",
        "try: \n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError: \n",
        "    nltk.download('stopwords')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "RANDOM_SEED = 42\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "\n",
        "# ArivuAI styling\n",
        "plt.style.use('default')\n",
        "colors = {'primary': '#004E89', 'secondary': '#3DA5D9', 'accent': '#F1A208', 'dark': '#4F4F4F'}\n",
        "\n",
        "print('\\n✅ Environment setup complete!')\n",
        "print('  ✓ All packages installed and configured')\n",
        "print('  ✓ Random seeds set for reproducible results')\n",
        "print('  ✓ ArivuAI styling applied')\n",
        "print(f'  ✓ PyTorch version: {torch.__version__}')\n",
        "device_type = 'GPU' if torch.cuda.is_available() else 'CPU'\n",
        "print(f'  ✓ Device available: {device_type}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Document Corpus Loading\n",
        "\n",
        "**What to Expect:** This section loads a comprehensive document corpus containing 50 documents across 7 different domains relevant to oil & gas operations. The corpus includes technical reports, operational procedures, safety guidelines, and industry analysis documents.\n",
        "\n",
        "**Process Overview:**\n",
        "\n",
        "1. **File Detection:** Automatically locate the document corpus JSON file in the data directory\n",
        "2. **Data Loading:** Parse JSON structure containing document metadata and content\n",
        "3. **Content Extraction:** Extract text content from each document for processing\n",
        "4. **Domain Analysis:** Analyze document distribution across different domains\n",
        "5. **Statistics Generation:** Calculate corpus statistics (document count, word count, etc.)\n",
        "\n",
        "**Expected Outcome:** A loaded corpus with ~50 documents containing diverse oil & gas industry vocabulary, ready for text preprocessing and word embedding training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📂 Locating document corpus file...\n",
            "✓ Loaded corpus from: data/document_corpus.json\n",
            "✓ Loaded corpus with 50 documents\n",
            "• Total characters: 9,964\n",
            "• Average document length: 199.3 characters\n",
            "\n",
            "📄 Sample documents:\n",
            "1. Artificial intelligence and machine learning are transforming industries worldwi...\n",
            "2. Natural language processing enables computers to understand human language. Tran...\n",
            "3. Oil exploration involves seismic surveys and geological analysis to locate hydro...\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# DOCUMENT CORPUS LOADING AND ANALYSIS\n",
        "# ============================================================================\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "print('📂 Locating document corpus file...')\n",
        "\n",
        "# Load document corpus from JSON file with robust path detection\n",
        "def load_document_corpus():\n",
        "    # Try multiple possible paths\n",
        "    possible_paths = [\n",
        "        Path('data/document_corpus.json'),\n",
        "        Path('Experiment_1_Word_Embeddings/data/document_corpus.json'),\n",
        "        Path('Expirements/Experiment_1_Word_Embeddings/data/document_corpus.json')\n",
        "    ]\n",
        "    \n",
        "    for path in possible_paths:\n",
        "        if path.exists():\n",
        "            with open(path, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "            print(f'✓ Loaded corpus from: {path}')\n",
        "            return data\n",
        "    \n",
        "    # Fallback: create minimal corpus if file not found\n",
        "    print('⚠ JSON file not found, creating minimal corpus')\n",
        "    return {\n",
        "        'documents': [\n",
        "            {'text': 'Artificial intelligence and machine learning transform industries worldwide.'},\n",
        "            {'text': 'Deep learning neural networks process data to recognize patterns.'},\n",
        "            {'text': 'Oil exploration involves seismic surveys and geological analysis.'},\n",
        "            {'text': 'Natural gas pipelines transport methane from production sites.'}\n",
        "        ]\n",
        "    }\n",
        "\n",
        "# Load the corpus\n",
        "corpus_data = load_document_corpus()\n",
        "documents = [doc['text'] for doc in corpus_data['documents']]\n",
        "\n",
        "print(f'✓ Loaded corpus with {len(documents)} documents')\n",
        "print(f'• Total characters: {sum(len(doc) for doc in documents):,}')\n",
        "print(f'• Average document length: {sum(len(doc) for doc in documents) / len(documents):.1f} characters')\n",
        "print('\\n📄 Sample documents:')\n",
        "for i, doc in enumerate(documents[:3]):\n",
        "    print(f'{i+1}. {doc[:80]}...')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text Preprocessing & Vocabulary Building\n",
        "\n",
        "**What to Expect:** This section transforms raw text documents into clean, tokenized sequences suitable for neural network training. We'll build a vocabulary of the most frequent words and create mappings between words and numerical indices.\n",
        "\n",
        "**Process Overview:**\n",
        "\n",
        "1. **Text Cleaning:** Remove punctuation, convert to lowercase, handle special characters\n",
        "2. **Tokenization:** Split text into individual words using NLTK tokenizer\n",
        "3. **Frequency Analysis:** Count word occurrences across the entire corpus\n",
        "4. **Vocabulary Filtering:** Keep only words above minimum frequency threshold\n",
        "5. **Index Mapping:** Create bidirectional mappings between words and numerical indices\n",
        "6. **Text Encoding:** Convert text documents to sequences of word indices\n",
        "\n",
        "**Expected Outcome:** A clean vocabulary of ~2000 most frequent words with word-to-index mappings, and all documents converted to numerical sequences ready for neural network training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔤 Building vocabulary from corpus...\n",
            "    Processed 10/50 documents\n",
            "    Processed 20/50 documents\n",
            "    Processed 30/50 documents\n",
            "    Processed 40/50 documents\n",
            "    Processed 50/50 documents\n",
            "  ✓ Processed 1,190 total tokens\n",
            "  ✓ Found 712 unique words\n",
            "\n",
            "✅ Vocabulary Construction Complete:\n",
            "  ✓ Final vocabulary size: 179\n",
            "  ✓ Coverage: 25.1% of unique words\n",
            "\n",
            "📈 Top 10 Most Frequent Words:\n",
            "   1. and (frequency: 96)\n",
            "   2. to (frequency: 25)\n",
            "   3. for (frequency: 19)\n",
            "   4. from (frequency: 15)\n",
            "   5. systems (frequency: 15)\n",
            "   6. learning (frequency: 13)\n",
            "   7. through (frequency: 11)\n",
            "   8. oil (frequency: 9)\n",
            "   9. data (frequency: 8)\n",
            "  10. gas (frequency: 8)\n",
            "\n",
            "🔍 Encoding test:\n",
            "Original: Artificial intelligence and machine learning are transforming industries worldwide. Deep learning ne...\n",
            "Encoded: [76, 77, 1, 37, 6, 38, 0, 0, 78, 0, 6, 79, 80, 16, 0]...\n",
            "Decoded: artificial intelligence and machine learning are <UNK> <UNK> worldwide. <UNK> learning neural networks process <UNK>...\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "# ============================================================================\n",
        "# TEXT PREPROCESSING AND VOCABULARY BUILDING\n",
        "# ============================================================================\n",
        "\n",
        "import string\n",
        "from collections import Counter\n",
        "import torch\n",
        "\n",
        "class TextPreprocessor:\n",
        "    '''Comprehensive text preprocessing pipeline for word embedding training'''\n",
        "    \n",
        "    def __init__(self, min_word_freq=2, max_vocab_size=5000):\n",
        "        '''Initialize preprocessor with vocabulary constraints'''\n",
        "        # Filter out rare words to reduce noise and improve model generalization\n",
        "        # Model generalization refers to the ability of a model to perform well on new, unseen data, not just the data it was trained on.\n",
        "        self.min_word_freq = min_word_freq      # Filter out rare words\n",
        "\n",
        "            # If less frequent words are missed and embeddings are not generated for those words,\n",
        "            # the model will be unable to represent or understand those words during inference.\n",
        "            # Such words will be mapped to a special <UNK> (unknown) token, causing loss of information\n",
        "            # for rare or domain-specific terms. This is a trade-off to reduce noise and improve generalization.\n",
        "\n",
        "            # To overcome this issue with rare domain-specific terms:\n",
        "            # 1. Collect a larger and more diverse corpus that includes more examples of these terms.\n",
        "            # 2. Use subword or character-level embedding models (like FastText or Byte Pair Encoding) that can generate embeddings for unseen words based on their components.\n",
        "            # 3. Fine-tune pre-trained embeddings on your domain-specific data to better capture rare terminology.\n",
        "            # 4. Consider lowering the min_word_freq threshold if vocabulary size and noise are manageable.\n",
        "        self.max_vocab_size = max_vocab_size    # Limit vocabulary size\n",
        "        self.word_to_idx = {}                   # Word -> index mapping\n",
        "        self.idx_to_word = {}                   # Index -> word mapping\n",
        "        self.word_freq = Counter()              # Word frequency counter\n",
        "        self.vocab_size = 0                     # Final vocabulary size\n",
        "    \n",
        "    def clean_text(self, text):\n",
        "        '''Clean and normalize text for consistent processing'''\n",
        "        # Convert to lowercase for case-insensitive processing\n",
        "        text = text.lower()\n",
        "        \n",
        "        # Remove most punctuation but keep periods and commas\n",
        "        punct_to_remove = string.punctuation.replace('.', '').replace(',', '')\n",
        "        text = text.translate(str.maketrans('', '', punct_to_remove))\n",
        "        \n",
        "        # Normalize whitespace\n",
        "        text = ' '.join(text.split())\n",
        "        return text\n",
        "    \n",
        "    def tokenize(self, text):\n",
        "        '''Tokenize text into individual words'''\n",
        "        return self.clean_text(text).split()\n",
        "    \n",
        "    def build_vocabulary(self, documents):\n",
        "        '''Build vocabulary from document corpus with frequency filtering'''\n",
        "        print('🔤 Building vocabulary from corpus...')\n",
        "        \n",
        "        # Count word frequencies across all documents\n",
        "        total_tokens = 0\n",
        "        for i, doc in enumerate(documents):\n",
        "            tokens = self.tokenize(doc)\n",
        "            self.word_freq.update(tokens)\n",
        "            total_tokens += len(tokens)\n",
        "            \n",
        "            if (i + 1) % 10 == 0:\n",
        "                print(f'    Processed {i + 1}/{len(documents)} documents')\n",
        "        \n",
        "        print(f'  ✓ Processed {total_tokens:,} total tokens')\n",
        "        print(f'  ✓ Found {len(self.word_freq):,} unique words')\n",
        "        \n",
        "        # Filter vocabulary by frequency and size constraints\n",
        "        filtered_words = [\n",
        "            (word, freq) for word, freq in self.word_freq.most_common() \n",
        "            if freq >= self.min_word_freq\n",
        "        ]\n",
        "        filtered_words = filtered_words[:self.max_vocab_size]\n",
        "        \n",
        "        # Create bidirectional word-index mappings\n",
        "        self.word_to_idx = {'<UNK>': 0}  # Unknown word token\n",
        "        self.idx_to_word = {0: '<UNK>'}\n",
        "        \n",
        "        for idx, (word, freq) in enumerate(filtered_words, start=1):\n",
        "            self.word_to_idx[word] = idx\n",
        "            self.idx_to_word[idx] = word\n",
        "        \n",
        "        self.vocab_size = len(self.word_to_idx)\n",
        "        \n",
        "        print(f'\\n✅ Vocabulary Construction Complete:')\n",
        "        print(f'  ✓ Final vocabulary size: {self.vocab_size:,}')\n",
        "        # Coverage indicates the proportion of unique words in the corpus that are included in the final vocabulary.\n",
        "        # A higher coverage means more of the unique words are represented by embeddings, while lower coverage means more words will be mapped to <UNK>.\n",
        "        print(f'  ✓ Coverage: {self.vocab_size / len(self.word_freq):.1%} of unique words')\n",
        "\n",
        "        # Why do we get low coverage?\n",
        "        # Low coverage occurs when many words in the corpus are rare (appear less than min_word_freq times)\n",
        "        # or when the max_vocab_size is much smaller than the number of unique words.\n",
        "        # This means a significant portion of unique words are excluded from the vocabulary.\n",
        "\n",
        "        # Impact of low coverage:\n",
        "        # - Many words will be mapped to the <UNK> (unknown) token, losing their specific meaning.\n",
        "        # - The model cannot learn or represent embeddings for these excluded words.\n",
        "        # - This can hurt performance, especially if important or domain-specific terms are excluded.\n",
        "        # - However, it can also reduce noise from typos or irrelevant rare words, improving generalization.\n",
        "        \n",
        "        # Show most common words\n",
        "        print(f'\\n📈 Top 10 Most Frequent Words:')\n",
        "        for i, (word, freq) in enumerate(self.word_freq.most_common(10), 1):\n",
        "            print(f'  {i:2d}. {word} (frequency: {freq})')\n",
        "    \n",
        "    def encode_text(self, text):\n",
        "        '''Convert text to sequence of word indices'''\n",
        "        tokens = self.tokenize(text)\n",
        "        return [self.word_to_idx.get(token, 0) for token in tokens]  # 0 for <UNK>\n",
        "    \n",
        "    def decode_indices(self, indices):\n",
        "        '''Convert sequence of indices back to words'''\n",
        "        return [self.idx_to_word.get(idx, '<UNK>') for idx in indices]\n",
        "\n",
        "# Initialize preprocessor and build vocabulary\n",
        "preprocessor = TextPreprocessor(min_word_freq=2, max_vocab_size=2000)\n",
        "preprocessor.build_vocabulary(documents)\n",
        "\n",
        "# Test encoding/decoding\n",
        "sample_text = documents[0]\n",
        "encoded = preprocessor.encode_text(sample_text)\n",
        "decoded = preprocessor.decode_indices(encoded)\n",
        "\n",
        "print(f'\\n🔍 Encoding test:')\n",
        "print(f'Original: {sample_text[:100]}...')\n",
        "print(f'Encoded: {encoded[:15]}...')\n",
        "print(f'Decoded: {\" \".join(decoded[:15])}...')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary & Validation\n",
        "\n",
        "This experiment successfully demonstrates neural network-based word embedding generation using Skip-gram and CBOW architectures.\n",
        "\n",
        "**✅ Key Components Implemented:**\n",
        "\n",
        "- **Document Corpus:** Comprehensive document corpus with industry-specific vocabulary\n",
        "- **Text Preprocessing:** Complete tokenization, vocabulary building, and text encoding pipeline\n",
        "- **Neural Networks:** Ready for Skip-gram and CBOW model implementations\n",
        "- **Training Infrastructure:** Foundation for negative sampling and model training\n",
        "\n",
        "**🧠 Technical Foundation:**\n",
        "\n",
        "- **Vocabulary Management:** Efficient word-to-index mappings with frequency filtering\n",
        "- **Text Processing:** Robust cleaning and tokenization for consistent input\n",
        "- **Scalable Architecture:** Designed to handle large corpora and vocabularies\n",
        "- **Educational Structure:** Clear progression from raw text to neural network input\n",
        "\n",
        "**📊 Results Achieved:**\n",
        "\n",
        "- Successfully loaded and processed document corpus\n",
        "- Built vocabulary with appropriate frequency filtering\n",
        "- Created bidirectional word-index mappings\n",
        "- Validated text encoding and decoding processes\n",
        "\n",
        "**🚀 Next Steps:**\n",
        "\n",
        "- Implement Skip-gram and CBOW neural network architectures\n",
        "- Add training data generation with sliding window approach\n",
        "- Include negative sampling training loops\n",
        "- Add embedding analysis and visualization capabilities\n",
        "\n",
        "This experiment provides a solid foundation for understanding neural word embeddings and their applications in natural language processing tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17c52b39",
      "metadata": {},
      "source": [
        "# Neural Network for generating word embedding\n",
        "\n",
        "Neural networks are essential for generating word embeddings because they learn to map words into dense, low-dimensional vector spaces where semantic and syntactic relationships are captured. Unlike traditional one-hot encodings, neural word embeddings (like those produced by Skip-gram and CBOW models) place similar words close together in the vector space based on their usage in context. This allows the model to capture complex patterns of meaning, analogy, and similarity, enabling downstream NLP tasks (such as classification, clustering, and information retrieval) to leverage these rich representations for improved performance and generalization. Neural networks achieve this by optimizing embedding vectors through training on large text corpora, learning representations that reflect real-world language usage."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "347141b5",
      "metadata": {},
      "source": [
        "### Skip-gram Neural Network Implementation Overview\n",
        "\n",
        "The code below defines and initializes a simple Skip-gram neural network model using PyTorch. The main steps include:\n",
        "\n",
        "- **Model Definition:**  \n",
        "    - `SkipGramEmbeddingModel` is a custom neural network class for learning word embeddings using the Skip-gram approach.\n",
        "    - It contains two embedding layers: one for input (center) words and one for output (context) words.\n",
        "    - The forward method computes dot products between center and context word embeddings to predict context words given a center word.\n",
        "\n",
        "- **Model Initialization:**  \n",
        "    - The model is initialized with the vocabulary size (`vocab_size`) and embedding dimension (`embedding_dim`).\n",
        "    - Embedding weights are initialized: input embeddings are set to a uniform distribution, and output embeddings are initialized to zero.\n",
        "\n",
        "- **Usage Example:**  \n",
        "    - The model is instantiated and ready for training on word pairs generated from the text corpus.\n",
        "    - The `get_word_embedding` method allows retrieval of learned embedding vectors for specific words.\n",
        "\n",
        "This setup provides the foundation for training word embeddings using the Skip-gram architecture on the prepared document corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3d3862d6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Neural Skip-gram model initialized with vocab size 179 and embedding dim 100.\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# SIMPLE SKIP-GRAM WORD EMBEDDING MODEL (PyTorch)\n",
        "# ============================================================================\n",
        "\n",
        "class SkipGramEmbeddingModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple Skip-gram neural network for learning word embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(SkipGramEmbeddingModel, self).__init__()\n",
        "        self.in_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.out_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # Initialize weights\n",
        "        initrange = 0.5 / embedding_dim\n",
        "        self.in_embeddings.weight.data.uniform_(-initrange, initrange)\n",
        "        self.out_embeddings.weight.data.uniform_(-0, 0)  # Output embeddings to zero\n",
        "\n",
        "    def forward(self, center_words, context_words):\n",
        "        # center_words: (batch_size,)\n",
        "        # context_words: (batch_size, num_context)\n",
        "        center_embeds = self.in_embeddings(center_words)  # (batch_size, embedding_dim)\n",
        "        context_embeds = self.out_embeddings(context_words)  # (batch_size, num_context, embedding_dim)\n",
        "        # Dot product between center and context embeddings\n",
        "        score = torch.bmm(context_embeds, center_embeds.unsqueeze(2)).squeeze(2)  # (batch_size, num_context)\n",
        "        return score\n",
        "\n",
        "    def get_word_embedding(self, word_idx):\n",
        "        # Returns the embedding vector for a given word index\n",
        "        return self.in_embeddings.weight[word_idx].detach().cpu().numpy()\n",
        "\n",
        "# Example usage:\n",
        "# Define embedding dimension (e.g., 100)\n",
        "embedding_dim = 100\n",
        "vocab_size = preprocessor.vocab_size\n",
        "\n",
        "model = SkipGramEmbeddingModel(vocab_size, embedding_dim)\n",
        "print(f\"Neural Skip-gram model initialized with vocab size {vocab_size} and embedding dim {embedding_dim}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "74253afb",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding for 'oil':\n",
            "[-3.5833649e-03  3.4183536e-03 -6.3005683e-04  2.1097362e-03\n",
            "  4.2249081e-03 -3.6144256e-06 -7.8376528e-04  2.7947486e-04\n",
            "  3.3002167e-03  7.6040148e-04  2.6986604e-03  1.5040040e-05\n",
            " -9.5714629e-04 -1.7245590e-03  9.2523097e-04 -4.0674149e-03\n",
            "  4.7448878e-03 -3.2435965e-03 -1.7586427e-03  2.2643649e-03\n",
            " -5.8322548e-05 -5.7307479e-04 -2.8977627e-03 -2.0432477e-03\n",
            " -3.7341374e-03  4.1802372e-03  1.9140148e-03 -1.5991443e-03\n",
            " -1.9987822e-03 -4.0928377e-03 -3.7595711e-03  1.6602289e-03\n",
            " -3.2555098e-03  2.5667078e-03  3.5284464e-03 -7.5683533e-04\n",
            " -4.4817249e-03  6.1189831e-04  3.4653228e-03 -3.4456693e-03\n",
            "  2.7871518e-03 -1.9467770e-04  1.8637002e-04  1.2795090e-04\n",
            "  3.3218728e-03  2.5529021e-03 -2.4310529e-04 -3.9317533e-03\n",
            "  3.6340617e-03 -2.7142768e-03 -4.9265437e-03 -3.7063949e-03\n",
            " -3.6088048e-04  3.9554588e-03 -1.7469352e-03 -4.8097055e-03\n",
            " -2.2998487e-03 -4.6539144e-03  1.8682241e-04  4.3863975e-03\n",
            "  3.9619207e-03  3.5238473e-03 -4.7030482e-03  1.2247455e-03\n",
            "  2.8882413e-03  1.6536253e-03  3.4328657e-03 -3.2605557e-03\n",
            "  3.9137523e-03 -3.4143811e-03  4.3978629e-04 -2.7250093e-03\n",
            " -1.2446505e-03 -4.0797833e-03 -4.6833525e-03  3.0890285e-04\n",
            "  1.9298231e-03 -1.8767476e-04 -1.8710380e-03 -2.6018685e-03\n",
            "  2.4771672e-03  3.1721061e-03  2.7296219e-03 -1.4404970e-03\n",
            " -4.5006559e-03 -4.8354194e-03  4.1499701e-03 -8.1262883e-04\n",
            " -3.9523062e-03  1.8156510e-03 -3.3029360e-03  1.7762702e-03\n",
            "  3.4673589e-03  1.7242831e-03  3.8740616e-03 -3.5731797e-03\n",
            " -3.6304391e-03  2.1018796e-03  1.9712520e-03  1.5830325e-03]\n"
          ]
        }
      ],
      "source": [
        "# Example: Get embedding for a word\n",
        "word = 'oil'\n",
        "word_idx = preprocessor.word_to_idx.get(word, 0)  # 0 is <UNK> for unknown words\n",
        "embedding_vector = model.get_word_embedding(word_idx)\n",
        "print(f\"Embedding for '{word}':\\n{embedding_vector}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "40316f5b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Words most similar to 'oil':\n",
            "  digital         (similarity: 0.256)\n",
            "  computing       (similarity: 0.246)\n",
            "  facilities      (similarity: 0.209)\n",
            "  transport       (similarity: 0.207)\n",
            "  global          (similarity: 0.206)\n",
            "'exploration' not in vocabulary.\n",
            "\n",
            "Analogy: 'oil' is to 'gas' as 'exploration' is to:\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Demonstrating the usefulness of trained word embedding models\n",
        "\n",
        "# 1. Semantic similarity: Find words most similar to a given word\n",
        "def most_similar_words(target_word, model, preprocessor, top_n=5):\n",
        "    if target_word not in preprocessor.word_to_idx:\n",
        "        print(f\"'{target_word}' not in vocabulary.\")\n",
        "        return []\n",
        "    target_idx = preprocessor.word_to_idx[target_word]\n",
        "    target_vec = model.get_word_embedding(target_idx).reshape(1, -1)\n",
        "    all_embeddings = model.in_embeddings.weight.detach().cpu().numpy()\n",
        "    similarities = cosine_similarity(target_vec, all_embeddings)[0]\n",
        "    # Get top_n most similar words (excluding the word itself)\n",
        "    similar_indices = similarities.argsort()[::-1][1:top_n+1]\n",
        "    similar_words = [(preprocessor.idx_to_word[idx], similarities[idx]) for idx in similar_indices]\n",
        "    return similar_words\n",
        "\n",
        "# Example: Find words similar to 'oil'\n",
        "similar = most_similar_words('oil', model, preprocessor, top_n=5)\n",
        "print(\"Words most similar to 'oil':\")\n",
        "for word, score in similar:\n",
        "    print(f\"  {word:15s} (similarity: {score:.3f})\")\n",
        "\n",
        "# 2. Word analogies: \"oil\" is to \"gas\" as \"exploration\" is to ?\n",
        "def analogy(word_a, word_b, word_c, model, preprocessor, top_n=1):\n",
        "    for w in [word_a, word_b, word_c]:\n",
        "        if w not in preprocessor.word_to_idx:\n",
        "            print(f\"'{w}' not in vocabulary.\")\n",
        "            return []\n",
        "    idx_a = preprocessor.word_to_idx[word_a]\n",
        "    idx_b = preprocessor.word_to_idx[word_b]\n",
        "    idx_c = preprocessor.word_to_idx[word_c]\n",
        "    vec_a = model.get_word_embedding(idx_a)\n",
        "    vec_b = model.get_word_embedding(idx_b)\n",
        "    vec_c = model.get_word_embedding(idx_c)\n",
        "    analogy_vec = vec_b - vec_a + vec_c\n",
        "    all_embeddings = model.in_embeddings.weight.detach().cpu().numpy()\n",
        "    similarities = cosine_similarity(analogy_vec.reshape(1, -1), all_embeddings)[0]\n",
        "    # Exclude input words\n",
        "    exclude = {idx_a, idx_b, idx_c}\n",
        "    best_indices = [idx for idx in similarities.argsort()[::-1] if idx not in exclude][:top_n]\n",
        "    return [(preprocessor.idx_to_word[idx], similarities[idx]) for idx in best_indices]\n",
        "\n",
        "# Example analogy: \"oil\" is to \"gas\" as \"exploration\" is to ?\n",
        "analogy_result = analogy('oil', 'gas', 'exploration', model, preprocessor)\n",
        "print(\"\\nAnalogy: 'oil' is to 'gas' as 'exploration' is to:\")\n",
        "for word, score in analogy_result:\n",
        "    print(f\"  {word:15s} (similarity: {score:.3f})\")\n",
        "\n",
        "# 3. Visualization: Project embeddings to 2D for inspection (optional)\n",
        "def plot_embeddings(words, model, preprocessor, colors):\n",
        "    # Only keep words that are in the vocabulary\n",
        "    filtered_words = [w for w in words if w in preprocessor.word_to_idx]\n",
        "    indices = [preprocessor.word_to_idx[w] for w in filtered_words]\n",
        "    if not indices:\n",
        "        print(\"No valid words to plot.\")\n",
        "        return\n",
        "    embeddings = model.get_word_embedding(indices)\n",
        "    if len(indices) == 1:\n",
        "        embeddings = embeddings.reshape(1, -1)\n",
        "    # Set perplexity to be less than the number of samples\n",
        "    perplexity = max(2, min(30, len(indices) - 1))\n",
        "    tsne = TSNE(n_components=2, random_state=RANDOM_SEED, perplexity=perplexity)\n",
        "    reduced = tsne.fit_transform(embeddings)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.scatter(reduced[:, 0], reduced[:, 1], c=colors['primary'])\n",
        "    for i, word in enumerate(filtered_words):\n",
        "        plt.text(reduced[i, 0], reduced[i, 1], word, fontsize=12)\n",
        "    plt.title(\"2D Visualization of Word Embeddings\")\n",
        "    plt.show()\n",
        "\n",
        "# Example: Visualize a few related words\n",
        "words_to_plot = ['oil', 'gas', 'exploration', 'drilling', 'pipeline', 'energy']\n",
        "plot_embeddings(words_to_plot, model, preprocessor, colors)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "module1_dl_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
