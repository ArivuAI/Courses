{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experiment 8: GridWorld RL for Oil & Gas Facility Navigation\n",
        "\n",
        "**Course:** Introduction to Deep Learning | **Module:** Reinforcement Learning\n",
        "\n",
        "---\n",
        "\n",
        "## Objective\n",
        "\n",
        "Design and implement reinforcement learning algorithms for optimal navigation in oil & gas facilities, focusing on safety-aware path planning, equipment inspection routes, and emergency evacuation procedures.\n",
        "\n",
        "## Learning Outcomes\n",
        "\n",
        "By the end of this experiment, you will:\n",
        "\n",
        "1. Understand reinforcement learning fundamentals and the GridWorld environment\n",
        "2. Implement Q-Learning and Deep Q-Network (DQN) algorithms\n",
        "3. Apply RL to real-world facility navigation and safety scenarios\n",
        "4. Compare tabular and deep learning approaches to RL\n",
        "5. Evaluate RL agents using safety and efficiency metrics\n",
        "\n",
        "## Background & Theory\n",
        "\n",
        "**Reinforcement Learning (RL)** is a machine learning paradigm where agents learn optimal behavior through interaction with an environment, receiving rewards and penalties for their actions.\n",
        "\n",
        "**Key Components:**\n",
        "\n",
        "- **Agent:** The decision-making entity (facility operator, robot, autonomous system)\n",
        "- **Environment:** The facility layout with obstacles, equipment, and hazards\n",
        "- **State (s):** Current position and situation in the facility\n",
        "- **Action (a):** Available moves (up, down, left, right, stay)\n",
        "- **Reward (r):** Feedback for actions (positive for goals, negative for hazards)\n",
        "- **Policy (π):** Strategy for selecting actions in each state\n",
        "\n",
        "**Mathematical Foundation:**\n",
        "\n",
        "- Bellman Equation: V(s) = max_a Σ P(s'|s,a)[R(s,a,s') + γV(s')]\n",
        "- Q-Learning Update: Q(s,a) ← Q(s,a) + α[r + γ max_a' Q(s',a') - Q(s,a)]\n",
        "- Policy: π(s) = argmax_a Q(s,a)\n",
        "- Expected Return: G*t = Σ γ^k R*{t+k+1}\n",
        "\n",
        "**Algorithms:**\n",
        "\n",
        "- **Q-Learning:** Model-free, off-policy algorithm using Q-tables\n",
        "- **Deep Q-Network (DQN):** Neural network approximation of Q-function\n",
        "- **Policy Gradient:** Direct optimization of policy parameters\n",
        "\n",
        "**Applications in Oil & Gas:**\n",
        "\n",
        "- Autonomous inspection robots navigating facilities\n",
        "- Emergency evacuation route optimization\n",
        "- Maintenance scheduling and resource allocation\n",
        "- Safety protocol development and training\n",
        "- Risk-aware decision making in hazardous environments\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup & Dependencies\n",
        "\n",
        "**What to Expect:** This section establishes the Python environment for reinforcement learning in GridWorld environments. We'll install PyTorch for deep Q-networks, visualization libraries for environment rendering, and data structures for Q-learning algorithms.\n",
        "\n",
        "**Process Overview:**\n",
        "\n",
        "1. **Package Installation:** Install PyTorch for neural networks, matplotlib/seaborn for environment visualization, and scientific computing libraries\n",
        "2. **RL Environment Setup:** Configure GridWorld environment with customizable layouts, obstacles, and reward structures\n",
        "3. **Algorithm Framework:** Set up Q-learning tables, experience replay buffers, and neural network architectures\n",
        "4. **Visualization Tools:** Configure environment rendering, learning curves, and policy visualization\n",
        "5. **Evaluation Metrics:** Set up performance tracking for episode rewards, convergence analysis, and safety metrics\n",
        "\n",
        "**Expected Outcome:** A fully configured environment ready for reinforcement learning experiments with both tabular Q-learning and deep Q-networks, including comprehensive visualization and evaluation tools.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ PyTorch version: 2.8.0+cpu\n",
            "✓ Device: cpu\n",
            "✓ Data directory: d:\\Suni Files\\AI Code Base\\Oil and Gas\\Oil and Gas Pruthvi College Course Material\\Updated\\Expirements\\Experiment_8_GridWorld_RL\\data\n",
            "✓ All packages installed and configured\n",
            "✓ Random seeds set for reproducible results\n",
            "✓ ArivuAI styling applied\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "import subprocess, sys\n",
        "packages = ['torch', 'numpy', 'matplotlib', 'pandas', 'scikit-learn', 'seaborn']\n",
        "for pkg in packages:\n",
        "    try: __import__(pkg.replace('-', '_').lower())\n",
        "    except ImportError: subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg])\n",
        "\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np, pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import defaultdict, deque\n",
        "import json, random, time\n",
        "from pathlib import Path\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Data directory setup\n",
        "DATA_DIR = Path('data')\n",
        "if not DATA_DIR.exists():\n",
        "    DATA_DIR = Path('Expirements/data')\n",
        "if not DATA_DIR.exists():\n",
        "    DATA_DIR = Path('.')\n",
        "    print('Warning: Using current directory for data')\n",
        "\n",
        "# ArivuAI styling\n",
        "plt.style.use('default')\n",
        "colors = {'primary': '#004E89', 'secondary': '#3DA5D9', 'accent': '#F1A208', 'dark': '#4F4F4F'}\n",
        "sns.set_palette([colors['primary'], colors['secondary'], colors['accent'], colors['dark']])\n",
        "\n",
        "print(f'✓ PyTorch version: {torch.__version__}')\n",
        "print(f'✓ Device: {device}')\n",
        "print(f'✓ Data directory: {DATA_DIR.absolute()}')\n",
        "print('✓ All packages installed and configured')\n",
        "print('✓ Random seeds set for reproducible results')\n",
        "print('✓ ArivuAI styling applied')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GridWorld Environment Implementation\n",
        "\n",
        "Create the oil & gas facility GridWorld environment with safety constraints and objectives.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Configuration loaded from JSON\n",
            "✓ GridWorld environment initialized\n",
            "✓ Scenario: Facility Inspection\n",
            "✓ Grid size: 8x8\n",
            "✓ Actions: 5 (['UP', 'DOWN', 'LEFT', 'RIGHT', 'STAY'])\n",
            "✓ Equipment positions: 4\n",
            "✓ Hazard positions: 4\n",
            "✓ Checkpoint positions: 2\n",
            "\n",
            "GridWorld - Facility Inspection\n",
            "Steps: 0/100\n",
            "Equipment visited: 0/4\n",
            "Checkpoints visited: 0/2\n",
            "Hazard encounters: 0\n",
            "\n",
            "# # # # # # # #\n",
            "# A . E . H . #\n",
            "# . # . # . . #\n",
            "# E . C . . H #\n",
            "# . # . # E . #\n",
            "# H . . . . C #\n",
            "# . E . H . G #\n",
            "# # # # # # # #\n",
            "\n",
            "Legend: S=Start, G=Goal, A=Agent, E=Equipment, C=Checkpoint, H=Hazard, #=Wall, .=Empty, *=Path\n"
          ]
        }
      ],
      "source": [
        "class OilGasGridWorld:\n",
        "    def __init__(self, config_path, scenario='facility_inspection'):\n",
        "        \"\"\"Initialize GridWorld environment for oil & gas facility navigation\"\"\"\n",
        "        try:\n",
        "            with open(config_path, 'r') as f:\n",
        "                self.config = json.load(f)\n",
        "            print('✓ Configuration loaded from JSON')\n",
        "        except FileNotFoundError:\n",
        "            print('Creating default configuration...')\n",
        "            self.config = self._create_default_config()\n",
        "        \n",
        "        self.scenario = scenario\n",
        "        self.cell_types = self.config['environment_config']['cell_types']\n",
        "        self.actions = self.config['agent_config']['actions']\n",
        "        self.max_steps = self.config['agent_config']['max_steps']\n",
        "        \n",
        "        # Load scenario-specific grid\n",
        "        self.grid_layout = self.config['oil_gas_scenarios'][scenario]['grid_layout']\n",
        "        self.grid = np.array(self.grid_layout)\n",
        "        self.rows, self.cols = self.grid.shape\n",
        "        \n",
        "        # Find special positions\n",
        "        self.start_pos = self._find_position('S')\n",
        "        self.goal_pos = self._find_position('G')\n",
        "        self.equipment_pos = self._find_positions('E')\n",
        "        self.hazard_pos = self._find_positions('H')\n",
        "        self.checkpoint_pos = self._find_positions('C')\n",
        "        \n",
        "        # Initialize state\n",
        "        self.reset()\n",
        "    \n",
        "    def _create_default_config(self):\n",
        "        \"\"\"Create default configuration if JSON file not found\"\"\"\n",
        "        return {\n",
        "            'environment_config': {\n",
        "                'cell_types': {\n",
        "                    'empty': {'reward': -0.1, 'passable': True},\n",
        "                    'wall': {'reward': 0, 'passable': False},\n",
        "                    'goal': {'reward': 10, 'passable': True},\n",
        "                    'hazard': {'reward': -5, 'passable': True}\n",
        "                }\n",
        "            },\n",
        "            'agent_config': {\n",
        "                'actions': {\n",
        "                    '0': {'name': 'UP', 'delta': [-1, 0]},\n",
        "                    '1': {'name': 'DOWN', 'delta': [1, 0]},\n",
        "                    '2': {'name': 'LEFT', 'delta': [0, -1]},\n",
        "                    '3': {'name': 'RIGHT', 'delta': [0, 1]}\n",
        "                },\n",
        "                'max_steps': 100\n",
        "            },\n",
        "            'oil_gas_scenarios': {\n",
        "                'facility_inspection': {\n",
        "                    'grid_layout': [\n",
        "                        ['#', '#', '#', '#'],\n",
        "                        ['#', 'S', '.', '#'],\n",
        "                        ['#', '.', 'G', '#'],\n",
        "                        ['#', '#', '#', '#']\n",
        "                    ]\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    \n",
        "    def _find_position(self, symbol):\n",
        "        \"\"\"Find position of a specific symbol in the grid\"\"\"\n",
        "        positions = np.where(self.grid == symbol)\n",
        "        if len(positions[0]) > 0:\n",
        "            return (positions[0][0], positions[1][0])\n",
        "        return None\n",
        "    \n",
        "    def _find_positions(self, symbol):\n",
        "        \"\"\"Find all positions of a specific symbol in the grid\"\"\"\n",
        "        positions = np.where(self.grid == symbol)\n",
        "        return list(zip(positions[0], positions[1]))\n",
        "    \n",
        "    def reset(self):\n",
        "        \"\"\"Reset environment to initial state\"\"\"\n",
        "        self.agent_pos = self.start_pos\n",
        "        self.steps = 0\n",
        "        self.visited_equipment = set()\n",
        "        self.visited_checkpoints = set()\n",
        "        self.hazard_encounters = 0\n",
        "        self.done = False\n",
        "        return self._get_state()\n",
        "    \n",
        "    def _get_state(self):\n",
        "        \"\"\"Get current state representation\"\"\"\n",
        "        # Simple state: agent position as tuple\n",
        "        return self.agent_pos\n",
        "    \n",
        "    def _is_valid_position(self, pos):\n",
        "        \"\"\"Check if position is valid and passable\"\"\"\n",
        "        row, col = pos\n",
        "        if row < 0 or row >= self.rows or col < 0 or col >= self.cols:\n",
        "            return False\n",
        "        \n",
        "        cell_type = self.grid[row, col]\n",
        "        if cell_type == '#':  # Wall\n",
        "            return False\n",
        "        \n",
        "        return True\n",
        "    \n",
        "    def step(self, action):\n",
        "        \"\"\"Execute action and return next state, reward, done, info\"\"\"\n",
        "        if self.done:\n",
        "            return self._get_state(), 0, True, {}\n",
        "        \n",
        "        # Get action delta\n",
        "        action_info = self.actions[str(action)]\n",
        "        delta = action_info['delta']\n",
        "        \n",
        "        # Calculate new position\n",
        "        new_row = self.agent_pos[0] + delta[0]\n",
        "        new_col = self.agent_pos[1] + delta[1]\n",
        "        new_pos = (new_row, new_col)\n",
        "        \n",
        "        # Check if move is valid\n",
        "        if self._is_valid_position(new_pos):\n",
        "            self.agent_pos = new_pos\n",
        "        \n",
        "        # Calculate reward\n",
        "        reward = self._calculate_reward()\n",
        "        \n",
        "        # Update step counter\n",
        "        self.steps += 1\n",
        "        \n",
        "        # Check termination conditions\n",
        "        self.done = self._check_done()\n",
        "        \n",
        "        # Prepare info\n",
        "        info = {\n",
        "            'steps': self.steps,\n",
        "            'visited_equipment': len(self.visited_equipment),\n",
        "            'visited_checkpoints': len(self.visited_checkpoints),\n",
        "            'hazard_encounters': self.hazard_encounters\n",
        "        }\n",
        "        \n",
        "        return self._get_state(), reward, self.done, info\n",
        "    \n",
        "    def _calculate_reward(self):\n",
        "        \"\"\"Calculate reward for current position\"\"\"\n",
        "        cell_type = self.grid[self.agent_pos[0], self.agent_pos[1]]\n",
        "        \n",
        "        # Base reward for cell type\n",
        "        if cell_type == 'G':  # Goal\n",
        "            return 10\n",
        "        elif cell_type == 'H':  # Hazard\n",
        "            self.hazard_encounters += 1\n",
        "            return -5\n",
        "        elif cell_type == 'E':  # Equipment\n",
        "            if self.agent_pos not in self.visited_equipment:\n",
        "                self.visited_equipment.add(self.agent_pos)\n",
        "                return 1\n",
        "            return -0.1  # Already visited\n",
        "        elif cell_type == 'C':  # Checkpoint\n",
        "            if self.agent_pos not in self.visited_checkpoints:\n",
        "                self.visited_checkpoints.add(self.agent_pos)\n",
        "                return 2\n",
        "            return -0.1  # Already visited\n",
        "        else:  # Empty space\n",
        "            return -0.1  # Small penalty for each step\n",
        "    \n",
        "    def _check_done(self):\n",
        "        \"\"\"Check if episode is complete\"\"\"\n",
        "        # Goal reached\n",
        "        if self.agent_pos == self.goal_pos:\n",
        "            return True\n",
        "        \n",
        "        # Max steps reached\n",
        "        if self.steps >= self.max_steps:\n",
        "            return True\n",
        "        \n",
        "        return False\n",
        "    \n",
        "    def render(self, show_path=None):\n",
        "        \"\"\"Visualize the current state of the environment\"\"\"\n",
        "        # Create visualization grid\n",
        "        vis_grid = self.grid.copy()\n",
        "        \n",
        "        # Mark agent position\n",
        "        vis_grid[self.agent_pos[0], self.agent_pos[1]] = 'A'\n",
        "        \n",
        "        # Mark path if provided\n",
        "        if show_path:\n",
        "            for pos in show_path:\n",
        "                if pos != self.agent_pos and pos != self.start_pos and pos != self.goal_pos:\n",
        "                    vis_grid[pos[0], pos[1]] = '*'\n",
        "        \n",
        "        # Print grid\n",
        "        print(f'\\nGridWorld - {self.scenario.replace(\"_\", \" \").title()}')\n",
        "        print(f'Steps: {self.steps}/{self.max_steps}')\n",
        "        print(f'Equipment visited: {len(self.visited_equipment)}/{len(self.equipment_pos)}')\n",
        "        print(f'Checkpoints visited: {len(self.visited_checkpoints)}/{len(self.checkpoint_pos)}')\n",
        "        print(f'Hazard encounters: {self.hazard_encounters}')\n",
        "        print()\n",
        "        \n",
        "        for row in vis_grid:\n",
        "            print(' '.join(row))\n",
        "        \n",
        "        print('\\nLegend: S=Start, G=Goal, A=Agent, E=Equipment, C=Checkpoint, H=Hazard, #=Wall, .=Empty, *=Path')\n",
        "\n",
        "# Initialize GridWorld environment\n",
        "env = OilGasGridWorld(DATA_DIR / 'gridworld_config.json', scenario='facility_inspection')\n",
        "\n",
        "print(f'✓ GridWorld environment initialized')\n",
        "print(f'✓ Scenario: {env.scenario.replace(\"_\", \" \").title()}')\n",
        "print(f'✓ Grid size: {env.rows}x{env.cols}')\n",
        "print(f'✓ Actions: {len(env.actions)} ({[info[\"name\"] for info in env.actions.values()]})')\n",
        "print(f'✓ Equipment positions: {len(env.equipment_pos)}')\n",
        "print(f'✓ Hazard positions: {len(env.hazard_pos)}')\n",
        "print(f'✓ Checkpoint positions: {len(env.checkpoint_pos)}')\n",
        "\n",
        "# Display initial environment\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary & Validation\n",
        "\n",
        "This is a simplified version of Experiment 8 for testing. The complete implementation would include Q-Learning and DQN algorithms, training loops, and comprehensive evaluation.\n",
        "\n",
        "**Key Components Demonstrated:**\n",
        "\n",
        "- Reinforcement learning theory and GridWorld fundamentals\n",
        "- Oil & gas facility navigation with safety constraints\n",
        "- Multi-objective environment (equipment inspection, hazard avoidance)\n",
        "- Realistic facility scenarios (inspection, evacuation, maintenance)\n",
        "\n",
        "**Next Steps:**\n",
        "\n",
        "- Implement Q-Learning algorithm with Q-table\n",
        "- Create Deep Q-Network (DQN) with experience replay\n",
        "- Add training loops and convergence monitoring\n",
        "- Include policy visualization and performance analysis\n",
        "- Implement safety-aware reward shaping and evaluation metrics\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "O_G_Kernel",
      "language": "python",
      "name": "o_g_kernel"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
