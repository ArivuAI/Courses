{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experiment 2: Deep Neural Network for Classification\n",
        "\n",
        "**Course:** Introduction to Deep Learning | **Module:** Core Deep Learning Concepts\n",
        "\n",
        "---\n",
        "\n",
        "## Objective\n",
        "\n",
        "Write a program to demonstrate the working of a deep neural network for classification tasks using PyTorch, including architecture design, training, and evaluation.\n",
        "\n",
        "## Learning Outcomes\n",
        "\n",
        "By the end of this experiment, you will:\n",
        "\n",
        "1. Design and implement multi-layer neural network architectures\n",
        "2. Understand forward and backward propagation in deep networks\n",
        "3. Apply various activation functions and optimization techniques\n",
        "4. Implement proper training loops with validation and testing\n",
        "5. Analyze model performance using comprehensive metrics and visualizations\n",
        "\n",
        "## Background & Theory\n",
        "\n",
        "**Deep Neural Networks (DNNs)** are multi-layer perceptrons with multiple hidden layers that can learn complex non-linear patterns in data through hierarchical feature extraction.\n",
        "\n",
        "**Key Concepts:**\n",
        "\n",
        "- **Multi-layer Architecture:** Input ‚Üí Hidden Layers ‚Üí Output\n",
        "- **Activation Functions:** ReLU, Sigmoid, Tanh for non-linearity\n",
        "- **Backpropagation:** Gradient-based learning algorithm\n",
        "- **Loss Functions:** CrossEntropy for classification, MSE for regression\n",
        "- **Optimization:** SGD, Adam, RMSprop for parameter updates\n",
        "- **Regularization:** Dropout, Weight Decay to prevent overfitting\n",
        "\n",
        "**Mathematical Foundation:**\n",
        "\n",
        "- Forward pass: h^(l+1) = œÉ(W^(l)h^(l) + b^(l))\n",
        "- Loss function: L = -Œ£ y_i log(≈∑_i) (Cross-entropy)\n",
        "- Gradient descent: Œ∏ = Œ∏ - Œ±‚àáL(Œ∏)\n",
        "- Where œÉ is activation function, W weights, b biases, Œ± learning rate\n",
        "\n",
        "**Applications:**\n",
        "\n",
        "- Image classification and object recognition\n",
        "- Text classification and sentiment analysis\n",
        "- Medical diagnosis and drug discovery\n",
        "- Financial fraud detection and risk assessment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup & Data Preparation\n",
        "\n",
        "**What to Expect:** This section establishes the Python environment for deep neural network training and loads a comprehensive equipment classification dataset. We'll install all necessary packages, configure PyTorch for optimal performance, and prepare structured data for multi-class classification.\n",
        "\n",
        "**Process Overview:**\n",
        "\n",
        "1. **Package Installation:** Automatically install PyTorch, scikit-learn, and visualization libraries\n",
        "2. **Environment Configuration:** Set up device detection (CPU/GPU), random seeds, and styling\n",
        "3. **Dataset Loading:** Load equipment classification data with features and labels\n",
        "4. **Data Exploration:** Analyze dataset structure, class distribution, and feature characteristics\n",
        "5. **Data Preprocessing:** Scale features and prepare train/validation/test splits\n",
        "\n",
        "**Expected Outcome:** A fully configured environment with a loaded dataset containing ~1000 equipment samples across 5 classes, ready for deep neural network training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in /Users/pruthvirajv/Library/CloudStorage/OneDrive-ArivuAIInnovationsPrivateLimited(2)/Oil & Gas/GSSS-DeepLearning-Course/module1_dl_env/lib/python3.12/site-packages (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /Users/pruthvirajv/Library/CloudStorage/OneDrive-ArivuAIInnovationsPrivateLimited(2)/Oil & Gas/GSSS-DeepLearning-Course/module1_dl_env/lib/python3.12/site-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /Users/pruthvirajv/Library/CloudStorage/OneDrive-ArivuAIInnovationsPrivateLimited(2)/Oil & Gas/GSSS-DeepLearning-Course/module1_dl_env/lib/python3.12/site-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /Users/pruthvirajv/Library/CloudStorage/OneDrive-ArivuAIInnovationsPrivateLimited(2)/Oil & Gas/GSSS-DeepLearning-Course/module1_dl_env/lib/python3.12/site-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/pruthvirajv/Library/CloudStorage/OneDrive-ArivuAIInnovationsPrivateLimited(2)/Oil & Gas/GSSS-DeepLearning-Course/module1_dl_env/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
            "‚úì All packages installed and configured\n",
            "‚úì Random seeds set for reproducible results\n",
            "‚úì ArivuAI styling applied\n",
            "‚úì Data directory: data\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "import subprocess, sys\n",
        "packages = ['torch', 'numpy', 'matplotlib', 'pandas', 'scikit-learn', 'seaborn']\n",
        "for pkg in packages:\n",
        "    try: __import__(pkg)\n",
        "    except ImportError: subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg])\n",
        "\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "import numpy as np, pandas as pd\n",
        "import matplotlib.pyplot as plt, seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import json, os\n",
        "from pathlib import Path\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# ArivuAI styling\n",
        "plt.style.use('default')\n",
        "colors = {'primary': '#004E89', 'secondary': '#3DA5D9', 'accent': '#F1A208', 'dark': '#4F4F4F'}\n",
        "sns.set_palette([colors['primary'], colors['secondary'], colors['accent'], colors['dark']])\n",
        "\n",
        "# Data path detection\n",
        "DATA_DIR = Path('Expirements/data')\n",
        "if not DATA_DIR.exists():\n",
        "    DATA_DIR = Path('data')\n",
        "if not DATA_DIR.exists():\n",
        "    DATA_DIR = Path.cwd() / 'Expirements' / 'data'\n",
        "    DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print('‚úì All packages installed and configured')\n",
        "print('‚úì Random seeds set for reproducible results')\n",
        "print('‚úì ArivuAI styling applied')\n",
        "print(f'‚úì Data directory: {DATA_DIR}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset Creation & Loading\n",
        "\n",
        "**What to Expect:** This section creates a comprehensive synthetic dataset for oil & gas equipment classification. We'll generate realistic equipment data with 8 technical features across 5 equipment categories, simulating real-world industrial classification scenarios.\n",
        "\n",
        "**Process Overview:**\n",
        "\n",
        "1. **Equipment Categories:** Define 5 distinct equipment types (Drilling, Production, Processing, Safety, Transportation)\n",
        "2. **Feature Engineering:** Generate 8 realistic technical features (pressure, temperature, flow rate, power, weight, maintenance, age, efficiency)\n",
        "3. **Data Generation:** Create 1000 samples with category-specific feature distributions\n",
        "4. **Data Validation:** Verify dataset balance, feature ranges, and statistical properties\n",
        "5. **Exploratory Analysis:** Visualize feature distributions and class separability\n",
        "\n",
        "**Expected Outcome:** A balanced dataset with 1000 equipment samples (200 per class) featuring realistic technical specifications, ready for deep neural network training and evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Dataset creation function defined\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# SYNTHETIC DATASET CREATION FOR EQUIPMENT CLASSIFICATION\n",
        "# ============================================================================\n",
        "\n",
        "def create_equipment_dataset():\n",
        "    '''Create comprehensive synthetic oil & gas equipment classification dataset\n",
        "    \n",
        "    This function generates realistic equipment data with 8 technical features\n",
        "    across 5 equipment categories, simulating real-world industrial scenarios.\n",
        "    '''\n",
        "    print('üè≠ Creating synthetic oil & gas equipment dataset...')\n",
        "    \n",
        "    # Set random seed for reproducible dataset generation\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    # ========================================================================\n",
        "    # EQUIPMENT CATEGORY DEFINITIONS\n",
        "    # ========================================================================\n",
        "    \n",
        "    # Define 5 distinct equipment categories with unique characteristics\n",
        "    categories = {\n",
        "        0: 'Drilling_Equipment',        # High pressure, high power drilling systems\n",
        "        1: 'Production_Equipment',      # High efficiency production systems\n",
        "        2: 'Processing_Equipment',      # High temperature processing units\n",
        "        3: 'Safety_Equipment',          # Low power, high reliability safety systems\n",
        "        4: 'Transportation_Equipment'   # High flow rate transport systems\n",
        "    }\n",
        "    \n",
        "    print(f'  üìã Equipment categories: {list(categories.values())}')\n",
        "    \n",
        "    # ========================================================================\n",
        "    # FEATURE DEFINITIONS\n",
        "    # ========================================================================\n",
        "    \n",
        "    # Define 8 technical features that characterize each equipment type:\n",
        "    # 1. pressure_rating (PSI) - Operating pressure capacity\n",
        "    # 2. temperature_rating (¬∞C) - Operating temperature range\n",
        "    # 3. flow_rate (L/min) - Fluid flow capacity\n",
        "    # 4. power_consumption (kW) - Electrical power requirements\n",
        "    # 5. weight (kg) - Physical weight of equipment\n",
        "    # 6. maintenance_hours (hours/year) - Annual maintenance requirements\n",
        "    # 7. age_years (years) - Equipment age since installation\n",
        "    # 8. efficiency_percent (%) - Operational efficiency rating\n",
        "    \n",
        "    feature_names = [\n",
        "        'pressure_rating', 'temperature_rating', 'flow_rate', 'power_consumption',\n",
        "        'weight', 'maintenance_hours', 'age_years', 'efficiency_percent'\n",
        "    ]\n",
        "    \n",
        "    print(f'  üîß Features per equipment: {len(feature_names)}')\n",
        "    \n",
        "    # Initialize data storage\n",
        "    data = []           # Feature vectors for all samples\n",
        "    labels = []         # Class labels for all samples\n",
        "    \n",
        "    # ========================================================================\n",
        "    # CATEGORY-SPECIFIC DATA GENERATION\n",
        "    # ========================================================================\n",
        "    \n",
        "    print('\\n  üé≤ Generating samples for each equipment category...')\n",
        "    \n",
        "    # Generate samples for each equipment category with realistic distributions\n",
        "    for category_id, category_name in categories.items():\n",
        "        n_samples = 200  # Generate 200 samples per category for balanced dataset\n",
        "        print(f'    üìä {category_name}: {n_samples} samples')\n",
        "        \n",
        "        # ====================================================================\n",
        "        # DRILLING EQUIPMENT - High pressure, high power systems\n",
        "        # ====================================================================\n",
        "        if category_id == 0:  # Drilling Equipment\n",
        "            # Drilling equipment operates under extreme conditions\n",
        "            pressure = np.random.normal(5000, 1000, n_samples)      # Very high pressure (5000¬±1000 PSI)\n",
        "            temperature = np.random.normal(150, 30, n_samples)      # High temperature (150¬±30¬∞C)\n",
        "            flow_rate = np.random.normal(500, 100, n_samples)       # Moderate flow rate (500¬±100 L/min)\n",
        "            power = np.random.normal(2000, 400, n_samples)          # High power consumption (2000¬±400 kW)\n",
        "            weight = np.random.normal(15000, 3000, n_samples)       # Very heavy equipment (15000¬±3000 kg)\n",
        "            maintenance = np.random.normal(120, 20, n_samples)      # High maintenance (120¬±20 hours/year)\n",
        "            age = np.random.uniform(1, 15, n_samples)               # Age range 1-15 years\n",
        "            efficiency = np.random.normal(75, 10, n_samples)        # Moderate efficiency (75¬±10%)\n",
        "            \n",
        "        elif category_id == 1:  # Production Equipment\n",
        "            pressure = np.random.normal(3000, 800, n_samples)\n",
        "            temperature = np.random.normal(100, 25, n_samples)\n",
        "            flow_rate = np.random.normal(800, 150, n_samples)  # High flow\n",
        "            power = np.random.normal(1500, 300, n_samples)\n",
        "            weight = np.random.normal(8000, 2000, n_samples)\n",
        "            maintenance = np.random.normal(80, 15, n_samples)\n",
        "            age = np.random.uniform(2, 20, n_samples)\n",
        "            efficiency = np.random.normal(85, 8, n_samples)  # High efficiency\n",
        "            \n",
        "        elif category_id == 2:  # Processing Equipment\n",
        "            pressure = np.random.normal(2000, 500, n_samples)\n",
        "            temperature = np.random.normal(200, 40, n_samples)  # High temperature\n",
        "            flow_rate = np.random.normal(300, 80, n_samples)\n",
        "            power = np.random.normal(3000, 600, n_samples)  # Very high power\n",
        "            weight = np.random.normal(12000, 2500, n_samples)\n",
        "            maintenance = np.random.normal(150, 25, n_samples)  # High maintenance\n",
        "            age = np.random.uniform(3, 25, n_samples)\n",
        "            efficiency = np.random.normal(70, 12, n_samples)\n",
        "            \n",
        "        elif category_id == 3:  # Safety Equipment\n",
        "            pressure = np.random.normal(1000, 300, n_samples)  # Low pressure\n",
        "            temperature = np.random.normal(50, 15, n_samples)  # Low temperature\n",
        "            flow_rate = np.random.normal(100, 30, n_samples)  # Low flow\n",
        "            power = np.random.normal(500, 100, n_samples)  # Low power\n",
        "            weight = np.random.normal(2000, 500, n_samples)  # Light\n",
        "            maintenance = np.random.normal(40, 10, n_samples)  # Low maintenance\n",
        "            age = np.random.uniform(1, 10, n_samples)\n",
        "            efficiency = np.random.normal(95, 5, n_samples)  # Very high efficiency\n",
        "            \n",
        "        else:  # Transportation Equipment\n",
        "            pressure = np.random.normal(1500, 400, n_samples)\n",
        "            temperature = np.random.normal(80, 20, n_samples)\n",
        "            flow_rate = np.random.normal(1000, 200, n_samples)  # Very high flow\n",
        "            power = np.random.normal(1000, 200, n_samples)\n",
        "            weight = np.random.normal(5000, 1000, n_samples)\n",
        "            maintenance = np.random.normal(60, 12, n_samples)\n",
        "            age = np.random.uniform(2, 18, n_samples)\n",
        "            efficiency = np.random.normal(80, 10, n_samples)\n",
        "        \n",
        "        # Combine features\n",
        "        for i in range(n_samples):\n",
        "            features = [\n",
        "                max(0, pressure[i]),  # Ensure positive values\n",
        "                max(0, temperature[i]),\n",
        "                max(0, flow_rate[i]),\n",
        "                max(0, power[i]),\n",
        "                max(0, weight[i]),\n",
        "                max(0, maintenance[i]),\n",
        "                max(0, age[i]),\n",
        "                np.clip(efficiency[i], 0, 100)  # Efficiency 0-100%\n",
        "            ]\n",
        "            data.append(features)\n",
        "            labels.append(category_id)\n",
        "    \n",
        "    return np.array(data), np.array(labels), categories\n",
        "\n",
        "# This function is kept for reference but we'll use JSON data loading instead\n",
        "print('‚úì Dataset creation function defined')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loading & Preprocessing\n",
        "\n",
        "Load the equipment classification dataset and prepare it for neural network training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Dataset loaded from JSON file\n",
            "Dataset loaded:\n",
            "‚Ä¢ Samples: 1,000\n",
            "‚Ä¢ Features: 8 (['Pressure_Rating', 'Temperature_Rating', 'Flow_Rate', 'Power_Consumption', 'Weight', 'Maintenance_Hours', 'Age_Years', 'Efficiency_Percent'])\n",
            "‚Ä¢ Classes: 5 (['Drilling_Equipment', 'Production_Equipment', 'Processing_Equipment', 'Safety_Equipment', 'Transportation_Equipment'])\n",
            "‚Ä¢ Class distribution: [200 200 200 200 200]\n",
            "\n",
            "Data splits:\n",
            "‚Ä¢ Training: 600 samples\n",
            "‚Ä¢ Validation: 200 samples\n",
            "‚Ä¢ Test: 200 samples\n",
            "‚úì Data preprocessing completed\n",
            "‚úì PyTorch tensors created\n"
          ]
        }
      ],
      "source": [
        "# Load dataset from JSON file\n",
        "def load_equipment_data():\n",
        "    \"\"\"Load equipment classification data from JSON file\"\"\"\n",
        "    try:\n",
        "        with open(DATA_DIR / 'equipment_classification.json', 'r') as f:\n",
        "            dataset_info = json.load(f)\n",
        "        print('‚úì Dataset loaded from JSON file')\n",
        "    except FileNotFoundError:\n",
        "        print('Creating dataset from scratch...')\n",
        "        X, y, class_names = create_equipment_dataset()\n",
        "        return X, y, class_names, feature_names\n",
        "    \n",
        "    # Generate data based on JSON specifications\n",
        "    np.random.seed(42)\n",
        "    X, y = [], []\n",
        "    \n",
        "    for class_id, class_name in dataset_info['class_names'].items():\n",
        "        class_id = int(class_id)\n",
        "        equipment_type = class_name.lower()\n",
        "        \n",
        "        if equipment_type in dataset_info['sample_data']:\n",
        "            specs = dataset_info['sample_data'][equipment_type]\n",
        "            n_samples = dataset_info['metadata']['samples_per_class']\n",
        "            \n",
        "            for _ in range(n_samples):\n",
        "                # Generate features based on ranges\n",
        "                pressure = np.random.uniform(*specs['pressure_range'])\n",
        "                temperature = np.random.uniform(*specs['temperature_range'])\n",
        "                flow_rate = np.random.uniform(*specs['flow_rate_range'])\n",
        "                power = np.random.uniform(*specs['power_range'])\n",
        "                weight = np.random.uniform(*specs['weight_range'])\n",
        "                maintenance = np.random.uniform(*specs['maintenance_range'])\n",
        "                age = np.random.uniform(*specs['age_range'])\n",
        "                efficiency = np.random.uniform(*specs['efficiency_range'])\n",
        "                \n",
        "                X.append([pressure, temperature, flow_rate, power, weight, maintenance, age, efficiency])\n",
        "                y.append(class_id)\n",
        "    \n",
        "    return np.array(X), np.array(y), dataset_info['class_names'], dataset_info['feature_names']\n",
        "\n",
        "# Load and prepare data\n",
        "X, y, class_names, feature_names = load_equipment_data()\n",
        "\n",
        "print(f'Dataset loaded:')\n",
        "print(f'‚Ä¢ Samples: {X.shape[0]:,}')\n",
        "print(f'‚Ä¢ Features: {X.shape[1]} ({feature_names})')\n",
        "print(f'‚Ä¢ Classes: {len(class_names)} ({list(class_names.values())})')\n",
        "print(f'‚Ä¢ Class distribution: {np.bincount(y)}')\n",
        "\n",
        "# Data preprocessing\n",
        "# Split into train/validation/test sets\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f'\\nData splits:')\n",
        "print(f'‚Ä¢ Training: {X_train_scaled.shape[0]} samples')\n",
        "print(f'‚Ä¢ Validation: {X_val_scaled.shape[0]} samples')\n",
        "print(f'‚Ä¢ Test: {X_test_scaled.shape[0]} samples')\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
        "y_train_tensor = torch.LongTensor(y_train)\n",
        "X_val_tensor = torch.FloatTensor(X_val_scaled)\n",
        "y_val_tensor = torch.LongTensor(y_val)\n",
        "X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
        "y_test_tensor = torch.LongTensor(y_test)\n",
        "\n",
        "print('‚úì Data preprocessing completed')\n",
        "print('‚úì PyTorch tensors created')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deep Neural Network Architecture\n",
        "\n",
        "Design and implement a multi-layer neural network for equipment classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deep Neural Network Architecture:\n",
            "‚Ä¢ Input dimension: 8\n",
            "‚Ä¢ Hidden layers: [128, 64, 32]\n",
            "‚Ä¢ Output dimension: 5\n",
            "‚Ä¢ Dropout rate: 0.3\n",
            "‚Ä¢ Total parameters: 12,101\n",
            "\n",
            "Model Architecture:\n",
            "  network.0: Linear(in_features=8, out_features=128, bias=True)\n",
            "  network.1: BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  network.2: ReLU()\n",
            "  network.3: Dropout(p=0.3, inplace=False)\n",
            "  network.4: Linear(in_features=128, out_features=64, bias=True)\n",
            "  network.5: BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  network.6: ReLU()\n",
            "  network.7: Dropout(p=0.3, inplace=False)\n",
            "  network.8: Linear(in_features=64, out_features=32, bias=True)\n",
            "  network.9: BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  network.10: ReLU()\n",
            "  network.11: Dropout(p=0.3, inplace=False)\n",
            "  network.12: Linear(in_features=32, out_features=5, bias=True)\n"
          ]
        }
      ],
      "source": [
        "class DeepClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims, output_dim, dropout_rate=0.3):\n",
        "        super(DeepClassifier, self).__init__()\n",
        "        \n",
        "        # Build layers dynamically\n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "        \n",
        "        for hidden_dim in hidden_dims:\n",
        "            layers.extend([\n",
        "                nn.Linear(prev_dim, hidden_dim),\n",
        "                nn.BatchNorm1d(hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(dropout_rate)\n",
        "            ])\n",
        "            prev_dim = hidden_dim\n",
        "        \n",
        "        # Output layer\n",
        "        layers.append(nn.Linear(prev_dim, output_dim))\n",
        "        \n",
        "        self.network = nn.Sequential(*layers)\n",
        "        \n",
        "        # Initialize weights\n",
        "        self.apply(self._init_weights)\n",
        "    \n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            nn.init.xavier_uniform_(module.weight)\n",
        "            nn.init.constant_(module.bias, 0)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "# Model configuration\n",
        "input_dim = X_train_scaled.shape[1]\n",
        "hidden_dims = [128, 64, 32]  # Three hidden layers\n",
        "output_dim = len(class_names)\n",
        "dropout_rate = 0.3\n",
        "\n",
        "# Create model\n",
        "model = DeepClassifier(input_dim, hidden_dims, output_dim, dropout_rate)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=10, factor=0.5)\n",
        "\n",
        "print(f'Deep Neural Network Architecture:')\n",
        "print(f'‚Ä¢ Input dimension: {input_dim}')\n",
        "print(f'‚Ä¢ Hidden layers: {hidden_dims}')\n",
        "print(f'‚Ä¢ Output dimension: {output_dim}')\n",
        "print(f'‚Ä¢ Dropout rate: {dropout_rate}')\n",
        "print(f'‚Ä¢ Total parameters: {sum(p.numel() for p in model.parameters()):,}')\n",
        "\n",
        "# Model summary\n",
        "print('\\nModel Architecture:')\n",
        "for i, (name, module) in enumerate(model.named_modules()):\n",
        "    if isinstance(module, (nn.Linear, nn.BatchNorm1d, nn.ReLU, nn.Dropout)):\n",
        "        print(f'  {name}: {module}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training & Validation\n",
        "\n",
        "Train the deep neural network with proper validation and monitoring.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Deep Neural Network...\n",
            "Epoch [20/100]:\n",
            "  Train Loss: 0.6047, Train Acc: 0.8683\n",
            "  Val Loss: 0.5834, Val Acc: 0.9950\n",
            "Epoch [40/100]:\n",
            "  Train Loss: 0.3311, Train Acc: 0.9600\n",
            "  Val Loss: 0.2066, Val Acc: 1.0000\n",
            "Epoch [60/100]:\n",
            "  Train Loss: 0.2236, Train Acc: 0.9783\n",
            "  Val Loss: 0.1051, Val Acc: 1.0000\n",
            "Epoch [80/100]:\n",
            "  Train Loss: 0.1605, Train Acc: 0.9900\n",
            "  Val Loss: 0.0635, Val Acc: 1.0000\n",
            "Epoch [100/100]:\n",
            "  Train Loss: 0.1368, Train Acc: 0.9883\n",
            "  Val Loss: 0.0414, Val Acc: 1.0000\n",
            "\n",
            "‚úÖ Training completed!\n",
            "Final training accuracy: 0.9883\n",
            "Final validation accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "def train_model(model, train_data, val_data, criterion, optimizer, scheduler, num_epochs=100):\n",
        "    \"\"\"Train the deep neural network with validation monitoring\"\"\"\n",
        "    \n",
        "    X_train, y_train = train_data\n",
        "    X_val, y_val = val_data\n",
        "    \n",
        "    train_losses, val_losses = [], []\n",
        "    train_accuracies, val_accuracies = [], []\n",
        "    \n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    early_stopping_patience = 20\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        train_outputs = model(X_train)\n",
        "        train_loss = criterion(train_outputs, y_train)\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Calculate training accuracy\n",
        "        _, train_predicted = torch.max(train_outputs.data, 1)\n",
        "        train_accuracy = (train_predicted == y_train).float().mean().item()\n",
        "        \n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_outputs = model(X_val)\n",
        "            val_loss = criterion(val_outputs, y_val)\n",
        "            \n",
        "            _, val_predicted = torch.max(val_outputs.data, 1)\n",
        "            val_accuracy = (val_predicted == y_val).float().mean().item()\n",
        "        \n",
        "        # Store metrics\n",
        "        train_losses.append(train_loss.item())\n",
        "        val_losses.append(val_loss.item())\n",
        "        train_accuracies.append(train_accuracy)\n",
        "        val_accuracies.append(val_accuracy)\n",
        "        \n",
        "        # Learning rate scheduling\n",
        "        scheduler.step(val_loss)\n",
        "        \n",
        "        # Early stopping\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "            # Save best model\n",
        "            torch.save(model.state_dict(), DATA_DIR / 'best_model.pth')\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "        \n",
        "        if patience_counter >= early_stopping_patience:\n",
        "            print(f'Early stopping at epoch {epoch+1}')\n",
        "            break\n",
        "        \n",
        "        # Print progress\n",
        "        if (epoch + 1) % 20 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}]:')\n",
        "            print(f'  Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}')\n",
        "            print(f'  Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}')\n",
        "    \n",
        "    return train_losses, val_losses, train_accuracies, val_accuracies\n",
        "\n",
        "# Train the model\n",
        "print('Training Deep Neural Network...')\n",
        "train_data = (X_train_tensor, y_train_tensor)\n",
        "val_data = (X_val_tensor, y_val_tensor)\n",
        "\n",
        "train_losses, val_losses, train_accs, val_accs = train_model(\n",
        "    model, train_data, val_data, criterion, optimizer, scheduler, num_epochs=100\n",
        ")\n",
        "\n",
        "print('\\n‚úÖ Training completed!')\n",
        "print(f'Final training accuracy: {train_accs[-1]:.4f}')\n",
        "print(f'Final validation accuracy: {val_accs[-1]:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary & Validation\n",
        "\n",
        "This experiment successfully demonstrates deep neural network implementation for multi-class equipment classification.\n",
        "\n",
        "**‚úÖ Key Components Implemented:**\n",
        "\n",
        "- **Comprehensive Dataset:** 1000 equipment samples across 5 categories with 8 realistic technical features\n",
        "- **Deep Neural Network:** Multi-layer architecture with ReLU activation, batch normalization, and dropout\n",
        "- **Training Pipeline:** Complete training loop with validation monitoring and learning rate scheduling\n",
        "- **Data Preprocessing:** Feature scaling and proper train/validation/test splits\n",
        "- **Performance Evaluation:** Accuracy tracking and loss monitoring throughout training\n",
        "\n",
        "**üß† Neural Network Architecture:**\n",
        "\n",
        "- **Input Layer:** 8 technical features (pressure, temperature, flow rate, power, weight, maintenance, age, efficiency)\n",
        "- **Hidden Layers:** Multiple fully connected layers with progressive dimensionality reduction\n",
        "- **Activation Functions:** ReLU for non-linearity and improved gradient flow\n",
        "- **Regularization:** Batch normalization and dropout to prevent overfitting\n",
        "- **Output Layer:** 5-class classification with softmax activation\n",
        "\n",
        "**üìä Results Achieved:**\n",
        "\n",
        "- Successfully trained deep neural network for equipment classification\n",
        "- Model learns to distinguish between 5 different equipment categories\n",
        "- Validation accuracy tracks training accuracy indicating good generalization\n",
        "- Learning rate scheduling helps achieve stable convergence\n",
        "\n",
        "**üîç Technical Insights:**\n",
        "\n",
        "- **Feature Engineering:** Realistic equipment specifications create meaningful class separability\n",
        "- **Architecture Design:** Progressive layer sizing allows hierarchical feature learning\n",
        "- **Training Dynamics:** Proper initialization and optimization lead to stable learning\n",
        "- **Regularization Effects:** Dropout and batch normalization prevent overfitting on small dataset\n",
        "\n",
        "**üöÄ Real-world Applications:**\n",
        "\n",
        "- **Equipment Classification:** Automated categorization of industrial equipment\n",
        "- **Predictive Maintenance:** Equipment type-specific maintenance scheduling\n",
        "- **Inventory Management:** Automated equipment identification and cataloging\n",
        "- **Quality Control:** Classification-based equipment inspection and validation\n",
        "- **Resource Planning:** Equipment type-based resource allocation and planning\n",
        "\n",
        "**üìà Next Steps:**\n",
        "\n",
        "- Implement cross-validation for more robust performance estimation\n",
        "- Add feature importance analysis to understand key classification factors\n",
        "- Experiment with different architectures (deeper networks, different activation functions)\n",
        "- Include ensemble methods for improved classification accuracy\n",
        "- Deploy model for real-time equipment classification in production environments\n",
        "\n",
        "This experiment provides a comprehensive foundation for understanding deep neural networks and their applications in industrial equipment classification tasks.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "module1_dl_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
