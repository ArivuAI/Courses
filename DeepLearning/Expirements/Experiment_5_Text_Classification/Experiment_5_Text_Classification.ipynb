{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experiment 5: Deep Learning for Oil & Gas Text Classification\n",
        "**Course:** Introduction to Deep Learning | **Module:** Natural Language Processing\n",
        "\n",
        "---\n",
        "\n",
        "## Objective\n",
        "Design and implement deep learning models for classifying oil & gas industry reports and documents using neural networks, word embeddings, and sequence modeling techniques.\n",
        "\n",
        "## Learning Outcomes\n",
        "By the end of this experiment, you will:\n",
        "1. Understand text preprocessing and tokenization for deep learning\n",
        "2. Implement word embeddings and sequence models for text classification\n",
        "3. Build and train LSTM/GRU networks for document classification\n",
        "4. Apply attention mechanisms and transformer concepts\n",
        "5. Evaluate text classification models and interpret results\n",
        "\n",
        "## Background & Theory\n",
        "\n",
        "**Text Classification** is the task of automatically categorizing text documents into predefined classes. Deep learning approaches use neural networks to learn hierarchical representations of text for improved classification accuracy.\n",
        "\n",
        "**Key Components:**\n",
        "- **Tokenization:** Converting text into numerical tokens for neural network processing\n",
        "- **Word Embeddings:** Dense vector representations capturing semantic relationships\n",
        "- **Sequence Models:** RNNs, LSTMs, GRUs for processing sequential text data\n",
        "- **Attention Mechanisms:** Focusing on relevant parts of input sequences\n",
        "- **Classification Head:** Final layers mapping representations to class probabilities\n",
        "\n",
        "**Mathematical Foundation:**\n",
        "- Word embedding: w_i â†’ e_i âˆˆ R^d where d is embedding dimension\n",
        "- LSTM cell: f_t = Ïƒ(W_fÂ·[h_{t-1}, x_t] + b_f), i_t = Ïƒ(W_iÂ·[h_{t-1}, x_t] + b_i)\n",
        "- Attention: Î±_i = softmax(e_i), context = Î£Î±_i h_i\n",
        "- Classification: P(y|x) = softmax(W_c h + b_c)\n",
        "\n",
        "**Applications in Oil & Gas:**\n",
        "- Automated classification of incident reports and safety documents\n",
        "- Maintenance report categorization for predictive analytics\n",
        "- Regulatory compliance document processing\n",
        "- Production report analysis and trend identification\n",
        "- Knowledge management and document retrieval systems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup & Dependencies\n",
        "\n",
        "**What to Expect:** This section establishes the Python environment for deep learning-based text classification. We'll install all necessary packages including PyTorch for neural networks, NLTK for text processing, and scikit-learn for evaluation metrics.\n",
        "\n",
        "**Process Overview:**\n",
        "1. **Package Installation:** Automatically install PyTorch, NLTK, transformers, and scientific computing libraries\n",
        "2. **Environment Configuration:** Set up device detection (CPU/GPU), random seeds for reproducibility\n",
        "3. **NLTK Data Download:** Download required tokenizers and language resources\n",
        "4. **Styling Setup:** Apply ArivuAI color scheme for consistent visualizations\n",
        "5. **Validation:** Confirm all dependencies are properly installed and configured\n",
        "\n",
        "**Expected Outcome:** A fully configured environment ready for text classification with neural networks, including all NLP preprocessing tools and deep learning frameworks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“¦ Checking and installing required packages...\n",
            "  âœ“ torch already installed\n",
            "  âœ“ numpy already installed\n",
            "  âœ“ matplotlib already installed\n",
            "  âœ“ pandas already installed\n",
            "  ðŸ“¥ Installing scikit-learn...\n",
            "  âœ“ nltk already installed\n",
            "  âœ“ wordcloud already installed\n",
            "\n",
            "ðŸ“š Setting up NLTK data...\n",
            "  âœ“ Punkt tokenizer already available\n",
            "  âœ“ Stopwords corpus already available\n",
            "\n",
            "ðŸŽ² Setting random seeds for reproducible results...\n",
            "\n",
            "ðŸ–¥ï¸ Configuring compute device...\n",
            "  âœ“ Using device: cpu\n",
            "  â„¹ï¸ GPU not available, using CPU\n",
            "\n",
            "ðŸ“‚ Setting up data directories...\n",
            "  ðŸ“ Using local path: data\n",
            "\n",
            "ðŸŽ¨ Applying ArivuAI styling...\n",
            "\n",
            "âœ… Environment setup complete!\n",
            "  âœ“ All packages installed and configured\n",
            "  âœ“ Random seeds set for reproducible results\n",
            "  âœ“ Device configured for optimal performance\n",
            "  âœ“ Data directories established\n",
            "  âœ“ ArivuAI styling applied\n",
            "  âœ“ PyTorch version: 2.8.0+cpu\n",
            "  âœ“ Ready for text classification experiments!\n",
            "âœ“ PyTorch version: 2.8.0+cpu\n",
            "âœ“ Device: cpu\n",
            "âœ“ Data directory: d:\\Suni Files\\AI Code Base\\Oil and Gas\\Oil and Gas Pruthvi College Course Material\\Updated\\Expirements\\Experiment_5_Text_Classification\\data\n",
            "âœ“ All packages installed and configured\n",
            "âœ“ Random seeds set for reproducible results\n",
            "âœ“ ArivuAI styling applied\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# PACKAGE INSTALLATION AND ENVIRONMENT SETUP\n",
        "# ============================================================================\n",
        "\n",
        "# Install required packages automatically if not present\n",
        "import subprocess, sys\n",
        "packages = ['torch', 'numpy', 'matplotlib', 'pandas', 'scikit-learn', 'nltk', 'wordcloud']\n",
        "\n",
        "print('ðŸ“¦ Checking and installing required packages...')\n",
        "for pkg in packages:\n",
        "    try: \n",
        "        __import__(pkg.replace('-', '_').lower())\n",
        "        print(f'  âœ“ {pkg} already installed')\n",
        "    except ImportError: \n",
        "        print(f'  ðŸ“¥ Installing {pkg}...')\n",
        "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg])\n",
        "\n",
        "# ============================================================================\n",
        "# CORE LIBRARY IMPORTS\n",
        "# ============================================================================\n",
        "\n",
        "# Deep learning framework imports\n",
        "import torch                    # Main PyTorch library for tensor operations\n",
        "import torch.nn as nn          # Neural network modules and loss functions\n",
        "import torch.optim as optim    # Optimization algorithms (Adam, SGD, etc.)\n",
        "import torch.nn.functional as F # Functional interface for neural network operations\n",
        "\n",
        "# PyTorch data handling utilities\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset  # Data loading utilities\n",
        "from torch.nn.utils.rnn import pad_sequence                      # Sequence padding for RNNs\n",
        "\n",
        "# Scientific computing and data manipulation\n",
        "import numpy as np             # Numerical computing with arrays\n",
        "import pandas as pd            # Data manipulation and analysis\n",
        "import matplotlib.pyplot as plt # Plotting and visualization\n",
        "\n",
        "# Machine learning utilities\n",
        "from sklearn.model_selection import train_test_split  # Data splitting\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score  # Evaluation metrics\n",
        "\n",
        "# Data structures and utilities\n",
        "from collections import Counter, defaultdict  # Efficient counting and dictionaries\n",
        "import json, random, re, time  # JSON processing, random numbers, regex, timing\n",
        "from pathlib import Path       # Path manipulation\n",
        "\n",
        "# Natural language processing\n",
        "import nltk                    # Natural Language Toolkit\n",
        "from wordcloud import WordCloud  # Word cloud generation for visualization\n",
        "\n",
        "# ============================================================================\n",
        "# NLTK DATA DOWNLOAD\n",
        "# ============================================================================\n",
        "\n",
        "print('\\nðŸ“š Setting up NLTK data...')\n",
        "# Download required NLTK datasets if not already present\n",
        "try: \n",
        "    nltk.data.find('tokenizers/punkt')  # Sentence tokenizer\n",
        "    print('  âœ“ Punkt tokenizer already available')\n",
        "except LookupError: \n",
        "    print('  ðŸ“¥ Downloading punkt tokenizer...')\n",
        "    nltk.download('punkt')\n",
        "\n",
        "try: \n",
        "    nltk.data.find('corpora/stopwords')  # Common stop words\n",
        "    print('  âœ“ Stopwords corpus already available')\n",
        "except LookupError: \n",
        "    print('  ðŸ“¥ Downloading stopwords corpus...')\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "# Import NLTK components after ensuring data is available\n",
        "from nltk.tokenize import word_tokenize  # Word tokenization\n",
        "from nltk.corpus import stopwords        # Stop words list\n",
        "\n",
        "# ============================================================================\n",
        "# REPRODUCIBILITY SETUP\n",
        "# ============================================================================\n",
        "\n",
        "print('\\nðŸŽ² Setting random seeds for reproducible results...')\n",
        "# Set random seeds for all libraries to ensure reproducible results\n",
        "RANDOM_SEED = 42\n",
        "torch.manual_seed(RANDOM_SEED)      # PyTorch random number generator\n",
        "np.random.seed(RANDOM_SEED)         # NumPy random number generator\n",
        "random.seed(RANDOM_SEED)            # Python built-in random module\n",
        "\n",
        "# Set PyTorch to deterministic mode for reproducibility\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(RANDOM_SEED)\n",
        "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
        "\n",
        "# ============================================================================\n",
        "# DEVICE CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "print('\\nðŸ–¥ï¸ Configuring compute device...')\n",
        "# Configure device for optimal performance (GPU if available, otherwise CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'  âœ“ Using device: {device}')\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f'  âœ“ GPU: {torch.cuda.get_device_name(0)}')\n",
        "    print(f'  âœ“ CUDA version: {torch.version.cuda}')\n",
        "else:\n",
        "    print('  â„¹ï¸ GPU not available, using CPU')\n",
        "\n",
        "# ============================================================================\n",
        "# DATA DIRECTORY SETUP\n",
        "# ============================================================================\n",
        "\n",
        "print('\\nðŸ“‚ Setting up data directories...')\n",
        "# Flexible path detection to handle different execution contexts\n",
        "DATA_DIR = Path('data')\n",
        "if not DATA_DIR.exists():\n",
        "    DATA_DIR = Path('Expirements/Experiment_5_Text_Classification/data')\n",
        "    print(f'  ðŸ“ Using full path: {DATA_DIR}')\n",
        "else:\n",
        "    print(f'  ðŸ“ Using local path: {DATA_DIR}')\n",
        "\n",
        "# ============================================================================\n",
        "# VISUALIZATION STYLING\n",
        "# ============================================================================\n",
        "\n",
        "print('\\nðŸŽ¨ Applying ArivuAI styling...')\n",
        "# Configure matplotlib for consistent, professional visualizations\n",
        "plt.style.use('default')  # Start with clean default style\n",
        "\n",
        "# ArivuAI color palette for consistent branding\n",
        "colors = {\n",
        "    'primary': '#004E89',    # Deep blue for main elements\n",
        "    'secondary': '#3DA5D9',  # Light blue for secondary elements\n",
        "    'accent': '#F1A208',     # Orange for highlights and accents\n",
        "    'dark': '#4F4F4F'        # Dark gray for text and borders\n",
        "}\n",
        "\n",
        "# Set default figure parameters\n",
        "plt.rcParams['figure.figsize'] = (12, 8)  # Default figure size for text analysis\n",
        "plt.rcParams['font.size'] = 11             # Default font size\n",
        "\n",
        "# ============================================================================\n",
        "# SETUP VALIDATION\n",
        "# ============================================================================\n",
        "\n",
        "print('\\nâœ… Environment setup complete!')\n",
        "print('  âœ“ All packages installed and configured')\n",
        "print('  âœ“ Random seeds set for reproducible results')\n",
        "print('  âœ“ Device configured for optimal performance')\n",
        "print('  âœ“ Data directories established')\n",
        "print('  âœ“ ArivuAI styling applied')\n",
        "print(f'  âœ“ PyTorch version: {torch.__version__}')\n",
        "print('  âœ“ Ready for text classification experiments!')\n",
        "if not DATA_DIR.exists():\n",
        "    DATA_DIR = Path('Expirements/data')\n",
        "if not DATA_DIR.exists():\n",
        "    DATA_DIR = Path('.')\n",
        "    print('Warning: Using current directory for data')\n",
        "\n",
        "# ArivuAI styling\n",
        "plt.style.use('default')\n",
        "colors = {'primary': '#004E89', 'secondary': '#3DA5D9', 'accent': '#F1A208', 'dark': '#4F4F4F'}\n",
        "\n",
        "print(f'âœ“ PyTorch version: {torch.__version__}')\n",
        "print(f'âœ“ Device: {device}')\n",
        "print(f'âœ“ Data directory: {DATA_DIR.absolute()}')\n",
        "print('âœ“ All packages installed and configured')\n",
        "print('âœ“ Random seeds set for reproducible results')\n",
        "print('âœ“ ArivuAI styling applied')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text Dataset Generation & Preprocessing\n",
        "Create and preprocess oil & gas industry text data for classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Configuration loaded from JSON\n",
            "âœ“ Text dataset generated:\n",
            "â€¢ Total samples: 1,000\n",
            "â€¢ Categories: 5\n",
            "â€¢ Category names: ['Safety_Incident', 'Equipment_Maintenance', 'Production_Report', 'Environmental_Compliance', 'Operational_Update']\n",
            "â€¢ Sample text length: 25.2 words\n",
            "\n",
            "Sample texts:\n",
            "1. [Safety_Incident] INCIDENT: Gas leak detected at wellhead station 7 during routine inspection. Emergency shutdown proc...\n",
            "2. [Safety_Incident] URGENT: Near miss incident involving crane operations at offshore platform. Load swing occurred due ...\n",
            "3. [Safety_Incident] URGENT: Hydrogen sulfide exposure risk identified during well servicing operations. Area evacuated a...\n"
          ]
        }
      ],
      "source": [
        "class TextDataGenerator:\n",
        "    def __init__(self, config_path):\n",
        "        \"\"\"Initialize text data generator with configuration\"\"\"\n",
        "        try:\n",
        "            with open(config_path, 'r') as f:\n",
        "                self.config = json.load(f)\n",
        "            print('âœ“ Configuration loaded from JSON')\n",
        "        except FileNotFoundError:\n",
        "            print('Creating default configuration...')\n",
        "            self.config = self._create_default_config()\n",
        "        \n",
        "        self.categories = self.config['categories']\n",
        "        self.sample_texts = self.config['sample_texts']\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "    \n",
        "    def _create_default_config(self):\n",
        "        \"\"\"Create default configuration if JSON file not found\"\"\"\n",
        "        return {\n",
        "            'categories': {'0': 'Safety_Incident', '1': 'Equipment_Maintenance', '2': 'Production_Report', '3': 'Environmental_Compliance', '4': 'Operational_Update'},\n",
        "            'sample_texts': {'Safety_Incident': ['Gas leak detected at facility'], 'Equipment_Maintenance': ['Pump maintenance completed']},\n",
        "            'text_statistics': {'samples_per_category': 200}\n",
        "        }\n",
        "    \n",
        "    def generate_expanded_dataset(self, samples_per_category=200):\n",
        "        \"\"\"Generate expanded dataset by creating variations of sample texts\"\"\"\n",
        "        texts = []\n",
        "        labels = []\n",
        "        \n",
        "        for category_id, category_name in self.categories.items():\n",
        "            category_id = int(category_id)\n",
        "            base_texts = self.sample_texts.get(category_name, [])\n",
        "            \n",
        "            # Generate variations of base texts\n",
        "            for i in range(samples_per_category):\n",
        "                if base_texts:\n",
        "                    # Select base text and create variation\n",
        "                    base_text = random.choice(base_texts)\n",
        "                    varied_text = self._create_text_variation(base_text, category_name)\n",
        "                    texts.append(varied_text)\n",
        "                    labels.append(category_id)\n",
        "                else:\n",
        "                    # Fallback generic text\n",
        "                    generic_text = f\"Report related to {category_name.lower().replace('_', ' ')} activities and operations.\"\n",
        "                    texts.append(generic_text)\n",
        "                    labels.append(category_id)\n",
        "        \n",
        "        return texts, labels\n",
        "    \n",
        "    def _create_text_variation(self, base_text, category):\n",
        "        \"\"\"Create variations of base text while maintaining category characteristics\"\"\"\n",
        "        # Simple variation techniques\n",
        "        variations = [\n",
        "            base_text,  # Original\n",
        "            base_text.replace('completed', 'finished'),\n",
        "            base_text.replace('detected', 'identified'),\n",
        "            base_text.replace('scheduled', 'planned'),\n",
        "            base_text.replace('required', 'needed'),\n",
        "        ]\n",
        "        \n",
        "        # Add category-specific prefixes/suffixes\n",
        "        if category == 'Safety_Incident':\n",
        "            prefixes = ['URGENT: ', 'ALERT: ', 'INCIDENT: ', '']\n",
        "            suffixes = [' Immediate action required.', ' Safety protocols activated.', ' Investigation ongoing.', '']\n",
        "        elif category == 'Equipment_Maintenance':\n",
        "            prefixes = ['MAINTENANCE: ', 'SERVICE: ', 'REPAIR: ', '']\n",
        "            suffixes = [' Work order completed.', ' Equipment operational.', ' Next service due in 30 days.', '']\n",
        "        else:\n",
        "            prefixes = ['REPORT: ', 'UPDATE: ', 'STATUS: ', '']\n",
        "            suffixes = [' End of report.', ' Status confirmed.', ' Documentation updated.', '']\n",
        "        \n",
        "        varied_text = random.choice(variations)\n",
        "        prefix = random.choice(prefixes)\n",
        "        suffix = random.choice(suffixes)\n",
        "        \n",
        "        return prefix + varied_text + suffix\n",
        "    \n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"Preprocess text for neural network training\"\"\"\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "        \n",
        "        # Remove special characters and digits\n",
        "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "        \n",
        "        # Tokenize\n",
        "        tokens = word_tokenize(text)\n",
        "        \n",
        "        # Remove stopwords and short tokens\n",
        "        tokens = [token for token in tokens if token not in self.stop_words and len(token) > 2]\n",
        "        \n",
        "        return tokens\n",
        "\n",
        "# Initialize generator and create dataset\n",
        "generator = TextDataGenerator(DATA_DIR / 'oil_gas_reports.json')\n",
        "texts, labels = generator.generate_expanded_dataset(samples_per_category=200)\n",
        "\n",
        "print(f'âœ“ Text dataset generated:')\n",
        "print(f'â€¢ Total samples: {len(texts):,}')\n",
        "print(f'â€¢ Categories: {len(generator.categories)}')\n",
        "print(f'â€¢ Category names: {list(generator.categories.values())}')\n",
        "print(f'â€¢ Sample text length: {np.mean([len(text.split()) for text in texts]):.1f} words')\n",
        "\n",
        "# Show sample texts\n",
        "print('\\nSample texts:')\n",
        "for i in range(3):\n",
        "    print(f'{i+1}. [{generator.categories[str(labels[i])]}] {texts[i][:100]}...')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary & Validation\n",
        "This is a simplified version of Experiment 5 for testing. The complete implementation would include LSTM/GRU models, attention mechanisms, and comprehensive evaluation.\n",
        "\n",
        "**Key Components Demonstrated:**\n",
        "- Text classification theory and NLP foundations\n",
        "- Oil & gas industry text dataset with 5 categories\n",
        "- Text preprocessing and tokenization pipeline\n",
        "- Synthetic text generation with realistic variations\n",
        "\n",
        "**Next Steps:**\n",
        "- Implement vocabulary building and word embeddings\n",
        "- Create LSTM/GRU sequence models for classification\n",
        "- Add attention mechanisms and transformer components\n",
        "- Include training loops with validation monitoring\n",
        "- Implement comprehensive evaluation and text analysis"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "O_G_Kernel",
      "language": "python",
      "name": "o_g_kernel"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
