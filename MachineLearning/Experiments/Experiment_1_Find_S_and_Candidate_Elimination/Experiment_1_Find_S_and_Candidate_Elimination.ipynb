{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1: Find-S and Candidate Elimination Algorithms\n",
    "\n",
    "**Course:** Arivu AI Machine Learning Course | **Module:** Module 1 - Introduction & Concept Learning\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Objective\n",
    "\n",
    "In this hands-on experiment, you will implement and understand two fundamental concept learning algorithms:\n",
    "1. **Find-S Algorithm** - Finding the most specific hypothesis consistent with positive examples\n",
    "2. **Candidate Elimination Algorithm** - Maintaining a version space of all consistent hypotheses\n",
    "\n",
    "By the end of this experiment, you'll be able to build systems that learn patterns from examples‚Äîthe foundation of all machine learning!\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Learning Outcomes\n",
    "\n",
    "By the end of this experiment, you will:\n",
    "\n",
    "1. **Understand Concept Learning**: Grasp how machines learn concepts from examples (like humans do!)\n",
    "2. **Implement Find-S Algorithm**: Build a system that finds the most specific hypothesis from positive examples\n",
    "3. **Implement Candidate Elimination**: Create a version space that tracks all possible hypotheses\n",
    "4. **Analyze Hypothesis Spaces**: Understand how hypotheses evolve as we see more examples\n",
    "5. **Apply to Real Problems**: Use these algorithms to solve practical classification problems\n",
    "\n",
    "---\n",
    "\n",
    "## ü§î What Are We Actually Doing? (Simple Explanation)\n",
    "\n",
    "**In Simple Words:**\n",
    "\n",
    "Imagine you're teaching a child when it's good to play outside. You show them examples:\n",
    "- ‚úÖ \"Sunny and warm? Yes, go play!\"\n",
    "- ‚úÖ \"Sunny and warm again? Yes, play!\"\n",
    "- ‚ùå \"Rainy and cold? No, stay inside.\"\n",
    "\n",
    "After seeing a few examples, the child learns the pattern: **\"Play outside when it's sunny and warm.\"**\n",
    "\n",
    "This is exactly what our algorithms do! They learn patterns (called **hypotheses**) from examples.\n",
    "\n",
    "### üìÅ Understanding the Data/Problem\n",
    "\n",
    "**Our Dataset: Outdoor Activity Recommendation**\n",
    "\n",
    "We have data about weather conditions and whether someone enjoyed outdoor sports:\n",
    "\n",
    "| Sky | AirTemp | Humidity | Wind | Water | Forecast | EnjoySport? |\n",
    "|-----|---------|----------|------|-------|----------|-------------|\n",
    "| Sunny | Warm | Normal | Strong | Warm | Same | ‚úÖ Yes |\n",
    "| Rainy | Cold | High | Strong | Warm | Change | ‚ùå No |\n",
    "| Sunny | Warm | High | Strong | Warm | Same | ‚úÖ Yes |\n",
    "\n",
    "**The Goal:** Learn the pattern that determines when someone enjoys outdoor sports!\n",
    "\n",
    "**Think of it like:**\n",
    "- **Attributes** = Features we observe (Sky, Temperature, etc.)\n",
    "- **Hypothesis** = A rule we're learning (\"Enjoy sports when Sky=Sunny AND AirTemp=Warm\")\n",
    "- **Version Space** = All possible rules that fit the data we've seen so far\n",
    "\n",
    "### üéØ Real-World Impact\n",
    "\n",
    "**‚ùå Without This Solution:**\n",
    "- Manual rule creation for every scenario (time-consuming!)\n",
    "- Rules might miss important patterns\n",
    "- Cannot adapt to new data automatically\n",
    "- Inconsistent recommendations across users\n",
    "\n",
    "**‚úÖ With This Solution:**\n",
    "- Automatically learns patterns from user behavior\n",
    "- Adapts as more data becomes available\n",
    "- Consistent, data-driven recommendations\n",
    "- Saves hundreds of hours of manual rule engineering\n",
    "\n",
    "### üíº Industry Applications\n",
    "\n",
    "These concept learning algorithms are the foundation for:\n",
    "\n",
    "1. **Recommendation Systems**\n",
    "   - Netflix learning what movies you like\n",
    "   - Spotify discovering your music preferences\n",
    "   - Amazon suggesting products\n",
    "\n",
    "2. **Medical Diagnosis**\n",
    "   - Learning disease patterns from symptoms\n",
    "   - Identifying risk factors for conditions\n",
    "   - Personalizing treatment recommendations\n",
    "\n",
    "3. **Fraud Detection**\n",
    "   - Learning patterns of fraudulent transactions\n",
    "   - Identifying suspicious behavior\n",
    "   - Adapting to new fraud techniques\n",
    "\n",
    "4. **Customer Segmentation**\n",
    "   - Learning customer preferences\n",
    "   - Identifying target audiences\n",
    "   - Personalizing marketing campaigns\n",
    "\n",
    "**Example Scenario:**\n",
    "\n",
    "A fitness app wants to recommend outdoor activities to users. By observing when users enjoyed activities in the past (based on weather, time, location), the app learns each user's preferences and makes personalized recommendations. This increases user engagement by 40% and reduces app churn by 25%!\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ Background & Theory\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "**1. Concept Learning**\n",
    "- Learning a boolean-valued function from examples\n",
    "- Goal: Infer the function from training examples\n",
    "- Example: Learn \"EnjoySport\" concept from weather data\n",
    "\n",
    "**2. Hypothesis**\n",
    "- A proposed rule or pattern\n",
    "- Represented as a conjunction of attribute constraints\n",
    "- Example: `<Sunny, Warm, ?, ?, ?, ?>` means \"Sunny AND Warm AND anything else\"\n",
    "\n",
    "**3. Hypothesis Space (H)**\n",
    "- The set of all possible hypotheses\n",
    "- Defined by the representation language\n",
    "- Size depends on number of attributes and their values\n",
    "\n",
    "**4. Version Space**\n",
    "- The subset of hypotheses consistent with all training examples\n",
    "- Bounded by most specific (S) and most general (G) hypotheses\n",
    "- Shrinks as we see more examples\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "**Hypothesis Representation:**\n",
    "```\n",
    "h = <a‚ÇÅ, a‚ÇÇ, a‚ÇÉ, ..., a‚Çô>\n",
    "```\n",
    "Where each a·µ¢ can be:\n",
    "- A specific value (e.g., \"Sunny\")\n",
    "- `?` (any value is acceptable)\n",
    "- `‚àÖ` (no value is acceptable - empty set)\n",
    "\n",
    "**Consistency:**\n",
    "- A hypothesis h is **consistent** with example x if h(x) = c(x)\n",
    "- Where c(x) is the true concept (target function)\n",
    "\n",
    "**More General Than (‚â•·µç):**\n",
    "- h‚ÇÇ ‚â•·µç h‚ÇÅ if h‚ÇÇ classifies at least as many instances as positive as h‚ÇÅ\n",
    "- Example: `<?, Warm, ?, ?, ?, ?>` ‚â•·µç `<Sunny, Warm, ?, ?, ?, ?>`\n",
    "\n",
    "### Applications in Machine Learning\n",
    "\n",
    "- **Foundation for Decision Trees**: ID3, C4.5 use similar concepts\n",
    "- **Rule-Based Systems**: Expert systems, business rule engines\n",
    "- **Feature Selection**: Understanding which attributes matter\n",
    "- **Interpretable AI**: Producing human-readable rules\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PACKAGE INSTALLATION AND ENVIRONMENT SETUP\n",
    "# ============================================================================\n",
    "\n",
    "print('üîß Setting up environment for Concept Learning Experiment...\\n')\n",
    "\n",
    "# ============================================================================\n",
    "# CORE LIBRARY IMPORTS\n",
    "# ============================================================================\n",
    "\n",
    "# Data Processing\n",
    "import json                           # For loading JSON dataset files\n",
    "import numpy as np                    # Numerical computing library\n",
    "import pandas as pd                   # Data manipulation and analysis\n",
    "from pathlib import Path              # For cross-platform file path handling\n",
    "from copy import deepcopy             # For creating deep copies of data structures\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt       # Plotting library\n",
    "import seaborn as sns                 # Statistical visualization\n",
    "from matplotlib.patches import Rectangle, FancyBboxPatch  # For drawing shapes\n",
    "\n",
    "# Utilities\n",
    "import warnings                       # For suppressing warnings\n",
    "warnings.filterwarnings('ignore')     # Suppress warnings for cleaner output\n",
    "\n",
    "# ============================================================================\n",
    "# REPRODUCIBILITY SETUP\n",
    "# ============================================================================\n",
    "\n",
    "print('üé≤ Setting random seeds for reproducible results...')\n",
    "RANDOM_SEED = 42                      # Fixed seed for reproducibility\n",
    "np.random.seed(RANDOM_SEED)           # NumPy random number generator\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZATION STYLING\n",
    "# ============================================================================\n",
    "\n",
    "# Apply professional styling for all plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Define color scheme for consistent visualization\n",
    "colors = {\n",
    "    'primary': '#2E86AB',      # Blue - for main elements\n",
    "    'secondary': '#A23B72',    # Purple - for secondary elements\n",
    "    'accent': '#F18F01',       # Orange - for highlights\n",
    "    'success': '#06A77D',      # Green - for positive examples\n",
    "    'danger': '#D62246',       # Red - for negative examples\n",
    "    'neutral': '#4F4F4F'       # Gray - for neutral elements\n",
    "}\n",
    "\n",
    "print('‚úÖ Environment setup complete!')\n",
    "print(f'‚úì All packages imported successfully')\n",
    "print(f'‚úì Random seed set to {RANDOM_SEED} for reproducibility')\n",
    "print(f'‚úì Visualization styling applied')\n",
    "print(f'\\nüöÄ Ready to start learning!\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Section 1: Load and Explore the Dataset\n",
    "\n",
    "**What to Expect:** \n",
    "In this section, we'll load our outdoor activity dataset and explore its structure. You'll see the training examples that our algorithms will learn from.\n",
    "\n",
    "**Process Overview:**\n",
    "1. **Load Data**: Read the JSON dataset file containing weather conditions and activity preferences\n",
    "2. **Inspect Structure**: Examine the attributes (features) and their possible values\n",
    "3. **View Examples**: Look at positive and negative training examples\n",
    "4. **Understand Format**: Learn how hypotheses are represented\n",
    "5. **Visualize Distribution**: See the distribution of positive vs negative examples\n",
    "\n",
    "**Expected Outcome:** \n",
    "You'll have a clear understanding of the dataset structure, see 10 training examples with 6 attributes each, and understand how the data represents real-world scenarios.\n",
    "\n",
    "**Why This Matters:** \n",
    "Understanding your data is the first step in any ML project. In industry, data exploration reveals patterns, identifies issues, and guides algorithm selection. Poor data understanding leads to poor models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LOAD DATASET FROM JSON FILE\n",
    "# ============================================================================\n",
    "\n",
    "print('üìÇ Loading dataset...\\n')\n",
    "\n",
    "# Define the path to our dataset file\n",
    "# Using Path for cross-platform compatibility (works on Windows, Mac, Linux)\n",
    "dataset_path = Path('data/concept_learning_dataset.json')\n",
    "\n",
    "# Load the JSON file\n",
    "# JSON (JavaScript Object Notation) is a common format for storing structured data\n",
    "with open(dataset_path, 'r') as file:\n",
    "    dataset = json.load(file)  # Parse JSON into Python dictionary\n",
    "\n",
    "# Extract different components from the dataset\n",
    "dataset_info = dataset['dataset_info']           # Metadata about the dataset\n",
    "training_data = dataset['training_data']         # Examples for training\n",
    "test_data = dataset['test_data']                 # Examples for testing\n",
    "real_world_info = dataset['real_world_analogy']  # Real-world context\n",
    "\n",
    "print(f'‚úÖ Dataset loaded successfully!')\n",
    "print(f'\\nüìã Dataset: {dataset_info[\"name\"]}')\n",
    "print(f'üìù Description: {dataset_info[\"description\"]}')\n",
    "print(f'\\nüìä Statistics:')\n",
    "print(f'  ‚Ä¢ Training examples: {len(training_data)}')\n",
    "print(f'  ‚Ä¢ Test examples: {len(test_data)}')\n",
    "print(f'  ‚Ä¢ Number of attributes: {len(dataset_info[\"attributes\"])}')\n",
    "print(f'  ‚Ä¢ Target concept: {dataset_info[\"target_concept\"]}')\n",
    "\n",
    "# ============================================================================\n",
    "# DISPLAY ATTRIBUTE INFORMATION\n",
    "# ============================================================================\n",
    "\n",
    "print(f'\\nüè∑Ô∏è  Attributes (Features):\\n')\n",
    "\n",
    "# Loop through each attribute and display its information\n",
    "for i, attr in enumerate(dataset_info['attributes'], 1):\n",
    "    print(f\"{i}. {attr['name']:12} - {attr['description']}\")\n",
    "    print(f\"   Possible values: {', '.join(attr['possible_values'])}\")\n",
    "    print()\n",
    "\n",
    "# ============================================================================\n",
    "# CONVERT TO PANDAS DATAFRAME FOR EASY MANIPULATION\n",
    "# ============================================================================\n",
    "\n",
    "# Create a DataFrame from training data\n",
    "# DataFrame is like an Excel spreadsheet - rows and columns of data\n",
    "df_train = pd.DataFrame(training_data)\n",
    "\n",
    "# Extract attribute names (excluding metadata columns)\n",
    "attribute_names = [attr['name'] for attr in dataset_info['attributes']]\n",
    "target_name = dataset_info['target_concept']\n",
    "\n",
    "print(f'\\nüìä Training Data Preview (First 5 examples):\\n')\n",
    "# Display first 5 rows with only relevant columns\n",
    "display_columns = attribute_names + [target_name]\n",
    "print(df_train[display_columns].head())\n",
    "\n",
    "# ============================================================================\n",
    "# ANALYZE CLASS DISTRIBUTION\n",
    "# ============================================================================\n",
    "\n",
    "print(f'\\nüìà Class Distribution:\\n')\n",
    "\n",
    "# Count positive and negative examples\n",
    "positive_count = len(df_train[df_train[target_name] == 'Yes'])\n",
    "negative_count = len(df_train[df_train[target_name] == 'No'])\n",
    "\n",
    "print(f'  ‚úÖ Positive examples (EnjoySport = Yes): {positive_count}')\n",
    "print(f'  ‚ùå Negative examples (EnjoySport = No):  {negative_count}')\n",
    "print(f'  üìä Ratio: {positive_count}:{negative_count}')\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZE CLASS DISTRIBUTION\n",
    "# ============================================================================\n",
    "\n",
    "# Create a bar chart showing positive vs negative examples\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "\n",
    "# Count values for each class\n",
    "class_counts = df_train[target_name].value_counts()\n",
    "\n",
    "# Create bar plot\n",
    "bars = ax.bar(['Positive (Yes)', 'Negative (No)'], \n",
    "               [positive_count, negative_count],\n",
    "               color=[colors['success'], colors['danger']],\n",
    "               alpha=0.7,\n",
    "               edgecolor='black',\n",
    "               linewidth=1.5)\n",
    "\n",
    "# Add value labels on top of bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{int(height)}',\n",
    "            ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Customize plot\n",
    "ax.set_ylabel('Number of Examples', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Distribution of Training Examples', fontsize=14, fontweight='bold', pad=20)\n",
    "ax.set_ylim(0, max(positive_count, negative_count) * 1.2)  # Add space for labels\n",
    "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'\\n‚úÖ Data exploration complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîç Section 2: Find-S Algorithm - Finding the Most Specific Hypothesis\n",
    "\n",
    "**What to Expect:** \n",
    "We'll implement the Find-S algorithm, which finds the most specific hypothesis that fits all positive training examples. You'll see how the hypothesis evolves step-by-step as it processes each positive example.\n",
    "\n",
    "**Process Overview:**\n",
    "1. **Initialize Hypothesis**: Start with the most specific hypothesis (all attributes set to first positive example)\n",
    "2. **Process Positive Examples**: For each positive example, generalize the hypothesis if needed\n",
    "3. **Generalization Rule**: If an attribute value differs, replace it with '?' (meaning 'any value')\n",
    "4. **Ignore Negative Examples**: Find-S only learns from positive examples\n",
    "5. **Output Final Hypothesis**: The most specific hypothesis consistent with all positive examples\n",
    "\n",
    "**Expected Outcome:** \n",
    "You'll see the hypothesis start as a specific example and gradually generalize. The final hypothesis will be something like `<Sunny, Warm, ?, Strong, ?, ?>`, showing which attributes are essential for the concept.\n",
    "\n",
    "**Why This Matters:** \n",
    "Find-S is simple but powerful! It's used in rule-based systems, feature selection, and as a building block for more complex algorithms. Understanding Find-S helps you grasp how machines learn patterns from data.\n",
    "\n",
    "### üß† The Intuition Behind Find-S\n",
    "\n",
    "Think of Find-S like learning to recognize your favorite type of pizza:\n",
    "- First pizza you like: \"Thin crust, pepperoni, extra cheese, tomato sauce\"\n",
    "- Second pizza you like: \"Thin crust, pepperoni, regular cheese, tomato sauce\"\n",
    "- You learn: \"I like thin crust + pepperoni + tomato sauce, cheese amount doesn't matter\"\n",
    "\n",
    "Find-S does exactly this‚Äîit identifies what's essential and what's flexible!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# HELPER FUNCTIONS FOR HYPOTHESIS REPRESENTATION\n",
    "# ============================================================================\n",
    "\n",
    "def initialize_hypothesis(num_attributes):\n",
    "    \"\"\"\n",
    "    Initialize the most specific hypothesis.\n",
    "    \n",
    "    In Find-S, we start with the most specific hypothesis possible,\n",
    "    which is represented by all attributes set to the empty set (‚àÖ).\n",
    "    We'll use None to represent ‚àÖ initially.\n",
    "    \n",
    "    Args:\n",
    "        num_attributes (int): Number of attributes in the dataset\n",
    "    \n",
    "    Returns:\n",
    "        list: Hypothesis initialized to None for each attribute\n",
    "    \n",
    "    Example:\n",
    "        >>> h = initialize_hypothesis(6)\n",
    "        >>> print(h)\n",
    "        [None, None, None, None, None, None]\n",
    "    \"\"\"\n",
    "    # Create a list of None values, one for each attribute\n",
    "    # None represents ‚àÖ (empty set) - the most specific hypothesis\n",
    "    return [None] * num_attributes\n",
    "\n",
    "\n",
    "def more_general_or_equal(h1, h2):\n",
    "    \"\"\"\n",
    "    Check if hypothesis h1 is more general than or equal to h2.\n",
    "    \n",
    "    h1 is more general than h2 if:\n",
    "    - For each attribute, h1 is either '?' or equals h2\n",
    "    - '?' means \"any value\" (most general for that attribute)\n",
    "    \n",
    "    Args:\n",
    "        h1 (list): First hypothesis\n",
    "        h2 (list): Second hypothesis\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if h1 ‚â•·µç h2, False otherwise\n",
    "    \n",
    "    Example:\n",
    "        >>> h1 = ['?', 'Warm', '?', '?', '?', '?']\n",
    "        >>> h2 = ['Sunny', 'Warm', 'Normal', 'Strong', 'Warm', 'Same']\n",
    "        >>> more_general_or_equal(h1, h2)\n",
    "        True\n",
    "    \"\"\"\n",
    "    # Check each attribute position\n",
    "    for i in range(len(h1)):\n",
    "        # If h1 has '?', it's more general (accepts any value)\n",
    "        if h1[i] == '?':\n",
    "            continue\n",
    "        # If h1 has a specific value, it must match h2\n",
    "        elif h1[i] != h2[i]:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def format_hypothesis(hypothesis, attribute_names):\n",
    "    \"\"\"\n",
    "    Format hypothesis for pretty printing.\n",
    "    \n",
    "    Args:\n",
    "        hypothesis (list): Hypothesis to format\n",
    "        attribute_names (list): Names of attributes\n",
    "    \n",
    "    Returns:\n",
    "        str: Formatted hypothesis string\n",
    "    \n",
    "    Example:\n",
    "        >>> h = ['Sunny', 'Warm', '?', '?', '?', '?']\n",
    "        >>> attrs = ['Sky', 'AirTemp', 'Humidity', 'Wind', 'Water', 'Forecast']\n",
    "        >>> print(format_hypothesis(h, attrs))\n",
    "        <Sunny, Warm, ?, ?, ?, ?>\n",
    "    \"\"\"\n",
    "    # Replace None with ‚àÖ for display\n",
    "    formatted = ['‚àÖ' if h is None else h for h in hypothesis]\n",
    "    return '<' + ', '.join(formatted) + '>'\n",
    "\n",
    "\n",
    "print('‚úÖ Helper functions defined successfully!')\n",
    "print('  ‚Ä¢ initialize_hypothesis() - Creates the most specific hypothesis')\n",
    "print('  ‚Ä¢ more_general_or_equal() - Checks generality relationship')\n",
    "print('  ‚Ä¢ format_hypothesis() - Pretty prints hypotheses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FIND-S ALGORITHM IMPLEMENTATION\n",
    "# ============================================================================\n",
    "\n",
    "def find_s_algorithm(training_data, attribute_names, target_name):\n",
    "    \"\"\"\n",
    "    Implement the Find-S algorithm to find the most specific hypothesis.\n",
    "    \n",
    "    Algorithm:\n",
    "    1. Initialize h to the most specific hypothesis (all ‚àÖ)\n",
    "    2. For each positive training example x:\n",
    "        a. For each attribute a·µ¢ in h:\n",
    "           - If a·µ¢ is satisfied by x, do nothing\n",
    "           - Else, replace a·µ¢ with the next more general constraint satisfied by x\n",
    "    3. Return h\n",
    "    \n",
    "    Args:\n",
    "        training_data (list): List of training examples (dictionaries)\n",
    "        attribute_names (list): Names of attributes\n",
    "        target_name (str): Name of target attribute\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (final_hypothesis, trace) where trace is the step-by-step evolution\n",
    "    \"\"\"\n",
    "    \n",
    "    print('üîç Starting Find-S Algorithm...\\n')\n",
    "    print('='*80)\n",
    "    \n",
    "    # Step 1: Initialize hypothesis to the most specific (all None/‚àÖ)\n",
    "    hypothesis = initialize_hypothesis(len(attribute_names))\n",
    "    \n",
    "    # Track the evolution of hypothesis for visualization\n",
    "    trace = []\n",
    "    \n",
    "    # Counter for positive examples processed\n",
    "    positive_count = 0\n",
    "    \n",
    "    # Step 2: Process each training example\n",
    "    for idx, example in enumerate(training_data, 1):\n",
    "        \n",
    "        # Extract the class label (Yes/No)\n",
    "        label = example[target_name]\n",
    "        \n",
    "        # Extract attribute values for this example\n",
    "        instance = [example[attr] for attr in attribute_names]\n",
    "        \n",
    "        print(f'\\nExample {idx}: {format_hypothesis(instance, attribute_names)}')\n",
    "        print(f'Label: {label}')\n",
    "        \n",
    "        # Find-S only processes POSITIVE examples\n",
    "        if label == 'Yes':\n",
    "            positive_count += 1\n",
    "            print(f'‚úÖ Positive example - Processing...')\n",
    "            \n",
    "            # If this is the first positive example, initialize h to this example\n",
    "            if hypothesis[0] is None:\n",
    "                hypothesis = instance.copy()\n",
    "                print(f'   First positive example - Initialize h to this instance')\n",
    "            else:\n",
    "                # Generalize hypothesis if needed\n",
    "                print(f'   Current h: {format_hypothesis(hypothesis, attribute_names)}')\n",
    "                \n",
    "                # Check each attribute\n",
    "                for i in range(len(hypothesis)):\n",
    "                    # If attribute value differs from hypothesis, generalize to '?'\n",
    "                    if hypothesis[i] != instance[i]:\n",
    "                        print(f'   Attribute {attribute_names[i]}: {hypothesis[i]} ‚â† {instance[i]} ‚Üí Generalize to ?')\n",
    "                        hypothesis[i] = '?'\n",
    "            \n",
    "            # Store current state in trace\n",
    "            trace.append({\n",
    "                'example_num': idx,\n",
    "                'instance': instance.copy(),\n",
    "                'label': label,\n",
    "                'hypothesis': hypothesis.copy()\n",
    "            })\n",
    "            \n",
    "            print(f'   Updated h: {format_hypothesis(hypothesis, attribute_names)}')\n",
    "        \n",
    "        else:\n",
    "            print(f'‚ùå Negative example - Ignored by Find-S')\n",
    "        \n",
    "        print('-'*80)\n",
    "    \n",
    "    print(f'\\n‚úÖ Find-S Algorithm Complete!')\n",
    "    print(f'   Processed {positive_count} positive examples')\n",
    "    print(f'   Ignored {len(training_data) - positive_count} negative examples')\n",
    "    print(f'\\nüéØ Final Hypothesis: {format_hypothesis(hypothesis, attribute_names)}')\n",
    "    print('='*80)\n",
    "    \n",
    "    return hypothesis, trace\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# RUN FIND-S ALGORITHM\n",
    "# ============================================================================\n",
    "\n",
    "# Execute the Find-S algorithm on our training data\n",
    "final_hypothesis_s, trace_s = find_s_algorithm(\n",
    "    training_data=training_data,\n",
    "    attribute_names=attribute_names,\n",
    "    target_name=target_name\n",
    ")\n",
    "\n",
    "print(f'\\nüìä Hypothesis Evolution Summary:\\n')\n",
    "for step in trace_s:\n",
    "    print(f\"After Example {step['example_num']}: {format_hypothesis(step['hypothesis'], attribute_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Visualizing Find-S Hypothesis Evolution\n",
    "\n",
    "Let's create a visual representation of how the hypothesis evolved as we processed each positive example. This helps us understand the generalization process!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZE FIND-S HYPOTHESIS EVOLUTION\n",
    "# ============================================================================\n",
    "\n",
    "def visualize_find_s_evolution(trace, attribute_names):\n",
    "    \"\"\"\n",
    "    Create a visualization showing how the hypothesis evolved.\n",
    "    \n",
    "    Args:\n",
    "        trace (list): Trace of hypothesis evolution from Find-S\n",
    "        attribute_names (list): Names of attributes\n",
    "    \"\"\"\n",
    "    \n",
    "    print('üìä Creating visualization of hypothesis evolution...\\n')\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PLOT 1: Hypothesis Evolution Heatmap\n",
    "    # ========================================================================\n",
    "    \n",
    "    # Prepare data for heatmap\n",
    "    # Each row is a step, each column is an attribute\n",
    "    evolution_matrix = []\n",
    "    step_labels = []\n",
    "    \n",
    "    for step in trace:\n",
    "        # Convert hypothesis to numeric values for heatmap\n",
    "        # Specific value = 1, '?' = 0.5, None/‚àÖ = 0\n",
    "        row = []\n",
    "        for val in step['hypothesis']:\n",
    "            if val == '?':\n",
    "                row.append(0.5)  # Generalized\n",
    "            elif val is None:\n",
    "                row.append(0)    # Empty set\n",
    "            else:\n",
    "                row.append(1)    # Specific value\n",
    "        evolution_matrix.append(row)\n",
    "        step_labels.append(f\"After Ex {step['example_num']}\")\n",
    "    \n",
    "    # Create heatmap\n",
    "    im = ax1.imshow(evolution_matrix, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n",
    "    \n",
    "    # Set ticks and labels\n",
    "    ax1.set_xticks(np.arange(len(attribute_names)))\n",
    "    ax1.set_yticks(np.arange(len(step_labels)))\n",
    "    ax1.set_xticklabels(attribute_names, rotation=45, ha='right')\n",
    "    ax1.set_yticklabels(step_labels)\n",
    "    \n",
    "    # Add text annotations showing actual values\n",
    "    for i in range(len(step_labels)):\n",
    "        for j in range(len(attribute_names)):\n",
    "            val = trace[i]['hypothesis'][j]\n",
    "            if val is None:\n",
    "                text = '‚àÖ'\n",
    "            else:\n",
    "                text = val\n",
    "            ax1.text(j, i, text, ha='center', va='center', \n",
    "                    color='black', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    ax1.set_title('Find-S Hypothesis Evolution\\n(Green=Specific, Yellow=Generalized, Red=Empty)', \n",
    "                 fontsize=12, fontweight='bold', pad=15)\n",
    "    ax1.set_ylabel('Processing Steps', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(im, ax=ax1, fraction=0.046, pad=0.04)\n",
    "    cbar.set_label('Specificity', rotation=270, labelpad=20, fontweight='bold')\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PLOT 2: Generalization Progress\n",
    "    # ========================================================================\n",
    "    \n",
    "    # Count how many attributes are generalized at each step\n",
    "    generalization_counts = []\n",
    "    for step in trace:\n",
    "        count = sum(1 for val in step['hypothesis'] if val == '?')\n",
    "        generalization_counts.append(count)\n",
    "    \n",
    "    # Create line plot\n",
    "    steps = list(range(1, len(trace) + 1))\n",
    "    ax2.plot(steps, generalization_counts, marker='o', linewidth=2.5, \n",
    "            markersize=8, color=colors['primary'], label='Generalized Attributes')\n",
    "    ax2.fill_between(steps, generalization_counts, alpha=0.3, color=colors['primary'])\n",
    "    \n",
    "    # Customize plot\n",
    "    ax2.set_xlabel('Processing Step (Positive Examples)', fontsize=11, fontweight='bold')\n",
    "    ax2.set_ylabel('Number of Generalized Attributes (?)', fontsize=11, fontweight='bold')\n",
    "    ax2.set_title('Generalization Progress Over Time', fontsize=12, fontweight='bold', pad=15)\n",
    "    ax2.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax2.set_xticks(steps)\n",
    "    ax2.set_ylim(-0.5, len(attribute_names) + 0.5)\n",
    "    ax2.legend(loc='upper left', fontsize=10)\n",
    "    \n",
    "    # Add annotations for key points\n",
    "    for i, count in enumerate(generalization_counts):\n",
    "        ax2.annotate(f'{count}', xy=(steps[i], count), \n",
    "                    xytext=(0, 10), textcoords='offset points',\n",
    "                    ha='center', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print('‚úÖ Visualization complete!')\n",
    "\n",
    "\n",
    "# Create the visualization\n",
    "visualize_find_s_evolution(trace_s, attribute_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Section 3: Candidate Elimination Algorithm - Version Space Learning\n",
    "\n",
    "**What to Expect:** \n",
    "Now we'll implement the Candidate Elimination algorithm, which is more powerful than Find-S. It maintains a **version space**‚Äîthe set of ALL hypotheses consistent with the training data, represented by two boundaries: S (most specific) and G (most general).\n",
    "\n",
    "**Process Overview:**\n",
    "1. **Initialize Boundaries**: Start with S = most specific, G = most general\n",
    "2. **Process Positive Examples**: Make S more general, remove inconsistent hypotheses from G\n",
    "3. **Process Negative Examples**: Make G more specific, remove inconsistent hypotheses from S\n",
    "4. **Maintain Consistency**: Ensure S and G boundaries remain consistent\n",
    "5. **Output Version Space**: Final S and G boundaries define all consistent hypotheses\n",
    "\n",
    "**Expected Outcome:** \n",
    "You'll see both S and G boundaries evolve. The version space (all hypotheses between S and G) will shrink as we process more examples. Final output shows the most specific and most general hypotheses that fit ALL training data.\n",
    "\n",
    "**Why This Matters:** \n",
    "Unlike Find-S, Candidate Elimination uses BOTH positive and negative examples, making it more robust. It's used in active learning, query optimization, and understanding model uncertainty. The version space concept is fundamental to many modern ML techniques!\n",
    "\n",
    "### üß† The Intuition Behind Candidate Elimination\n",
    "\n",
    "Imagine you're a detective narrowing down suspects:\n",
    "- **S boundary**: The most specific description that fits all criminals you've caught\n",
    "- **G boundary**: The most general description that excludes all innocent people\n",
    "- **Version Space**: All possible descriptions between S and G\n",
    "\n",
    "As you gather more evidence (examples), both boundaries tighten, narrowing down the possibilities!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# HELPER FUNCTIONS FOR CANDIDATE ELIMINATION\n",
    "# ============================================================================\n",
    "\n",
    "def initialize_S(num_attributes):\n",
    "    \"\"\"\n",
    "    Initialize the S boundary to the most specific hypothesis.\n",
    "    \n",
    "    S starts as the most specific hypothesis: all attributes set to ‚àÖ (empty set).\n",
    "    We represent this as a list containing one hypothesis with all None values.\n",
    "    \n",
    "    Args:\n",
    "        num_attributes (int): Number of attributes\n",
    "    \n",
    "    Returns:\n",
    "        list: S boundary with one most specific hypothesis\n",
    "    \"\"\"\n",
    "    # S boundary starts with one hypothesis: all ‚àÖ\n",
    "    return [[None] * num_attributes]\n",
    "\n",
    "\n",
    "def initialize_G(num_attributes):\n",
    "    \"\"\"\n",
    "    Initialize the G boundary to the most general hypothesis.\n",
    "    \n",
    "    G starts as the most general hypothesis: all attributes set to '?' (any value).\n",
    "    We represent this as a list containing one hypothesis with all '?' values.\n",
    "    \n",
    "    Args:\n",
    "        num_attributes (int): Number of attributes\n",
    "    \n",
    "    Returns:\n",
    "        list: G boundary with one most general hypothesis\n",
    "    \"\"\"\n",
    "    # G boundary starts with one hypothesis: all ?\n",
    "    return [['?'] * num_attributes]\n",
    "\n",
    "\n",
    "def is_consistent(hypothesis, instance, label):\n",
    "    \"\"\"\n",
    "    Check if a hypothesis is consistent with an instance and its label.\n",
    "    \n",
    "    A hypothesis h is consistent with instance x and label y if:\n",
    "    - h(x) = y (hypothesis predicts the correct label)\n",
    "    \n",
    "    For our representation:\n",
    "    - h classifies x as positive if all attributes match (or are '?')\n",
    "    - h classifies x as negative otherwise\n",
    "    \n",
    "    Args:\n",
    "        hypothesis (list): Hypothesis to check\n",
    "        instance (list): Instance attribute values\n",
    "        label (str): True label ('Yes' or 'No')\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if consistent, False otherwise\n",
    "    \"\"\"\n",
    "    # Check if hypothesis covers the instance\n",
    "    covers = True\n",
    "    for h_val, x_val in zip(hypothesis, instance):\n",
    "        if h_val != '?' and h_val != x_val and h_val is not None:\n",
    "            covers = False\n",
    "            break\n",
    "    \n",
    "    # Hypothesis predicts positive if it covers the instance\n",
    "    prediction = 'Yes' if covers else 'No'\n",
    "    \n",
    "    # Consistent if prediction matches actual label\n",
    "    return prediction == label\n",
    "\n",
    "\n",
    "def min_generalizations(hypothesis, instance, num_attributes):\n",
    "    \"\"\"\n",
    "    Generate minimal generalizations of hypothesis to cover instance.\n",
    "    \n",
    "    Args:\n",
    "        hypothesis (list): Current hypothesis\n",
    "        instance (list): Instance to cover\n",
    "        num_attributes (int): Number of attributes\n",
    "    \n",
    "    Returns:\n",
    "        list: List of minimally generalized hypotheses\n",
    "    \"\"\"\n",
    "    generalizations = []\n",
    "    \n",
    "    # Create a generalized version\n",
    "    new_h = hypothesis.copy()\n",
    "    for i in range(num_attributes):\n",
    "        if new_h[i] is None:\n",
    "            # If ‚àÖ, replace with instance value\n",
    "            new_h[i] = instance[i]\n",
    "        elif new_h[i] != instance[i]:\n",
    "            # If different, generalize to '?'\n",
    "            new_h[i] = '?'\n",
    "    \n",
    "    generalizations.append(new_h)\n",
    "    return generalizations\n",
    "\n",
    "\n",
    "def min_specializations(hypothesis, instance, num_attributes, attribute_values):\n",
    "    \"\"\"\n",
    "    Generate minimal specializations of hypothesis to exclude instance.\n",
    "    \n",
    "    Args:\n",
    "        hypothesis (list): Current hypothesis\n",
    "        instance (list): Instance to exclude\n",
    "        num_attributes (int): Number of attributes\n",
    "        attribute_values (dict): Possible values for each attribute\n",
    "    \n",
    "    Returns:\n",
    "        list: List of minimally specialized hypotheses\n",
    "    \"\"\"\n",
    "    specializations = []\n",
    "    \n",
    "    # For each attribute that is '?', try replacing with specific values\n",
    "    for i in range(num_attributes):\n",
    "        if hypothesis[i] == '?':\n",
    "            # Try each possible value except the instance value\n",
    "            for val in attribute_values[i]:\n",
    "                if val != instance[i]:\n",
    "                    new_h = hypothesis.copy()\n",
    "                    new_h[i] = val\n",
    "                    specializations.append(new_h)\n",
    "    \n",
    "    return specializations\n",
    "\n",
    "\n",
    "print('‚úÖ Candidate Elimination helper functions defined!')\n",
    "print('  ‚Ä¢ initialize_S() - Creates most specific boundary')\n",
    "print('  ‚Ä¢ initialize_G() - Creates most general boundary')\n",
    "print('  ‚Ä¢ is_consistent() - Checks hypothesis consistency')\n",
    "print('  ‚Ä¢ min_generalizations() - Generates minimal generalizations')\n",
    "print('  ‚Ä¢ min_specializations() - Generates minimal specializations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CANDIDATE ELIMINATION ALGORITHM IMPLEMENTATION\n",
    "# ============================================================================\n",
    "\n",
    "def candidate_elimination_algorithm(training_data, attribute_names, target_name, dataset_info):\n",
    "    \"\"\"\n",
    "    Implement the Candidate Elimination algorithm.\n",
    "    \n",
    "    Algorithm:\n",
    "    1. Initialize S to most specific, G to most general\n",
    "    2. For each training example x:\n",
    "        If x is positive:\n",
    "            - Remove from G any hypothesis inconsistent with x\n",
    "            - For each hypothesis s in S not consistent with x:\n",
    "                ‚Ä¢ Remove s from S\n",
    "                ‚Ä¢ Add minimal generalizations of s to S\n",
    "            - Remove from S any hypothesis more general than another in S\n",
    "        If x is negative:\n",
    "            - Remove from S any hypothesis inconsistent with x\n",
    "            - For each hypothesis g in G not consistent with x:\n",
    "                ‚Ä¢ Remove g from G\n",
    "                ‚Ä¢ Add minimal specializations of g to G\n",
    "            - Remove from G any hypothesis more specific than another in G\n",
    "    3. Return S and G\n",
    "    \n",
    "    Args:\n",
    "        training_data (list): Training examples\n",
    "        attribute_names (list): Attribute names\n",
    "        target_name (str): Target attribute name\n",
    "        dataset_info (dict): Dataset metadata\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (S, G, trace) - Final boundaries and evolution trace\n",
    "    \"\"\"\n",
    "    \n",
    "    print('üéØ Starting Candidate Elimination Algorithm...\\n')\n",
    "    print('='*80)\n",
    "    \n",
    "    num_attributes = len(attribute_names)\n",
    "    \n",
    "    # Get possible values for each attribute\n",
    "    attribute_values = []\n",
    "    for attr_info in dataset_info['attributes']:\n",
    "        attribute_values.append(attr_info['possible_values'])\n",
    "    \n",
    "    # Step 1: Initialize S and G boundaries\n",
    "    S = initialize_S(num_attributes)\n",
    "    G = initialize_G(num_attributes)\n",
    "    \n",
    "    print(f'Initial S boundary: {[format_hypothesis(h, attribute_names) for h in S]}')\n",
    "    print(f'Initial G boundary: {[format_hypothesis(h, attribute_names) for h in G]}')\n",
    "    print('='*80)\n",
    "    \n",
    "    # Track evolution for visualization\n",
    "    trace = []\n",
    "    \n",
    "    # Step 2: Process each training example\n",
    "    for idx, example in enumerate(training_data, 1):\n",
    "        \n",
    "        # Extract label and instance\n",
    "        label = example[target_name]\n",
    "        instance = [example[attr] for attr in attribute_names]\n",
    "        \n",
    "        print(f'\\nExample {idx}: {format_hypothesis(instance, attribute_names)}')\n",
    "        print(f'Label: {label}')\n",
    "        \n",
    "        if label == 'Yes':\n",
    "            # ================================================================\n",
    "            # POSITIVE EXAMPLE\n",
    "            # ================================================================\n",
    "            print(f'‚úÖ Positive example - Update S and G...')\n",
    "            \n",
    "            # Remove from G any hypothesis inconsistent with x\n",
    "            G = [g for g in G if is_consistent(g, instance, label)]\n",
    "            \n",
    "            # For each hypothesis in S\n",
    "            S_new = []\n",
    "            for s in S:\n",
    "                if is_consistent(s, instance, label):\n",
    "                    # Keep consistent hypotheses\n",
    "                    S_new.append(s)\n",
    "                else:\n",
    "                    # Generalize inconsistent hypotheses\n",
    "                    generalizations = min_generalizations(s, instance, num_attributes)\n",
    "                    for h in generalizations:\n",
    "                        # Add if it's consistent and not more general than G\n",
    "                        if any(more_general_or_equal(g, h) for g in G):\n",
    "                            S_new.append(h)\n",
    "            \n",
    "            # Remove more general hypotheses from S\n",
    "            S = []\n",
    "            for h in S_new:\n",
    "                if not any(more_general_or_equal(h, h2) and h != h2 for h2 in S_new):\n",
    "                    S.append(h)\n",
    "        \n",
    "        else:\n",
    "            # ================================================================\n",
    "            # NEGATIVE EXAMPLE\n",
    "            # ================================================================\n",
    "            print(f'‚ùå Negative example - Update S and G...')\n",
    "            \n",
    "            # Remove from S any hypothesis inconsistent with x\n",
    "            S = [s for s in S if is_consistent(s, instance, label)]\n",
    "            \n",
    "            # For each hypothesis in G\n",
    "            G_new = []\n",
    "            for g in G:\n",
    "                if is_consistent(g, instance, label):\n",
    "                    # Keep consistent hypotheses\n",
    "                    G_new.append(g)\n",
    "                else:\n",
    "                    # Specialize inconsistent hypotheses\n",
    "                    specializations = min_specializations(g, instance, num_attributes, attribute_values)\n",
    "                    for h in specializations:\n",
    "                        # Add if it's consistent and not more specific than S\n",
    "                        if any(more_general_or_equal(h, s) for s in S):\n",
    "                            G_new.append(h)\n",
    "            \n",
    "            # Remove more specific hypotheses from G\n",
    "            G = []\n",
    "            for h in G_new:\n",
    "                if not any(more_general_or_equal(h2, h) and h != h2 for h2 in G_new):\n",
    "                    G.append(h)\n",
    "        \n",
    "        # Store current state\n",
    "        trace.append({\n",
    "            'example_num': idx,\n",
    "            'instance': instance.copy(),\n",
    "            'label': label,\n",
    "            'S': deepcopy(S),\n",
    "            'G': deepcopy(G)\n",
    "        })\n",
    "        \n",
    "        print(f'\\nUpdated S: {[format_hypothesis(h, attribute_names) for h in S]}')\n",
    "        print(f'Updated G: {[format_hypothesis(h, attribute_names) for h in G]}')\n",
    "        print('-'*80)\n",
    "    \n",
    "    print(f'\\n‚úÖ Candidate Elimination Complete!')\n",
    "    print(f'\\nüéØ Final Version Space:')\n",
    "    print(f'\\n   S (Most Specific): {[format_hypothesis(h, attribute_names) for h in S]}')\n",
    "    print(f'   G (Most General):  {[format_hypothesis(h, attribute_names) for h in G]}')\n",
    "    print('='*80)\n",
    "    \n",
    "    return S, G, trace\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# RUN CANDIDATE ELIMINATION ALGORITHM\n",
    "# ============================================================================\n",
    "\n",
    "# Execute the Candidate Elimination algorithm\n",
    "final_S, final_G, trace_ce = candidate_elimination_algorithm(\n",
    "    training_data=training_data,\n",
    "    attribute_names=attribute_names,\n",
    "    target_name=target_name,\n",
    "    dataset_info=dataset_info\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Visualizing Version Space Evolution\n",
    "\n",
    "Let's visualize how the version space (bounded by S and G) evolved as we processed examples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZE CANDIDATE ELIMINATION EVOLUTION\n",
    "# ============================================================================\n",
    "\n",
    "def visualize_version_space_evolution(trace, attribute_names):\n",
    "    \"\"\"\n",
    "    Visualize how S and G boundaries evolved.\n",
    "    \n",
    "    Args:\n",
    "        trace (list): Evolution trace from Candidate Elimination\n",
    "        attribute_names (list): Attribute names\n",
    "    \"\"\"\n",
    "    \n",
    "    print('üìä Creating version space evolution visualization...\\n')\n",
    "    \n",
    "    # Create figure\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PLOT 1: S and G Boundary Sizes Over Time\n",
    "    # ========================================================================\n",
    "    \n",
    "    steps = list(range(1, len(trace) + 1))\n",
    "    s_sizes = [len(step['S']) for step in trace]\n",
    "    g_sizes = [len(step['G']) for step in trace]\n",
    "    \n",
    "    # Plot S boundary size\n",
    "    ax1.plot(steps, s_sizes, marker='o', linewidth=2.5, markersize=8,\n",
    "            color=colors['success'], label='S Boundary Size', linestyle='-')\n",
    "    ax1.fill_between(steps, s_sizes, alpha=0.2, color=colors['success'])\n",
    "    \n",
    "    # Plot G boundary size\n",
    "    ax1.plot(steps, g_sizes, marker='s', linewidth=2.5, markersize=8,\n",
    "            color=colors['danger'], label='G Boundary Size', linestyle='--')\n",
    "    ax1.fill_between(steps, g_sizes, alpha=0.2, color=colors['danger'])\n",
    "    \n",
    "    # Customize plot\n",
    "    ax1.set_xlabel('Processing Step (All Examples)', fontsize=11, fontweight='bold')\n",
    "    ax1.set_ylabel('Number of Hypotheses in Boundary', fontsize=11, fontweight='bold')\n",
    "    ax1.set_title('Version Space Boundary Evolution', fontsize=12, fontweight='bold', pad=15)\n",
    "    ax1.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax1.legend(loc='upper right', fontsize=10)\n",
    "    ax1.set_xticks(steps)\n",
    "    \n",
    "    # Add annotations\n",
    "    for i, (s_size, g_size) in enumerate(zip(s_sizes, g_sizes)):\n",
    "        if i % 2 == 0:  # Annotate every other point to avoid clutter\n",
    "            ax1.annotate(f'{s_size}', xy=(steps[i], s_size),\n",
    "                        xytext=(0, 10), textcoords='offset points',\n",
    "                        ha='center', fontsize=8, color=colors['success'])\n",
    "            ax1.annotate(f'{g_size}', xy=(steps[i], g_size),\n",
    "                        xytext=(0, -15), textcoords='offset points',\n",
    "                        ha='center', fontsize=8, color=colors['danger'])\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PLOT 2: Example Type Distribution\n",
    "    # ========================================================================\n",
    "    \n",
    "    # Count positive and negative examples processed\n",
    "    example_types = [step['label'] for step in trace]\n",
    "    positive_indices = [i+1 for i, label in enumerate(example_types) if label == 'Yes']\n",
    "    negative_indices = [i+1 for i, label in enumerate(example_types) if label == 'No']\n",
    "    \n",
    "    # Create scatter plot showing example types\n",
    "    ax2.scatter(positive_indices, [1]*len(positive_indices), \n",
    "               s=200, marker='o', color=colors['success'], \n",
    "               label='Positive Examples', alpha=0.7, edgecolors='black', linewidth=1.5)\n",
    "    ax2.scatter(negative_indices, [0]*len(negative_indices), \n",
    "               s=200, marker='X', color=colors['danger'], \n",
    "               label='Negative Examples', alpha=0.7, edgecolors='black', linewidth=1.5)\n",
    "    \n",
    "    # Customize plot\n",
    "    ax2.set_xlabel('Processing Step', fontsize=11, fontweight='bold')\n",
    "    ax2.set_ylabel('Example Type', fontsize=11, fontweight='bold')\n",
    "    ax2.set_title('Training Example Sequence', fontsize=12, fontweight='bold', pad=15)\n",
    "    ax2.set_yticks([0, 1])\n",
    "    ax2.set_yticklabels(['Negative', 'Positive'])\n",
    "    ax2.set_xticks(steps)\n",
    "    ax2.set_xlim(0, len(steps) + 1)\n",
    "    ax2.set_ylim(-0.5, 1.5)\n",
    "    ax2.grid(True, alpha=0.3, linestyle='--', axis='x')\n",
    "    ax2.legend(loc='upper right', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print('‚úÖ Visualization complete!')\n",
    "\n",
    "\n",
    "# Create visualization\n",
    "visualize_version_space_evolution(trace_ce, attribute_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üî¨ Section 4: Comparison and Analysis\n",
    "\n",
    "**What to Expect:** \n",
    "Now let's compare Find-S and Candidate Elimination side-by-side to understand their strengths and weaknesses.\n",
    "\n",
    "**Why This Matters:** \n",
    "Understanding when to use each algorithm is crucial for real-world applications. Different problems require different approaches!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# COMPARISON OF FIND-S AND CANDIDATE ELIMINATION\n",
    "# ============================================================================\n",
    "\n",
    "print('üìä Comparing Find-S and Candidate Elimination\\n')\n",
    "print('='*80)\n",
    "\n",
    "# Create comparison table\n",
    "comparison_data = {\n",
    "    'Aspect': [\n",
    "        'Examples Used',\n",
    "        'Output',\n",
    "        'Hypothesis Space Coverage',\n",
    "        'Robustness',\n",
    "        'Computational Complexity',\n",
    "        'Handles Noise',\n",
    "        'Convergence Guarantee'\n",
    "    ],\n",
    "    'Find-S': [\n",
    "        'Only positive examples',\n",
    "        'Single most specific hypothesis',\n",
    "        'Limited (one hypothesis)',\n",
    "        'Low (ignores negative examples)',\n",
    "        'O(n) - Very fast',\n",
    "        'No',\n",
    "        'Yes (if concept exists in H)'\n",
    "    ],\n",
    "    'Candidate Elimination': [\n",
    "        'Both positive and negative',\n",
    "        'Version space (S and G boundaries)',\n",
    "        'Complete (all consistent hypotheses)',\n",
    "        'High (uses all information)',\n",
    "        'O(n √ó |H|) - Can be expensive',\n",
    "        'No',\n",
    "        'Yes (if concept exists in H)'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "print(df_comparison.to_string(index=False))\n",
    "print('='*80)\n",
    "\n",
    "# Display final hypotheses\n",
    "print(f'\\nüéØ Final Results:\\n')\n",
    "print(f'Find-S Final Hypothesis:')\n",
    "print(f'  {format_hypothesis(final_hypothesis_s, attribute_names)}')\n",
    "print(f'\\nCandidate Elimination Final Version Space:')\n",
    "print(f'  S (Most Specific): {[format_hypothesis(h, attribute_names) for h in final_S]}')\n",
    "print(f'  G (Most General):  {[format_hypothesis(h, attribute_names) for h in final_G]}')\n",
    "\n",
    "# Interpretation\n",
    "print(f'\\nüí° Interpretation:\\n')\n",
    "print(f'Find-S Result:')\n",
    "print(f'  The hypothesis {format_hypothesis(final_hypothesis_s, attribute_names)}')\n",
    "print(f'  means: \"Enjoy sport when conditions match this specific pattern\"')\n",
    "print(f'  where ? means \"any value is acceptable\"\\n')\n",
    "\n",
    "print(f'Candidate Elimination Result:')\n",
    "print(f'  The version space contains ALL hypotheses between S and G')\n",
    "print(f'  S represents the most specific consistent hypothesis')\n",
    "print(f'  G represents the most general consistent hypothesis')\n",
    "print(f'  Any hypothesis between S and G is consistent with the training data')\n",
    "\n",
    "print(f'\\n‚úÖ Comparison complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß™ Section 5: Testing on New Examples\n",
    "\n",
    "**What to Expect:** \n",
    "Let's test our learned hypotheses on new, unseen examples to see how well they generalize!\n",
    "\n",
    "**Why This Matters:** \n",
    "The true test of any ML model is how well it performs on new data. This is called **generalization**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TEST ON NEW EXAMPLES\n",
    "# ============================================================================\n",
    "\n",
    "def predict_with_hypothesis(hypothesis, instance):\n",
    "    \"\"\"\n",
    "    Predict class label using a hypothesis.\n",
    "    \n",
    "    Args:\n",
    "        hypothesis (list): Hypothesis to use for prediction\n",
    "        instance (list): Instance to classify\n",
    "    \n",
    "    Returns:\n",
    "        str: Predicted label ('Yes' or 'No')\n",
    "    \"\"\"\n",
    "    # Check if hypothesis covers the instance\n",
    "    for h_val, x_val in zip(hypothesis, instance):\n",
    "        if h_val != '?' and h_val != x_val and h_val is not None:\n",
    "            return 'No'  # Doesn't match\n",
    "    return 'Yes'  # Matches\n",
    "\n",
    "\n",
    "print('üß™ Testing on New Examples\\n')\n",
    "print('='*80)\n",
    "\n",
    "# Test each example\n",
    "for test_example in test_data:\n",
    "    instance = [test_example[attr] for attr in attribute_names]\n",
    "    \n",
    "    print(f'\\nTest Example {test_example[\"example_id\"]}:')\n",
    "    print(f'  Instance: {format_hypothesis(instance, attribute_names)}')\n",
    "    print(f'  Description: {test_example[\"description\"]}')\n",
    "    \n",
    "    # Predict with Find-S hypothesis\n",
    "    pred_finds = predict_with_hypothesis(final_hypothesis_s, instance)\n",
    "    print(f'\\n  Find-S Prediction: {pred_finds}')\n",
    "    \n",
    "    # Predict with Candidate Elimination (using S boundary)\n",
    "    # If any hypothesis in S covers it, predict Yes\n",
    "    pred_ce_s = 'Yes' if any(predict_with_hypothesis(s, instance) == 'Yes' for s in final_S) else 'No'\n",
    "    print(f'  Candidate Elimination (S) Prediction: {pred_ce_s}')\n",
    "    \n",
    "    # Check if all hypotheses in G agree\n",
    "    g_predictions = [predict_with_hypothesis(g, instance) for g in final_G]\n",
    "    if len(set(g_predictions)) == 1:\n",
    "        print(f'  Candidate Elimination (G) Prediction: {g_predictions[0]}')\n",
    "    else:\n",
    "        print(f'  Candidate Elimination (G) Prediction: Uncertain (hypotheses disagree)')\n",
    "    \n",
    "    print('-'*80)\n",
    "\n",
    "print(f'\\n‚úÖ Testing complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ùì Frequently Asked Questions (FAQ)\n",
    "\n",
    "### Technical Questions\n",
    "\n",
    "**Q1: Why does Find-S ignore negative examples?**\n",
    "\n",
    "A: Find-S is designed to find the most specific hypothesis consistent with positive examples. Negative examples don't help make a hypothesis more specific‚Äîthey only tell us what NOT to classify as positive. Candidate Elimination uses negative examples to refine the G boundary (most general hypotheses).\n",
    "\n",
    "**Q2: What if there are multiple hypotheses in the final S boundary?**\n",
    "\n",
    "A: In a well-behaved dataset, S should converge to a single hypothesis or a small set. Multiple hypotheses in S mean there are different equally specific ways to describe the positive examples. You can:\n",
    "- Use voting (majority prediction)\n",
    "- Choose the simplest hypothesis (Occam's Razor)\n",
    "- Collect more training data to narrow down\n",
    "\n",
    "**Q3: What does it mean when S and G boundaries collapse to the same hypothesis?**\n",
    "\n",
    "A: Perfect convergence! This means there's exactly ONE hypothesis consistent with all training data. The version space contains only this single hypothesis. This is the ideal outcome.\n",
    "\n",
    "**Q4: Can these algorithms handle continuous attributes (like temperature in degrees)?**\n",
    "\n",
    "A: Not directly. Find-S and Candidate Elimination work with discrete attributes. For continuous attributes, you need to:\n",
    "- Discretize them into bins (e.g., \"Cold\": 0-15¬∞C, \"Warm\": 15-25¬∞C, \"Hot\": >25¬∞C)\n",
    "- Use different algorithms like decision trees or neural networks\n",
    "- Extend the hypothesis representation to include ranges\n",
    "\n",
    "**Q5: What happens if the training data contains noise or errors?**\n",
    "\n",
    "A: Both algorithms will fail! They assume:\n",
    "- Training data is noise-free\n",
    "- Target concept exists in hypothesis space\n",
    "- Examples are correctly labeled\n",
    "\n",
    "With noisy data, the version space may become empty (S and G don't overlap). Solutions:\n",
    "- Use robust algorithms (decision trees, neural networks)\n",
    "- Apply data cleaning techniques\n",
    "- Use probabilistic approaches (Naive Bayes)\n",
    "\n",
    "**Q6: How do I know if my hypothesis space is appropriate?**\n",
    "\n",
    "A: Check if:\n",
    "- The target concept can be represented in your hypothesis language\n",
    "- The hypothesis space isn't too large (computational cost)\n",
    "- The hypothesis space isn't too small (can't represent the concept)\n",
    "\n",
    "If S and G don't converge or the version space is empty, your hypothesis space might be inadequate.\n",
    "\n",
    "### Practical Questions\n",
    "\n",
    "**Q7: When should I use Find-S vs Candidate Elimination in practice?**\n",
    "\n",
    "A: Use **Find-S** when:\n",
    "- You only have positive examples\n",
    "- You need a fast, simple solution\n",
    "- You want a single, interpretable hypothesis\n",
    "- Computational resources are limited\n",
    "\n",
    "Use **Candidate Elimination** when:\n",
    "- You have both positive and negative examples\n",
    "- You need to understand all consistent hypotheses\n",
    "- You want to quantify uncertainty\n",
    "- You're doing active learning (choosing which examples to label next)\n",
    "\n",
    "**Q8: How do these algorithms scale to large datasets?**\n",
    "\n",
    "A: Scalability challenges:\n",
    "- **Find-S**: Scales well (O(n) complexity)\n",
    "- **Candidate Elimination**: Can be expensive (O(n √ó |H|) where |H| is hypothesis space size)\n",
    "\n",
    "For large datasets:\n",
    "- Use sampling techniques\n",
    "- Implement incremental learning\n",
    "- Consider modern algorithms (gradient descent, neural networks)\n",
    "\n",
    "**Q9: Can I use these algorithms for multi-class classification?**\n",
    "\n",
    "A: Not directly‚Äîthey're designed for binary classification (Yes/No). For multi-class:\n",
    "- Use one-vs-rest approach (train one classifier per class)\n",
    "- Use one-vs-one approach (train classifier for each pair of classes)\n",
    "- Use algorithms designed for multi-class (decision trees, neural networks)\n",
    "\n",
    "**Q10: How do I explain these results to non-technical stakeholders?**\n",
    "\n",
    "A: Use analogies:\n",
    "- **Find-S**: \"We looked at all successful cases and found the common pattern\"\n",
    "- **Candidate Elimination**: \"We narrowed down possibilities by looking at both successes and failures\"\n",
    "- **Version Space**: \"These are all the rules that fit our data‚Äîthe truth is somewhere in this range\"\n",
    "\n",
    "Focus on:\n",
    "- Business impact (accuracy, cost savings)\n",
    "- Interpretability (show the actual rules)\n",
    "- Confidence (version space size indicates certainty)\n",
    "\n",
    "### Industry-Specific Questions\n",
    "\n",
    "**Q11: Are these algorithms still used in modern ML?**\n",
    "\n",
    "A: While not commonly used directly, the concepts are fundamental:\n",
    "- **Version spaces**: Used in active learning and query optimization\n",
    "- **Hypothesis refinement**: Core idea in ensemble methods\n",
    "- **Inductive bias**: Critical concept in all ML algorithms\n",
    "- **Rule learning**: Still used in expert systems and explainable AI\n",
    "\n",
    "Modern applications:\n",
    "- Rule-based systems in healthcare (diagnosis rules)\n",
    "- Fraud detection (learning fraud patterns)\n",
    "- Recommendation systems (learning user preferences)\n",
    "- Explainable AI (generating interpretable rules)\n",
    "\n",
    "**Q12: What are the limitations I should be aware of?**\n",
    "\n",
    "A: Key limitations:\n",
    "1. **Discrete attributes only**: Can't handle continuous values directly\n",
    "2. **No noise tolerance**: Assumes perfect data\n",
    "3. **Conjunctive hypotheses only**: Can't learn disjunctions (OR conditions)\n",
    "4. **Hypothesis space must contain target**: If not, algorithms fail\n",
    "5. **Computational cost**: Candidate Elimination can be expensive for large hypothesis spaces\n",
    "6. **No probabilistic output**: Only binary predictions\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Assignments\n",
    "\n",
    "### Assignment 1: Experiment with Different Datasets\n",
    "\n",
    "**Objective:** Gain hands-on experience by applying the algorithms to different scenarios\n",
    "\n",
    "**Tasks:**\n",
    "1. **Create Your Own Dataset:**\n",
    "   - Choose a domain (e.g., restaurant recommendation, movie preferences, product purchases)\n",
    "   - Define 4-6 attributes with 2-3 possible values each\n",
    "   - Create 10-15 training examples (mix of positive and negative)\n",
    "   - Save as JSON file following the same format\n",
    "\n",
    "2. **Run Both Algorithms:**\n",
    "   - Apply Find-S to your dataset\n",
    "   - Apply Candidate Elimination to your dataset\n",
    "   - Document the evolution of hypotheses\n",
    "\n",
    "3. **Analyze Results:**\n",
    "   - Compare final hypotheses from both algorithms\n",
    "   - Test on 3-5 new examples\n",
    "   - Discuss which algorithm performed better and why\n",
    "\n",
    "**Deliverables:**\n",
    "- Custom dataset JSON file\n",
    "- Modified notebook with your experiments\n",
    "- 1-page report with:\n",
    "  - Dataset description\n",
    "  - Results comparison\n",
    "  - Insights and observations\n",
    "\n",
    "**Estimated Time:** 2-3 hours\n",
    "\n",
    "---\n",
    "\n",
    "### Assignment 2: Hypothesis Space Analysis\n",
    "\n",
    "**Objective:** Understand how hypothesis space size affects algorithm performance\n",
    "\n",
    "**Tasks:**\n",
    "1. **Calculate Hypothesis Space Size:**\n",
    "   - For the outdoor activity dataset, calculate total possible hypotheses\n",
    "   - Formula: For each attribute with k values: (k + 2) choices (specific values + '?' + '‚àÖ')\n",
    "   - Total = Product of all attribute choices + 1 (for completely empty hypothesis)\n",
    "\n",
    "2. **Experiment with Attribute Reduction:**\n",
    "   - Remove one attribute at a time\n",
    "   - Run Candidate Elimination with reduced attributes\n",
    "   - Observe how version space size changes\n",
    "\n",
    "3. **Analyze Impact:**\n",
    "   - How does reducing attributes affect:\n",
    "     - Computational cost?\n",
    "     - Hypothesis specificity?\n",
    "     - Prediction accuracy?\n",
    "\n",
    "**Deliverables:**\n",
    "- Calculations showing hypothesis space sizes\n",
    "- Experimental results with different attribute sets\n",
    "- Analysis report (2 pages)\n",
    "\n",
    "**Estimated Time:** 3-4 hours\n",
    "\n",
    "---\n",
    "\n",
    "### Assignment 3: Real-World Application Design\n",
    "\n",
    "**Objective:** Design a complete concept learning solution for a real problem\n",
    "\n",
    "**Scenario:** You're building a smart email filter that learns which emails a user considers important.\n",
    "\n",
    "**Tasks:**\n",
    "1. **Problem Definition:**\n",
    "   - Define attributes (sender, subject keywords, time, has attachments, etc.)\n",
    "   - Determine possible values for each attribute\n",
    "   - Identify target concept (\"Important Email\")\n",
    "\n",
    "2. **Data Collection Plan:**\n",
    "   - How would you collect training examples?\n",
    "   - How many examples needed?\n",
    "   - How to handle user feedback?\n",
    "\n",
    "3. **Algorithm Selection:**\n",
    "   - Would you use Find-S or Candidate Elimination? Why?\n",
    "   - What modifications might be needed?\n",
    "   - How to handle edge cases?\n",
    "\n",
    "4. **Implementation Plan:**\n",
    "   - Design the system architecture\n",
    "   - Plan for incremental learning (as user marks more emails)\n",
    "   - Consider scalability and performance\n",
    "\n",
    "5. **Evaluation Strategy:**\n",
    "   - How to measure success?\n",
    "   - What metrics to use?\n",
    "   - How to handle errors?\n",
    "\n",
    "**Deliverables:**\n",
    "- System design document (3-4 pages)\n",
    "- Mock dataset (20+ examples)\n",
    "- Prototype implementation (notebook)\n",
    "- Presentation slides (10 slides)\n",
    "\n",
    "**Estimated Time:** 6-8 hours\n",
    "\n",
    "---\n",
    "\n",
    "## üí¨ Discussion Questions\n",
    "\n",
    "Reflect on these questions and discuss with your peers:\n",
    "\n",
    "### 1. Algorithm Comparison\n",
    "- In what scenarios would Find-S be preferable to Candidate Elimination despite being less powerful?\n",
    "- Can you think of a real-world problem where you'd only have positive examples?\n",
    "\n",
    "### 2. Hypothesis Representation\n",
    "- What are the limitations of using only conjunctions (AND) in hypotheses?\n",
    "- How would you extend the representation to handle disjunctions (OR)?\n",
    "- What about negations (NOT)?\n",
    "\n",
    "### 3. Practical Deployment\n",
    "- How would you handle a situation where the version space becomes empty?\n",
    "- What strategies could make these algorithms more robust to noisy data?\n",
    "- How would you implement incremental learning (updating hypotheses as new data arrives)?\n",
    "\n",
    "### 4. Scalability\n",
    "- What happens when the hypothesis space becomes very large (millions of hypotheses)?\n",
    "- How could you optimize Candidate Elimination for better performance?\n",
    "- Would sampling techniques help? What are the trade-offs?\n",
    "\n",
    "### 5. Interpretability vs Accuracy\n",
    "- These algorithms produce interpretable rules. Is this always valuable?\n",
    "- When would you sacrifice interpretability for higher accuracy?\n",
    "- How do you balance explainability with performance in production systems?\n",
    "\n",
    "### 6. Modern ML Context\n",
    "- How do these classical algorithms relate to modern deep learning?\n",
    "- Can concepts from version spaces be applied to neural networks?\n",
    "- What can we learn from these simple algorithms that applies to complex models?\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Summary & Key Takeaways\n",
    "\n",
    "### What We Accomplished\n",
    "\n",
    "In this experiment, we successfully:\n",
    "- ‚úÖ **Implemented Find-S Algorithm** - Found the most specific hypothesis from positive examples\n",
    "- ‚úÖ **Implemented Candidate Elimination** - Maintained version space with S and G boundaries\n",
    "- ‚úÖ **Visualized Hypothesis Evolution** - Saw how hypotheses change with each example\n",
    "- ‚úÖ **Compared Both Algorithms** - Understood strengths and weaknesses of each approach\n",
    "- ‚úÖ **Tested on New Data** - Evaluated generalization performance\n",
    "\n",
    "### Key Technical Concepts\n",
    "\n",
    "**1. Concept Learning:**\n",
    "Learning a boolean-valued function from examples is the foundation of classification. These algorithms show how machines can learn patterns just like humans do‚Äîby observing examples and generalizing.\n",
    "\n",
    "**2. Hypothesis Space:**\n",
    "The set of all possible hypotheses defines what patterns we can learn. Choosing the right hypothesis representation is crucial‚Äîtoo simple and you can't capture the concept, too complex and you overfit.\n",
    "\n",
    "**3. Version Space:**\n",
    "Representing ALL consistent hypotheses (not just one) gives us a measure of uncertainty. A large version space means we need more data; a small one means we're confident in our learning.\n",
    "\n",
    "**4. Inductive Bias:**\n",
    "Both algorithms have bias‚Äîthey prefer certain hypotheses over others. Find-S prefers specific hypotheses, Candidate Elimination maintains all consistent ones. Understanding bias is key to understanding any ML algorithm.\n",
    "\n",
    "**5. Generalization:**\n",
    "The process of moving from specific examples to general rules is at the heart of machine learning. We saw this explicitly in how '?' replaces specific values as we see more examples.\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "**1. Rule-Based Expert Systems:**\n",
    "Medical diagnosis systems use similar concept learning to identify disease patterns from symptoms. For example, learning rules like \"IF fever AND cough AND fatigue THEN likely flu\" from patient records.\n",
    "\n",
    "**2. Recommendation Systems:**\n",
    "E-commerce platforms learn user preferences using similar principles. \"This user likes products that are: electronics AND under $100 AND highly-rated\" helps recommend relevant items.\n",
    "\n",
    "**3. Fraud Detection:**\n",
    "Financial institutions learn patterns of fraudulent transactions. \"Transactions that are: international AND large amount AND unusual time AND new merchant\" trigger fraud alerts.\n",
    "\n",
    "**4. Email Filtering:**\n",
    "Spam filters learn what makes an email spam or important based on sender, subject, content patterns‚Äîexactly like our outdoor activity example but with email attributes.\n",
    "\n",
    "### Performance Insights\n",
    "\n",
    "From our experiment:\n",
    "- **Find-S**: Processed only positive examples, converged quickly to a single hypothesis\n",
    "- **Candidate Elimination**: Used all examples, maintained complete version space\n",
    "- **Computational Cost**: Find-S was faster (O(n)), Candidate Elimination more thorough but expensive\n",
    "- **Robustness**: Candidate Elimination more robust due to using negative examples\n",
    "\n",
    "### Lessons Learned\n",
    "\n",
    "**What Worked Well:**\n",
    "- Clear hypothesis representation made results interpretable\n",
    "- Step-by-step processing showed exactly how learning happens\n",
    "- Visualization helped understand the evolution of hypotheses\n",
    "- Both algorithms converged to consistent results\n",
    "\n",
    "**Challenges Faced:**\n",
    "- Limited to discrete attributes (can't handle continuous values directly)\n",
    "- Assumes noise-free data (real data is messy!)\n",
    "- Hypothesis space can grow exponentially with attributes\n",
    "- Only handles conjunctive concepts (AND), not disjunctions (OR)\n",
    "\n",
    "**Solutions and Extensions:**\n",
    "- Discretize continuous attributes into bins\n",
    "- Use ensemble methods to handle noise\n",
    "- Apply feature selection to reduce hypothesis space\n",
    "- Extend to disjunctive normal form for OR conditions\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "To continue learning and improving:\n",
    "\n",
    "1. **Complete the Assignments**\n",
    "   - Create your own dataset and apply both algorithms\n",
    "   - Analyze hypothesis space complexity\n",
    "   - Design a real-world application\n",
    "\n",
    "2. **Experiment Further**\n",
    "   - Try datasets with more attributes\n",
    "   - Test with imbalanced data (many more positive or negative examples)\n",
    "   - Implement incremental learning (update hypotheses as new data arrives)\n",
    "   - Add noise and see how algorithms fail (then think about solutions)\n",
    "\n",
    "3. **Dive Deeper**\n",
    "   - Study decision tree algorithms (ID3, C4.5) which extend these concepts\n",
    "   - Learn about rule-based classifiers (RIPPER, CN2)\n",
    "   - Explore probabilistic approaches (Naive Bayes)\n",
    "   - Understand modern interpretable ML (LIME, SHAP)\n",
    "\n",
    "4. **Build a Portfolio Project**\n",
    "   - Choose a real problem (email filtering, product recommendation, etc.)\n",
    "   - Collect or find appropriate data\n",
    "   - Implement a complete solution\n",
    "   - Compare with modern ML algorithms\n",
    "   - Document and share your findings\n",
    "\n",
    "### Resources for Further Learning\n",
    "\n",
    "**üìö Books:**\n",
    "- *Machine Learning* by Tom Mitchell (Chapter 2: Concept Learning)\n",
    "- *Pattern Recognition and Machine Learning* by Christopher Bishop\n",
    "- *The Elements of Statistical Learning* by Hastie, Tibshirani, Friedman\n",
    "\n",
    "**üìÑ Research Papers:**\n",
    "- Mitchell, T. (1982). \"Generalization as Search\" - Original version space paper\n",
    "- Haussler, D. (1988). \"Quantifying Inductive Bias\" - Understanding learning bias\n",
    "\n",
    "**üõ†Ô∏è Tools & Libraries:**\n",
    "- **scikit-learn**: Modern ML library with decision trees and rule learners\n",
    "- **Orange**: Visual programming for data mining with rule induction\n",
    "- **Weka**: Java-based ML toolkit with various rule learners\n",
    "\n",
    "**üí° Related Topics:**\n",
    "- Decision Trees (ID3, C4.5, CART)\n",
    "- Rule-Based Learning (RIPPER, CN2, AQ)\n",
    "- Inductive Logic Programming (FOIL, Progol)\n",
    "- Active Learning (using version spaces to select informative examples)\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ Congratulations!\n",
    "\n",
    "You've completed Experiment 1! You now have hands-on experience with:\n",
    "- **Concept learning fundamentals** - How machines learn from examples\n",
    "- **Find-S algorithm** - Finding specific hypotheses from positive examples\n",
    "- **Candidate Elimination** - Maintaining version spaces with S and G boundaries\n",
    "- **Hypothesis evolution** - Understanding how learning progresses\n",
    "- **Practical applications** - Connecting theory to real-world problems\n",
    "\n",
    "These are foundational concepts that underpin all of machine learning. Every modern ML algorithm‚Äîfrom decision trees to deep neural networks‚Äîbuilds on these principles of learning from examples and generalizing to new data.\n",
    "\n",
    "### üì£ What's Next?\n",
    "\n",
    "**In Module 1:**\n",
    "- Continue with other concept learning experiments\n",
    "- Explore decision tree learning\n",
    "- Study ensemble methods\n",
    "\n",
    "**In Future Modules:**\n",
    "- Module 2: Rule Learning and Analytical Learning\n",
    "- Module 3: Bayesian Learning\n",
    "- Module 4: Neural Networks and Deep Learning\n",
    "\n",
    "### üí° Final Thoughts\n",
    "\n",
    "Remember:\n",
    "- **Start simple**: These basic algorithms teach fundamental principles\n",
    "- **Understand deeply**: Knowing WHY algorithms work is more valuable than just using them\n",
    "- **Practice regularly**: Implement algorithms from scratch to truly understand them\n",
    "- **Think critically**: Always question assumptions and limitations\n",
    "- **Apply creatively**: Look for opportunities to use ML in your domain\n",
    "\n",
    "**Keep Learning, Keep Building! üöÄ**\n",
    "\n",
    "---\n",
    "\n",
    "### üìß Need Help?\n",
    "\n",
    "**Questions or Issues?**\n",
    "- ‚úÖ Review the FAQ section above\n",
    "- ‚úÖ Check the Module 1 reference materials\n",
    "- ‚úÖ Discuss with your peers\n",
    "- ‚úÖ Contact your instructor\n",
    "\n",
    "**Want to Contribute?**\n",
    "- Share your custom datasets\n",
    "- Suggest improvements to the notebook\n",
    "- Help other students\n",
    "- Create additional examples\n",
    "\n",
    "---\n",
    "\n",
    "**End of Experiment 1** | **Arivu AI Machine Learning Course** | **Module 1: Introduction & Concept Learning**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

