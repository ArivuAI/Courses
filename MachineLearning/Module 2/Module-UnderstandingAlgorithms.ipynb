{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71b3eec8",
   "metadata": {},
   "source": [
    "# Understanding Algorithm Notation: A Guide to Reading Algorithms\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Reading and understanding algorithms is a crucial skill for computer science students. Algorithms are often presented using mathematical notation and symbols that may seem intimidating at first. This guide will help you decode common algorithmic notation and build confidence in interpreting complex algorithms like AdaBoost.\n",
    "\n",
    "## Key Mathematical Symbols in Algorithms\n",
    "\n",
    "### Basic Notation\n",
    "\n",
    "- **Variables**: Usually represented by letters (x, y, z, α, β, γ)\n",
    "- **Subscripts**: Used to denote specific instances or iterations (x₁, x₂, ..., xₙ)\n",
    "- **Superscripts**: Often indicate powers or iteration numbers (x⁽ᵗ⁾ for iteration t)\n",
    "\n",
    "### Set Theory Symbols\n",
    "\n",
    "- **∈**: \"belongs to\" or \"is an element of\"\n",
    "    - Example: x ∈ X means \"x belongs to set X\"\n",
    "- **∀**: \"for all\" (universal quantifier)\n",
    "    - Example: ∀i ∈ {1, 2, ..., n} means \"for all i from 1 to n\"\n",
    "- **∃**: \"there exists\" (existential quantifier)\n",
    "- **⊆**: \"subset of\"\n",
    "\n",
    "### Summation and Products\n",
    "\n",
    "- **Σ**: Summation symbol\n",
    "    - Example: Σᵢ₌₁ⁿ xᵢ = x₁ + x₂ + ... + xₙ\n",
    "- **∏**: Product symbol\n",
    "    - Example: ∏ᵢ₌₁ⁿ xᵢ = x₁ × x₂ × ... × xₙ\n",
    "\n",
    "### Functions and Mappings\n",
    "\n",
    "- **f: X → Y**: Function f maps from domain X to codomain Y\n",
    "- **argmin/argmax**: Argument that minimizes/maximizes a function\n",
    "    - Example: argmin f(x) returns the value of x that minimizes f(x)\n",
    "\n",
    "## Reading Algorithm Pseudocode\n",
    "\n",
    "### Common Patterns\n",
    "\n",
    "1. **Initialization**: Setting up variables and data structures\n",
    "2. **Iteration**: Loops using \"for\" or \"while\" constructs\n",
    "3. **Conditionals**: \"if-then-else\" statements\n",
    "4. **Updates**: Modifying variables based on computations\n",
    "\n",
    "### Example: Simple Algorithm Structure\n",
    "\n",
    "```\n",
    "Algorithm: Example\n",
    "Input: Dataset D = {(x₁, y₁), ..., (xₙ, yₙ)}\n",
    "Output: Trained model h\n",
    "\n",
    "1. Initialize: Set w₀ = 0\n",
    "2. For t = 1 to T do:\n",
    "     3. For each (xᵢ, yᵢ) ∈ D do:\n",
    "            4. Compute prediction ŷᵢ = sign(wₜ · xᵢ)\n",
    "            5. If ŷᵢ ≠ yᵢ then:\n",
    "                 6. Update: wₜ₊₁ = wₜ + yᵢxᵢ\n",
    "3. Return: Final weights wₜ\n",
    "```\n",
    "\n",
    "## Tips for Reading Complex Algorithms\n",
    "\n",
    "### 1. Start with the Big Picture\n",
    "- Read the algorithm title and description\n",
    "- Identify inputs and outputs\n",
    "- Understand the overall goal\n",
    "\n",
    "### 2. Break Down Each Step\n",
    "- Don't rush through mathematical expressions\n",
    "- Identify what each variable represents\n",
    "- Trace through small examples manually\n",
    "\n",
    "### 3. Focus on Key Operations\n",
    "- Look for patterns: initialization, iteration, updates\n",
    "- Identify the core computational steps\n",
    "- Understand stopping conditions\n",
    "\n",
    "### 4. Practice with Examples\n",
    "- Work through algorithms with simple datasets\n",
    "- Verify your understanding by implementing the algorithm\n",
    "- Compare results with expected outcomes\n",
    "\n",
    "## Mathematical Foundations You Should Know\n",
    "\n",
    "### Probability and Statistics\n",
    "- **P(A)**: Probability of event A\n",
    "- **E[X]**: Expected value of random variable X\n",
    "- **∇**: Gradient operator (for optimization algorithms)\n",
    "\n",
    "### Linear Algebra\n",
    "- **‖x‖**: Norm of vector x (usually L2 norm)\n",
    "- **xᵀy**: Dot product of vectors x and y\n",
    "- **A⁻¹**: Inverse of matrix A\n",
    "\n",
    "### Optimization\n",
    "- **min/max**: Minimize/maximize an objective function\n",
    "- **subject to**: Constraints in optimization problems\n",
    "- **∇f(x) = 0**: First-order optimality condition\n",
    "\n",
    "## Building Your Algorithm Reading Skills\n",
    "\n",
    "1. **Start Simple**: Begin with basic algorithms (sorting, searching)\n",
    "2. **Use Multiple Sources**: Read different explanations of the same algorithm\n",
    "3. **Implement as You Learn**: Code the algorithms to verify understanding\n",
    "4. **Draw Diagrams**: Visualize the algorithm's flow and data transformations\n",
    "5. **Practice Regularly**: Make algorithm reading a daily habit\n",
    "\n",
    "## Common Mistakes to Avoid\n",
    "\n",
    "- Skipping over mathematical notation without understanding\n",
    "- Not keeping track of variable definitions\n",
    "- Ignoring algorithm assumptions and prerequisites\n",
    "- Rushing through complex expressions\n",
    "- Not testing understanding with examples\n",
    "\n",
    "Remember: Algorithm notation is a language that becomes more familiar with practice. Don't be discouraged if complex algorithms seem overwhelming at first – even experienced programmers take time to fully understand sophisticated algorithms like AdaBoost!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c8f370",
   "metadata": {},
   "source": [
    "## Greek Letters in Algorithm Notation\n",
    "\n",
    "### Uppercase Greek Letters\n",
    "\n",
    "| Symbol | Name | Common Usage in Algorithms |\n",
    "|--------|------|---------------------------|\n",
    "| Α | Alpha | Rarely used in uppercase |\n",
    "| Β | Beta | Rarely used in uppercase |\n",
    "| Γ | Gamma | Function spaces, complexity classes |\n",
    "| Δ | Delta | Change, difference, error |\n",
    "| Ε | Epsilon | Rarely used in uppercase |\n",
    "| Ζ | Zeta | Rarely used in uppercase |\n",
    "| Η | Eta | Rarely used in uppercase |\n",
    "| Θ | Theta | Big-Theta notation (tight bounds) |\n",
    "| Ι | Iota | Rarely used |\n",
    "| Κ | Kappa | Rarely used in uppercase |\n",
    "| Λ | Lambda | Eigenvalues, regularization parameter |\n",
    "| Μ | Mu | Rarely used in uppercase |\n",
    "| Ν | Nu | Rarely used in uppercase |\n",
    "| Ξ | Xi | Random variables |\n",
    "| Ο | Omicron | Big-O notation (upper bounds) |\n",
    "| Π | Pi | Product notation, probability |\n",
    "| Ρ | Rho | Rarely used in uppercase |\n",
    "| Σ | Sigma | Summation notation |\n",
    "| Τ | Tau | Rarely used in uppercase |\n",
    "| Υ | Upsilon | Rarely used |\n",
    "| Φ | Phi | Feature mappings, potential functions |\n",
    "| Χ | Chi | Chi-squared distribution |\n",
    "| Ψ | Psi | Wave functions, basis functions |\n",
    "| Ω | Omega | Big-Omega notation (lower bounds) |\n",
    "\n",
    "### Lowercase Greek Letters\n",
    "\n",
    "| Symbol | Name | Common Usage in Algorithms |\n",
    "|--------|------|---------------------------|\n",
    "| α | alpha | Learning rate, significance level, weights |\n",
    "| β | beta | Regularization parameter, coefficients |\n",
    "| γ | gamma | Discount factor, margin parameter |\n",
    "| δ | delta | Small change, Kronecker delta |\n",
    "| ε | epsilon | Error tolerance, small positive number |\n",
    "| ζ | zeta | Rarely used |\n",
    "| η | eta | Learning rate, step size |\n",
    "| θ | theta | Parameters, angles |\n",
    "| ι | iota | Rarely used |\n",
    "| κ | kappa | Condition number, curvature |\n",
    "| λ | lambda | Eigenvalue, regularization parameter |\n",
    "| μ | mu | Mean, expected value |\n",
    "| ν | nu | Degrees of freedom, frequency |\n",
    "| ξ | xi | Random variable, noise |\n",
    "| ο | omicron | Rarely used (looks like 'o') |\n",
    "| π | pi | Mathematical constant 3.14159..., probability |\n",
    "| ρ | rho | Correlation coefficient, density |\n",
    "| σ | sigma | Standard deviation, activation function |\n",
    "| τ | tau | Time constant, threshold |\n",
    "| υ | upsilon | Rarely used |\n",
    "| φ | phi | Feature function, basis function |\n",
    "| χ | chi | Chi statistic |\n",
    "| ψ | psi | Wave function, potential |\n",
    "| ω | omega | Angular frequency, weights |\n",
    "\n",
    "### Special Usage Notes\n",
    "\n",
    "**Most Common in Machine Learning:**\n",
    "- **α (alpha)**: Learning rate in gradient descent, weight in AdaBoost\n",
    "- **β (beta)**: Regularization strength, momentum parameter\n",
    "- **γ (gamma)**: Discount factor in reinforcement learning\n",
    "- **δ (delta)**: Error signal, gradient updates\n",
    "- **ε (epsilon)**: Convergence tolerance, exploration rate\n",
    "- **η (eta)**: Learning rate, step size\n",
    "- **θ (theta)**: Model parameters\n",
    "- **λ (lambda)**: Regularization parameter\n",
    "- **μ (mu)**: Mean of distributions\n",
    "- **σ (sigma)**: Standard deviation, sigmoid function\n",
    "- **τ (tau)**: Temperature parameter, time steps\n",
    "\n",
    "**Pronunciation Tips:**\n",
    "- **χ (chi)**: Pronounced \"kai\" (like \"sky\" without the 's')\n",
    "- **ψ (psi)**: Pronounced \"sigh\"\n",
    "- **ξ (xi)**: Pronounced \"zai\" or \"ksie\"\n",
    "- **φ (phi)**: Pronounced \"fie\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea986fc7",
   "metadata": {},
   "source": [
    "# AdaBoost Algorithm: Step-by-Step Breakdown\n",
    "\n",
    "## Algorithm Overview\n",
    "\n",
    "AdaBoost (Adaptive Boosting) is an ensemble learning method that combines multiple weak learners to create a strong classifier. The key insight is to iteratively train weak classifiers and adjust the importance of training examples based on previous classification errors.\n",
    "\n",
    "![images/AdaBoost.png](images/AdaBoost.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225e5bfe",
   "metadata": {},
   "source": [
    "## AdaBoost — Symbols & Notation\n",
    "\n",
    "Below are the symbols and notation used in the AdaBoost algorithm image (plain English):\n",
    "\n",
    "- **n**: number of training examples (dataset size).\n",
    "- **w<sub>i</sub><sup>(1)</sup> = 1/n**: initial weight for example *i* (all examples start with equal weight).\n",
    "- **w<sub>i</sub><sup>(t)</sup>**: weight of example *i* at boosting iteration *t* (a probability-like weight).\n",
    "- **t**: iteration index (t = 1, 2, ..., T).\n",
    "- **h<sub>t</sub>(x)**: the weak learner (classifier) produced at iteration *t*. Usually returns +1 or −1.\n",
    "- **y<sub>i</sub>**: true label for example *i*, typically coded as +1 or −1.\n",
    "- **ε<sub>t</sub> = Σ<sub>i=1</sub><sup>n</sup> 1[h<sub>t</sub>(x<sub>i</sub>) ≠ y<sub>i</sub>] · w<sub>i</sub><sup>(t)</sup>**: the weighted error of the weak learner *h<sub>t</sub>*.\n",
    "  - **1[condition]** is the indicator function: 1 if condition true, else 0.\n",
    "  - Intuition: ε<sub>t</sub> is the total weight of examples misclassified by h<sub>t</sub>.\n",
    "- **α<sub>t</sub> = (1/2) · ln((1 − ε<sub>t</sub>) / ε<sub>t</sub>)**: the weight (importance) assigned to weak learner *h<sub>t</sub>*.\n",
    "  - If ε<sub>t</sub> < 0.5 then α<sub>t</sub> > 0 (useful learner). Smaller ε<sub>t</sub> → larger α<sub>t</sub>.\n",
    "- **w<sub>i</sub><sup>(t+1)</sup> = w<sub>i</sub><sup>(t)</sup> · exp(−α<sub>t</sub> · y<sub>i</sub> · h<sub>t</sub>(x<sub>i</sub>))**: multiplicative update of example weights.\n",
    "  - Because y<sub>i</sub>·h<sub>t</sub>(x<sub>i</sub>) = +1 for correct classification and −1 for misclassification:\n",
    "    - correctly classified → multiply by exp(−α<sub>t</sub>) (weight decreases),\n",
    "    - misclassified → multiply by exp(+α<sub>t</sub>) (weight increases).\n",
    "- **Renormalize weights** so that Σ<sub>i=1</sub><sup>n</sup> w<sub>i</sub><sup>(t+1)</sup> = 1 (divide by the sum).\n",
    "- **H(x) = sign(Σ<sub>t=1</sub><sup>T</sup> α<sub>t</sub> · h<sub>t</sub>(x))**: final strong classifier — weighted majority vote of weak learners.\n",
    "  - **sign(z)** returns +1 if z > 0 and −1 if z < 0 (handle z = 0 as you prefer).\n",
    "\n",
    "### Additional notes / practical points:\n",
    "- Labels must be in {+1, −1}. If your data uses {0,1}, map 0 → −1 and 1 → +1 before using formulas.\n",
    "- **ε<sub>t</sub>** must be in (0,1). AdaBoost expects ε<sub>t</sub> < 0.5 (weak learner better than random). If ε<sub>t</sub> ≥ 0.5, the learner is not helpful and you should stop or change the base learner.\n",
    "- Numerical stability: protect against ε<sub>t</sub> = 0 or ε<sub>t</sub> = 1 when computing ln((1−ε)/ε). Clamp ε to a small interval like [1e-12, 1−1e-12] or handle perfect learners separately.\n",
    "- The multiplicative update uses exp(...) — equivalent to minimizing an exponential loss; the product of normalization constants bounds training error.\n",
    "- The weights w<sub>i</sub> form a distribution over examples (they sum to 1). Later learners focus on examples with larger weights (hard cases).\n",
    "\n",
    "This list matches the notation in the image and gives quick guidance for reading and implementing the algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "module1_dl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
