# Module 2: Quick Reference Guide

## üöÄ Sequential Covering, FOIL, and EBL

---

## üìã Sequential Covering Algorithm

### Pseudocode

```
function SEQUENTIAL_COVERING(Examples, Attributes, TargetAttr, TargetClass):
    LearnedRules = []
    RemainingExamples = Examples
    
    while positive examples remain in RemainingExamples:
        NewRule = LEARN_ONE_RULE(RemainingExamples, Attributes, TargetAttr, TargetClass)
        
        if NewRule.quality < threshold:
            break
        
        LearnedRules.append(NewRule)
        
        # Remove correctly covered positive examples
        RemainingExamples = RemainingExamples - CoveredBy(NewRule)
    
    return LearnedRules
```

### Learn-One-Rule (General-to-Specific)

```
function LEARN_ONE_RULE(Examples, Attributes, TargetAttr, TargetClass):
    Rule = {conditions: {}, prediction: TargetClass}
    
    while Rule.accuracy < threshold and Rule.coverage >= min_coverage:
        BestGain = -‚àû
        BestAttr = None
        BestValue = None
        
        for each Attribute in Attributes not in Rule.conditions:
            for each Value of Attribute:
                CandidateRule = Rule + (Attribute = Value)
                Gain = InformationGain(CandidateRule)
                
                if Gain > BestGain:
                    BestGain = Gain
                    BestAttr = Attribute
                    BestValue = Value
        
        if BestAttr is None:
            break
        
        Rule.conditions[BestAttr] = BestValue
    
    return Rule
```

### Key Characteristics

- **Search Strategy**: General-to-specific (start broad, add constraints)
- **Evaluation**: Information gain or accuracy improvement
- **Stopping Criteria**: Accuracy threshold or no improvement
- **Output**: Disjunctive set of rules (Rule1 OR Rule2 OR ...)

---

## üîß FOIL Algorithm

### Pseudocode

```
function FOIL(TargetPredicate, Predicates, PositiveExamples, NegativeExamples):
    LearnedRules = []
    Pos = PositiveExamples
    
    while Pos is not empty:
        NewRule = FOIL_LEARN_ONE_RULE(TargetPredicate, Predicates, Pos, NegativeExamples)
        LearnedRules.append(NewRule)
        Pos = Pos - CoveredBy(NewRule)
    
    return LearnedRules

function FOIL_LEARN_ONE_RULE(Target, Predicates, Pos, Neg):
    Rule = Target(x, y, ...) ‚Üê  # Initialize with target variables
    PosBindings = InitialBindings(Pos, Target)
    NegBindings = InitialBindings(Neg, Target)
    
    while NegBindings is not empty:
        Candidates = GENERATE_CANDIDATES(Rule, Predicates)
        BestLiteral = None
        BestGain = -‚àû
        
        for each Literal in Candidates:
            Gain = FOIL_GAIN(Literal, Rule, PosBindings, NegBindings)
            if Gain > BestGain:
                BestGain = Gain
                BestLiteral = Literal
        
        Rule = Rule + BestLiteral
        PosBindings = UpdateBindings(PosBindings, BestLiteral)
        NegBindings = UpdateBindings(NegBindings, BestLiteral)
    
    return Rule
```

### FoilGain Metric

```
FoilGain(L, Rule) = t √ó (log‚ÇÇ(p‚ÇÅ/(p‚ÇÅ+n‚ÇÅ)) - log‚ÇÇ(p‚ÇÄ/(p‚ÇÄ+n‚ÇÄ)))

Where:
    t  = number of positive bindings covered by Rule
    p‚ÇÄ = positive bindings before adding literal L
    n‚ÇÄ = negative bindings before adding literal L
    p‚ÇÅ = positive bindings after adding literal L
    n‚ÇÅ = negative bindings after adding literal L
```

### Three Types of Candidate Literals

**Type 1: New Predicate with Variables**
```
Q(v‚ÇÅ, v‚ÇÇ, ..., v·µ£)
```
- At least ONE variable must already exist in rule
- Can introduce NEW variables
- Example: `Parent(x, z)` where x exists, z is new

**Type 2: Equality Test**
```
Equal(x·µ¢, x‚±º)
```
- Both variables must already exist in rule
- Tests if two objects are the same

**Type 3: Negations**
```
¬¨Q(...) or ¬¨Equal(...)
```
- Negation of Types 1 or 2
- Example: `¬¨Friend(x, z)`

### Example: Learning Grandparent(x, y)

```
Iteration 1:
    Rule: Grandparent(x, y) ‚Üê
    (Covers everything)

Iteration 2:
    Best literal: Parent(x, z)  [introduces variable z]
    Rule: Grandparent(x, y) ‚Üê Parent(x, z)
    (x is parent of someone)

Iteration 3:
    Best literal: Parent(z, y)  [completes chain]
    Rule: Grandparent(x, y) ‚Üê Parent(x, z) ‚àß Parent(z, y)
    (x is parent of z, z is parent of y)
    ‚úì Perfect accuracy!
```

---

## üß¨ Explanation-Based Learning (EBL)

### Three-Step Process

```
function EBL(TrainingExample, DomainTheory, TargetConcept):
    # Step 1: EXPLAIN
    ProofTree = EXPLAIN(TrainingExample, DomainTheory, TargetConcept)
    
    # Step 2: ANALYZE
    GeneralConditions = ANALYZE(ProofTree)
    
    # Step 3: REFINE
    OperationalRule = REFINE(GeneralConditions)
    
    return OperationalRule
```

### Step 1: EXPLAIN

Build a proof tree showing why the example satisfies the target concept.

```
Example: SafeToStack(Obj1, Obj2)

Proof Tree:
    SafeToStack(Obj1, Obj2)
        ‚Üì [Rule: SafeToStack(x,y) ‚Üê Lighter(x,y)]
    Lighter(Obj1, Obj2)
        ‚Üì [Rule: Lighter(x,y) ‚Üê Weight(x,wx) ‚àß Weight(y,wy) ‚àß wx < wy]
    Weight(Obj1, 5) ‚àß Weight(Obj2, 12) ‚àß 5 < 12
        ‚Üì [Rule: Weight(x,w) ‚Üê Volume(x,v) ‚àß Density(x,d) ‚àß w = v√ód]
    Volume(Obj1, 10) ‚àß Density(Obj1, 0.5) ‚àß 5 = 10√ó0.5
    ‚àß Volume(Obj2, 20) ‚àß Density(Obj2, 0.6) ‚àß 12 = 20√ó0.6
    ‚àß 5 < 12
        ‚Üì
    TRUE ‚úì
```

### Step 2: ANALYZE

Extract the weakest preimage (most general conditions).

```
Replace specific values with variables:
    Obj1 ‚Üí x
    Obj2 ‚Üí y
    10, 0.5, 5 ‚Üí vx, dx, (vx √ó dx)
    20, 0.6, 12 ‚Üí vy, dy, (vy √ó dy)

Weakest Preimage:
    "For ANY x and y where (vx √ó dx) < (vy √ó dy)"
```

### Step 3: REFINE

Create operational rule.

```
Learned Rule:
    SafeToStack(x, y) ‚Üê 
        Volume(x, vx) ‚àß Density(x, dx) ‚àß
        Volume(y, vy) ‚àß Density(y, dy) ‚àß
        (vx √ó dx) < (vy √ó dy)

Operational: Direct computation, no intermediate reasoning!
```

### Perfect Domain Theory

A domain theory is **perfect** if:
1. **Correct**: Every assertion is TRUE
2. **Complete**: Every positive example can be proven

Examples:
- Chess legal move rules ‚úì
- Physics laws (F=ma) ‚úì
- Logic circuit behaviors ‚úì

### The EBL Paradox

**Question**: "If we have perfect knowledge, why learn?"

**Answer**: Transform deep, slow knowledge into fast operational rules.

```
BEFORE EBL:
    SafeToStack ‚Üí Lighter ‚Üí Weight ‚Üí Volume√óDensity
    (3-step reasoning chain, slow)

AFTER EBL:
    SafeToStack ‚Üí Direct check: vx√ódx < vy√ódy
    (1-step, fast!)
```

---

## üìä Comparison Table

### Sequential Covering vs Decision Trees

| Aspect | Sequential Covering | Decision Trees |
|--------|-------------------|----------------|
| Learning Style | One rule at a time | All rules simultaneously |
| Rule Independence | Independent rules | Shared structure |
| Natural Form | Disjunctive (OR) | Conjunctive (AND) |
| Modification | Easy to add/remove rules | Must rebuild tree |
| Explainability | Very high | High |

### Inductive vs Analytical Learning

| Aspect | Inductive | Analytical (EBL) |
|--------|-----------|------------------|
| Input | Data + Hypothesis space | Data + Hypothesis space + **Domain theory** |
| Sample Complexity | High (many examples) | Low (few examples) |
| Novelty Discovery | Can find new patterns | Reformulates existing knowledge |
| Guarantee | Statistical | Deductive (if theory correct) |
| Explainability | Often black box | Fully explainable |
| Robustness | Handles noise well | Sensitive to theory errors |

### FOIL vs Propositional Rules

| Aspect | Propositional | First-Order (FOIL) |
|--------|--------------|-------------------|
| Variables | No | Yes |
| Expressiveness | Single objects | Relationships between objects |
| Generalization | One rule per instance type | One rule for infinite instances |
| Recursion | No | Yes |
| Example | `age > 65 ‚Üí senior` | `Grandparent(x,y) ‚Üê Parent(x,z) ‚àß Parent(z,y)` |

---

## üí° Quick Tips

### When to Use Sequential Covering
‚úì Need explainable rules  
‚úì Domain experts must validate  
‚úì Regulatory compliance  
‚úì Disjunctive concepts  

### When to Use FOIL
‚úì Relational data (databases, graphs)  
‚úì Complex relationships between objects  
‚úì Need recursive rules  
‚úì Knowledge graph reasoning  

### When to Use EBL
‚úì Limited training data  
‚úì Strong domain theory available  
‚úì Need fast operational rules  
‚úì Correctness is critical  

### When NOT to Use Rule-Based Learning
‚úó High-dimensional perceptual data (images, audio)  
‚úó Complex non-linear patterns  
‚úó Massive datasets where interpretability not needed  
‚úó No good domain theory (for EBL)  

---

## üîë Key Formulas

### Information Gain
```
Gain(S, A) = Entropy(S) - Œ£(|S·µ•|/|S|) √ó Entropy(S·µ•)

Entropy(S) = -Œ£ p·µ¢ log‚ÇÇ(p·µ¢)
```

### FoilGain
```
FoilGain(L, Rule) = t √ó (log‚ÇÇ(p‚ÇÅ/(p‚ÇÅ+n‚ÇÅ)) - log‚ÇÇ(p‚ÇÄ/(p‚ÇÄ+n‚ÇÄ)))
```

### Rule Accuracy
```
Accuracy = (Correctly Covered Examples) / (Total Covered Examples)
```

### Rule Coverage
```
Coverage = Number of examples matching rule conditions
```

---

## üìö Study Checklist

- [ ] Understand sequential covering outer loop
- [ ] Implement learn-one-rule (general-to-specific)
- [ ] Explain difference from decision trees
- [ ] Describe three types of FOIL literals
- [ ] Calculate FoilGain for candidate literals
- [ ] Understand variable bindings in FOIL
- [ ] Walk through EBL three steps
- [ ] Explain the EBL paradox
- [ ] Compare inductive vs analytical learning
- [ ] Choose appropriate method for given problem

---

## üéØ Common Pitfalls

1. **Confusing sequential covering with decision trees**
   - Sequential: Rules learned independently
   - Decision trees: Rules share structure

2. **Forgetting to remove covered examples**
   - Must remove after each rule in sequential covering
   - Otherwise, same rule learned repeatedly

3. **Misunderstanding FoilGain**
   - Accounts for variable bindings, not just examples
   - One example can create multiple bindings

4. **Thinking EBL discovers new knowledge**
   - EBL reformulates existing knowledge
   - Cannot learn what's not in domain theory

5. **Using rule-based learning for wrong domains**
   - Not suitable for high-dimensional perceptual data
   - Deep learning better for images, audio

---

## üìñ Further Reading

- Mitchell, T. (1997). *Machine Learning*. Chapters 10-11
- Quinlan, J.R. (1990). "Learning Logical Definitions from Relations"
- DeJong, G. & Mooney, R. (1986). "Explanation-Based Learning"

---

**Quick Reference Complete! Use this guide for rapid review and exam preparation.** üåü

