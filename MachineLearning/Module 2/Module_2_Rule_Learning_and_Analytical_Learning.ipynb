{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2: Learning Sets of Rules & Analytical Learning\n",
    "\n",
    "## üéØ Sequential Covering, FOIL, and Explanation-Based Learning\n",
    "\n",
    "---\n",
    "\n",
    "**Arivu AI Machine Learning Course**  \n",
    "*A comprehensive, hands-on guide to rule-based machine learning*\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Module Overview\n",
    "\n",
    "Welcome to Module 2! In this module, we'll explore powerful rule-based learning algorithms that create **human-readable, explainable** models.\n",
    "\n",
    "### Why Rules Matter: The $50 Million Question\n",
    "\n",
    "Imagine you're a bank processing 10 million transactions daily:\n",
    "- Fraud costs **$50M annually**\n",
    "- False alarms annoy **100,000 legitimate customers**\n",
    "- Regulators demand **explainable decisions**\n",
    "\n",
    "**Neural Networks say:** *\"This is fraud\"* (but can't explain why)  \n",
    "**Rule-Based Systems say:** *\"This is fraud BECAUSE amount > $10,000 AND location = overseas AND time = 3am\"*\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "1. **Sequential Covering Algorithms** - Learn rules one at a time\n",
    "2. **FOIL Algorithm** - Learn first-order rules with variables\n",
    "3. **Explanation-Based Learning (EBL)** - Learn from prior knowledge\n",
    "4. **Inductive-Analytical Approaches** - Combine data and knowledge\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this module, you will be able to:\n",
    "\n",
    "‚úÖ Implement sequential covering algorithms from scratch  \n",
    "‚úÖ Design and apply FOIL to learn first-order rules  \n",
    "‚úÖ Use prior knowledge to guide learning (EBL)  \n",
    "‚úÖ Choose the right rule learning approach for your problem  \n",
    "‚úÖ Explain when rules are better than decision trees or neural networks  \n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import warnings\n",
    "from collections import defaultdict, Counter\n",
    "from itertools import combinations, product\n",
    "from copy import deepcopy\n",
    "import math\n",
    "\n",
    "# Configure visualization settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(\"üìä Visualization settings configured\")\n",
    "print(\"üöÄ Ready to learn rule-based machine learning!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_data(filename):\n",
    "    \"\"\"\n",
    "    Load JSON data from the data folder.\n",
    "    \n",
    "    Args:\n",
    "        filename: Name of the JSON file\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing the loaded data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(f'data/{filename}', 'r') as f:\n",
    "            data = json.load(f)\n",
    "        print(f\"‚úÖ Successfully loaded {filename}\")\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Error: {filename} not found in data folder\")\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"‚ùå Error: {filename} is not valid JSON\")\n",
    "        return None\n",
    "\n",
    "def print_section_header(title, emoji=\"üìå\"):\n",
    "    \"\"\"\n",
    "    Print a formatted section header.\n",
    "    \n",
    "    Args:\n",
    "        title: Section title\n",
    "        emoji: Emoji to display\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"{emoji} {title}\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "def calculate_entropy(examples, target_attr):\n",
    "    \"\"\"\n",
    "    Calculate entropy of a set of examples.\n",
    "    \n",
    "    Args:\n",
    "        examples: List of example dictionaries\n",
    "        target_attr: Name of the target attribute\n",
    "    \n",
    "    Returns:\n",
    "        Entropy value\n",
    "    \"\"\"\n",
    "    if len(examples) == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Count occurrences of each class\n",
    "    class_counts = Counter([ex[target_attr] for ex in examples])\n",
    "    total = len(examples)\n",
    "    \n",
    "    # Calculate entropy\n",
    "    entropy = 0\n",
    "    for count in class_counts.values():\n",
    "        if count > 0:\n",
    "            p = count / total\n",
    "            entropy -= p * math.log2(p)\n",
    "    \n",
    "    return entropy\n",
    "\n",
    "def calculate_information_gain(examples, attribute, target_attr):\n",
    "    \"\"\"\n",
    "    Calculate information gain for an attribute.\n",
    "    \n",
    "    Args:\n",
    "        examples: List of example dictionaries\n",
    "        attribute: Attribute to evaluate\n",
    "        target_attr: Name of the target attribute\n",
    "    \n",
    "    Returns:\n",
    "        Information gain value\n",
    "    \"\"\"\n",
    "    # Calculate entropy before split\n",
    "    total_entropy = calculate_entropy(examples, target_attr)\n",
    "    \n",
    "    # Get unique values for this attribute\n",
    "    values = set([ex[attribute] for ex in examples])\n",
    "    \n",
    "    # Calculate weighted entropy after split\n",
    "    weighted_entropy = 0\n",
    "    for value in values:\n",
    "        subset = [ex for ex in examples if ex[attribute] == value]\n",
    "        weight = len(subset) / len(examples)\n",
    "        weighted_entropy += weight * calculate_entropy(subset, target_attr)\n",
    "    \n",
    "    # Information gain is the reduction in entropy\n",
    "    return total_entropy - weighted_entropy\n",
    "\n",
    "print(\"‚úÖ Helper functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Understanding Rule-Based Learning\n",
    "\n",
    "## üß† Slide 3: How Humans Learn Rules\n",
    "\n",
    "### Learning Email Filtering Step by Step\n",
    "\n",
    "Let's simulate how humans naturally learn rules through trial, error, and refinement!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Human-Like Rule Learning: Email Spam Detection\", \"üß†\")\n",
    "\n",
    "# Simulate human learning process\n",
    "emails = [\n",
    "    {\"id\": 1, \"text\": \"FREE VIAGRA! Click now!!!\", \"spam\": True},\n",
    "    {\"id\": 2, \"text\": \"Meeting at 3pm tomorrow\", \"spam\": False},\n",
    "    {\"id\": 3, \"text\": \"You won $1,000,000!\", \"spam\": True},\n",
    "    {\"id\": 4, \"text\": \"Invoice for $500 attached\", \"spam\": False},\n",
    "]\n",
    "\n",
    "print(\"üìß Learning Process (Like a Human Brain):\\n\")\n",
    "\n",
    "# Email 1\n",
    "print(f\"Email {emails[0]['id']}: \\\"{emails[0]['text']}\\\" ‚Üí {'üö´ SPAM' if emails[0]['spam'] else '‚úÖ NOT SPAM'}\")\n",
    "print(\"   Your Brain: 'Hmm, all caps and FREE seems suspicious...'\")\n",
    "print(\"   Rule 1: IF contains('FREE') THEN spam\")\n",
    "print(\"   Memory: Remember this pattern\\n\")\n",
    "\n",
    "# Email 2\n",
    "print(f\"Email {emails[1]['id']}: \\\"{emails[1]['text']}\\\" ‚Üí {'üö´ SPAM' if emails[1]['spam'] else '‚úÖ NOT SPAM'}\")\n",
    "print(\"   Your Brain: 'Wait, this is fine. My rule still works!'\")\n",
    "print(\"   Rule 1 still valid\\n\")\n",
    "\n",
    "# Email 3\n",
    "print(f\"Email {emails[2]['id']}: \\\"{emails[2]['text']}\\\" ‚Üí {'üö´ SPAM' if emails[2]['spam'] else '‚úÖ NOT SPAM'}\")\n",
    "print(\"   Your Brain: 'Also spam, but different pattern... money amounts!'\")\n",
    "print(\"   Refined Rule: IF contains('FREE') OR (contains('won') AND contains('$')) THEN spam\")\n",
    "print(\"   Learning: Combining multiple patterns\\n\")\n",
    "\n",
    "# Email 4\n",
    "print(f\"Email {emails[3]['id']}: \\\"{emails[3]['text']}\\\" ‚Üí {'üö´ SPAM' if emails[3]['spam'] else '‚úÖ NOT SPAM'}\")\n",
    "print(\"   Your Brain: 'Oops! This has $ but it's legitimate...'\")\n",
    "print(\"   Final Rule: IF (contains('FREE') AND all_caps) OR\")\n",
    "print(\"               (contains('won') AND contains('$') AND no_attachment) THEN spam\")\n",
    "print(\"   Memory: Refined understanding\\n\")\n",
    "\n",
    "print(\"üí° Key Insight: Humans naturally learn rules through trial, error, and refinement\")\n",
    "print(\"   ‚Äîexactly what ML algorithms do!\\n\")\n",
    "\n",
    "# Visualize the learning process\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "stages = ['Initial\\n(No Rules)', 'After Email 1\\n(Rule 1)', 'After Email 3\\n(Rule 1 + 2)', 'After Email 4\\n(Refined Rules)']\n",
    "accuracy = [50, 75, 75, 100]  # Simulated accuracy\n",
    "complexity = [0, 1, 2, 3]  # Number of conditions\n",
    "\n",
    "x = np.arange(len(stages))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, accuracy, width, label='Accuracy (%)', color='#2ecc71', alpha=0.7)\n",
    "ax.bar(x + width/2, [c*20 for c in complexity], width, label='Rule Complexity (√ó20)', color='#3498db', alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Learning Stage', fontsize=12)\n",
    "ax.set_ylabel('Value', fontsize=12)\n",
    "ax.set_title('Human-Like Rule Learning Process', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(stages)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ This is exactly what Sequential Covering algorithms do!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Sequential Covering Algorithms\n",
    "\n",
    "## üéØ Slide 4-5: Sequential Covering - The Main Idea\n",
    "\n",
    "### Divide and Conquer Approach\n",
    "\n",
    "**Two Ways to Learn Rules:**\n",
    "\n",
    "1. **Decision Trees (ID3) - Simultaneous Covering:**\n",
    "   - Learn entire tree at once\n",
    "   - Each split affects all branches\n",
    "   - All rules share decisions at top nodes\n",
    "\n",
    "2. **Rule Learning (CN2, FOIL) - Sequential Covering:**\n",
    "   - Learn rules ONE AT A TIME\n",
    "   - Remove covered examples after each rule\n",
    "   - Each rule is independent\n",
    "\n",
    "**The Sequential Covering Process (Like Peeling an Onion):**\n",
    "\n",
    "1. Learn Rule 1 that covers many positive examples\n",
    "2. Remove all positive examples covered by Rule 1\n",
    "3. Learn Rule 2 on remaining examples\n",
    "4. Repeat until all (or most) positive examples are covered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fraud detection dataset\n",
    "print_section_header(\"Sequential Covering: Fraud Detection Example\", \"üéØ\")\n",
    "\n",
    "fraud_data = load_json_data('fraud_detection_dataset.json')\n",
    "\n",
    "print(\"üìä Dataset Description:\")\n",
    "print(f\"   {fraud_data['description']}\\n\")\n",
    "\n",
    "print(\"üîç Attributes:\")\n",
    "for attr, desc in fraud_data['attributes'].items():\n",
    "    print(f\"   ‚Ä¢ {attr}: {desc}\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "train_fraud = pd.DataFrame(fraud_data['training_data'])\n",
    "test_fraud = pd.DataFrame(fraud_data['test_data'])\n",
    "\n",
    "print(f\"\\nüìà Training Examples: {len(train_fraud)}\")\n",
    "print(f\"üìà Test Examples: {len(test_fraud)}\\n\")\n",
    "\n",
    "# Display sample data\n",
    "print(\"üìã Sample Training Data:\\n\")\n",
    "print(train_fraud.head(10).to_string(index=False))\n",
    "\n",
    "# Visualize class distribution\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "attributes = ['amount', 'location', 'time', 'merchant_type', 'frequency', 'card_present']\n",
    "colors_map = {'Yes': '#e74c3c', 'No': '#2ecc71'}\n",
    "\n",
    "for idx, attr in enumerate(attributes):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Count occurrences for each value\n",
    "    fraud_data_attr = train_fraud[train_fraud['is_fraud'] == 'Yes'][attr].value_counts()\n",
    "    legit_data_attr = train_fraud[train_fraud['is_fraud'] == 'No'][attr].value_counts()\n",
    "    \n",
    "    # Get all unique values\n",
    "    all_values = sorted(set(train_fraud[attr].unique()))\n",
    "    \n",
    "    # Create grouped bar chart\n",
    "    x = np.arange(len(all_values))\n",
    "    width = 0.35\n",
    "    \n",
    "    fraud_counts = [fraud_data_attr.get(val, 0) for val in all_values]\n",
    "    legit_counts = [legit_data_attr.get(val, 0) for val in all_values]\n",
    "    \n",
    "    ax.bar(x - width/2, fraud_counts, width, label='Fraud', color='#e74c3c', alpha=0.7)\n",
    "    ax.bar(x + width/2, legit_counts, width, label='Legitimate', color='#2ecc71', alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel(attr.replace('_', ' ').title(), fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel('Count', fontsize=10)\n",
    "    ax.set_title(f'{attr.replace(\"_\", \" \").title()} Distribution', fontsize=12, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(all_values, rotation=45, ha='right')\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Notice: Different attributes have different discriminative power!\")\n",
    "print(\"   Sequential covering will learn rules that exploit these patterns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Implementing Sequential Covering\n",
    "\n",
    "### Understanding the Algorithm\n",
    "\n",
    "**Sequential Covering works in two nested loops:**\n",
    "\n",
    "**OUTER LOOP (Sequential Covering):**\n",
    "- Learn one rule at a time\n",
    "- Remove examples covered by that rule\n",
    "- Repeat until stopping criterion met\n",
    "\n",
    "**INNER LOOP (Learn-One-Rule):**\n",
    "- Start with most general rule (covers everything)\n",
    "- Add constraints to specialize (general-to-specific search)\n",
    "- Stop when rule is accurate enough\n",
    "\n",
    "**Key Difference from Decision Trees:**\n",
    "- Decision trees: Learn all rules simultaneously (one tree)\n",
    "- Sequential covering: Learn rules independently (one at a time)\n",
    "\n",
    "**Why This Matters:**\n",
    "- Each rule can be very different from others\n",
    "- Easy to understand and modify individual rules\n",
    "- Natural for disjunctive concepts (Rule1 OR Rule2 OR Rule3)\n",
    "- Explainable to stakeholders (\"This transaction is fraud BECAUSE...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rule class to represent learned rules\n",
    "print_section_header(\"Rule Representation\", \"üìù\")\n",
    "\n",
    "class Rule:\n",
    "    \"\"\"\n",
    "    Represents a single if-then rule.\n",
    "    \n",
    "    A rule consists of:\n",
    "    - conditions: Dictionary of attribute-value pairs (the IF part)\n",
    "    - prediction: The predicted class (the THEN part)\n",
    "    - coverage: Number of examples covered\n",
    "    - accuracy: Proportion of covered examples correctly classified\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, conditions=None, prediction=None):\n",
    "        \"\"\"\n",
    "        Initialize a rule.\n",
    "        \n",
    "        Args:\n",
    "            conditions: Dictionary of {attribute: value} pairs\n",
    "            prediction: Predicted class label\n",
    "        \"\"\"\n",
    "        self.conditions = conditions if conditions else {}\n",
    "        self.prediction = prediction\n",
    "        self.coverage = 0\n",
    "        self.accuracy = 0.0\n",
    "        self.positive_covered = 0\n",
    "        self.negative_covered = 0\n",
    "    \n",
    "    def matches(self, example):\n",
    "        \"\"\"\n",
    "        Check if this rule matches (covers) an example.\n",
    "        \n",
    "        Args:\n",
    "            example: Dictionary representing an example\n",
    "        \n",
    "        Returns:\n",
    "            True if all conditions are satisfied, False otherwise\n",
    "        \"\"\"\n",
    "        for attr, value in self.conditions.items():\n",
    "            if attr not in example or example[attr] != value:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def evaluate(self, examples, target_attr):\n",
    "        \"\"\"\n",
    "        Evaluate rule performance on a set of examples.\n",
    "        \n",
    "        Args:\n",
    "            examples: List of example dictionaries\n",
    "            target_attr: Name of the target attribute\n",
    "        \"\"\"\n",
    "        covered = [ex for ex in examples if self.matches(ex)]\n",
    "        self.coverage = len(covered)\n",
    "        \n",
    "        if self.coverage > 0:\n",
    "            correct = sum(1 for ex in covered if ex[target_attr] == self.prediction)\n",
    "            self.accuracy = correct / self.coverage\n",
    "            \n",
    "            # Count positive and negative examples covered\n",
    "            self.positive_covered = sum(1 for ex in covered if ex[target_attr] == self.prediction)\n",
    "            self.negative_covered = self.coverage - self.positive_covered\n",
    "        else:\n",
    "            self.accuracy = 0.0\n",
    "            self.positive_covered = 0\n",
    "            self.negative_covered = 0\n",
    "    \n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        String representation of the rule.\n",
    "        \"\"\"\n",
    "        if not self.conditions:\n",
    "            return f\"IF TRUE THEN {self.prediction}\"\n",
    "        \n",
    "        conditions_str = \" AND \".join([f\"{attr}={value}\" for attr, value in self.conditions.items()])\n",
    "        return f\"IF {conditions_str} THEN {self.prediction}\"\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "# Example: Create and test a rule\n",
    "print(\"üìã Example Rule Creation:\\n\")\n",
    "\n",
    "# Create a rule: IF amount=VeryHigh AND location=Overseas THEN Fraud\n",
    "example_rule = Rule(\n",
    "    conditions={'amount': 'VeryHigh', 'location': 'Overseas'},\n",
    "    prediction='Yes'\n",
    ")\n",
    "\n",
    "print(f\"Rule: {example_rule}\")\n",
    "\n",
    "# Test on training data\n",
    "example_rule.evaluate(fraud_data['training_data'], 'is_fraud')\n",
    "\n",
    "print(f\"\\nPerformance:\")\n",
    "print(f\"   Coverage: {example_rule.coverage} examples\")\n",
    "print(f\"   Accuracy: {example_rule.accuracy*100:.1f}%\")\n",
    "print(f\"   Positive covered: {example_rule.positive_covered}\")\n",
    "print(f\"   Negative covered: {example_rule.negative_covered}\")\n",
    "\n",
    "# Show which examples it covers\n",
    "print(f\"\\nüìä Examples covered by this rule:\")\n",
    "for ex in fraud_data['training_data']:\n",
    "    if example_rule.matches(ex):\n",
    "        match = '‚úì' if ex['is_fraud'] == example_rule.prediction else '‚úó'\n",
    "        print(f\"   {ex['transaction_id']}: {ex['amount']}, {ex['location']} ‚Üí {ex['is_fraud']} {match}\")\n",
    "\n",
    "print(\"\\nüí° This rule has high accuracy but may not cover all fraud cases!\")\n",
    "print(\"   Sequential covering will learn additional rules for remaining cases.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Learn-One-Rule: General-to-Specific Search\n",
    "\n",
    "### How We Find One Good Rule\n",
    "\n",
    "**The Strategy:**\n",
    "1. Start with the most general rule: `IF TRUE THEN class` (covers everything!)\n",
    "2. Greedily add the constraint that most improves performance\n",
    "3. Repeat until rule is accurate enough or no improvement possible\n",
    "\n",
    "**Evaluation Metric:**\n",
    "We use **Information Gain** (like ID3) to select the best attribute-value pair to add.\n",
    "\n",
    "**Stopping Criteria:**\n",
    "- Rule accuracy exceeds threshold (e.g., 90%)\n",
    "- No more attributes improve performance\n",
    "- Rule becomes too specific (covers too few examples)\n",
    "\n",
    "**Beam Search Enhancement:**\n",
    "Instead of following just one path, we can keep top-k candidates at each step (beam width = k).\n",
    "This helps avoid getting stuck in local optima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Learn-One-Rule\n",
    "print_section_header(\"Learn-One-Rule Implementation\", \"üîç\")\n",
    "\n",
    "def learn_one_rule(examples, attributes, target_attr, target_class, min_coverage=2, accuracy_threshold=0.9):\n",
    "    \"\"\"\n",
    "    Learn a single rule using general-to-specific search.\n",
    "    \n",
    "    Args:\n",
    "        examples: List of training examples\n",
    "        attributes: List of attribute names to consider\n",
    "        target_attr: Name of the target attribute\n",
    "        target_class: The class this rule should predict\n",
    "        min_coverage: Minimum number of examples rule must cover\n",
    "        accuracy_threshold: Stop when accuracy exceeds this\n",
    "    \n",
    "    Returns:\n",
    "        Rule object representing the learned rule\n",
    "    \"\"\"\n",
    "    # Start with most general rule (no conditions)\n",
    "    current_rule = Rule(conditions={}, prediction=target_class)\n",
    "    current_rule.evaluate(examples, target_attr)\n",
    "    \n",
    "    print(f\"üöÄ Learning rule for class: {target_class}\")\n",
    "    print(f\"   Starting with: {current_rule}\")\n",
    "    print(f\"   Initial coverage: {current_rule.coverage}, accuracy: {current_rule.accuracy*100:.1f}%\\n\")\n",
    "    \n",
    "    iteration = 0\n",
    "    \n",
    "    # Iteratively add conditions\n",
    "    while current_rule.accuracy < accuracy_threshold and current_rule.coverage >= min_coverage:\n",
    "        iteration += 1\n",
    "        print(f\"Iteration {iteration}:\")\n",
    "        \n",
    "        # Get examples currently covered by rule\n",
    "        covered_examples = [ex for ex in examples if current_rule.matches(ex)]\n",
    "        \n",
    "        if len(covered_examples) == 0:\n",
    "            break\n",
    "        \n",
    "        # Find best attribute-value pair to add\n",
    "        best_gain = -1\n",
    "        best_attr = None\n",
    "        best_value = None\n",
    "        \n",
    "        for attr in attributes:\n",
    "            # Skip if already in conditions\n",
    "            if attr in current_rule.conditions:\n",
    "                continue\n",
    "            \n",
    "            # Try each possible value for this attribute\n",
    "            unique_values = set([ex[attr] for ex in covered_examples])\n",
    "            \n",
    "            for value in unique_values:\n",
    "                # Create candidate rule with this new condition\n",
    "                candidate_conditions = current_rule.conditions.copy()\n",
    "                candidate_conditions[attr] = value\n",
    "                candidate_rule = Rule(conditions=candidate_conditions, prediction=target_class)\n",
    "                candidate_rule.evaluate(examples, target_attr)\n",
    "                \n",
    "                # Calculate information gain\n",
    "                if candidate_rule.coverage >= min_coverage:\n",
    "                    # Simple gain: improvement in accuracy weighted by coverage\n",
    "                    gain = (candidate_rule.accuracy - current_rule.accuracy) * candidate_rule.coverage\n",
    "                    \n",
    "                    if gain > best_gain:\n",
    "                        best_gain = gain\n",
    "                        best_attr = attr\n",
    "                        best_value = value\n",
    "        \n",
    "        # If no improvement found, stop\n",
    "        if best_attr is None:\n",
    "            print(\"   No improvement possible. Stopping.\\n\")\n",
    "            break\n",
    "        \n",
    "        # Add best condition to rule\n",
    "        current_rule.conditions[best_attr] = best_value\n",
    "        current_rule.evaluate(examples, target_attr)\n",
    "        \n",
    "        print(f\"   Added: {best_attr}={best_value}\")\n",
    "        print(f\"   New rule: {current_rule}\")\n",
    "        print(f\"   Coverage: {current_rule.coverage}, Accuracy: {current_rule.accuracy*100:.1f}%\")\n",
    "        print(f\"   Gain: {best_gain:.3f}\\n\")\n",
    "        \n",
    "        # Check if we've reached accuracy threshold\n",
    "        if current_rule.accuracy >= accuracy_threshold:\n",
    "            print(f\"   ‚úÖ Accuracy threshold reached!\\n\")\n",
    "            break\n",
    "    \n",
    "    print(f\"Final rule: {current_rule}\")\n",
    "    print(f\"Coverage: {current_rule.coverage}, Accuracy: {current_rule.accuracy*100:.1f}%\\n\")\n",
    "    \n",
    "    return current_rule\n",
    "\n",
    "# Test learn-one-rule on fraud detection\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Testing Learn-One-Rule on Fraud Detection\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Get attribute names (exclude transaction_id and target)\n",
    "fraud_attributes = ['amount', 'location', 'time', 'merchant_type', 'frequency', 'card_present']\n",
    "\n",
    "# Learn a rule for fraud class\n",
    "fraud_rule = learn_one_rule(\n",
    "    examples=fraud_data['training_data'],\n",
    "    attributes=fraud_attributes,\n",
    "    target_attr='is_fraud',\n",
    "    target_class='Yes',\n",
    "    min_coverage=2,\n",
    "    accuracy_threshold=0.95\n",
    ")\n",
    "\n",
    "print(\"\\nüí° Notice how the algorithm:\")\n",
    "print(\"   1. Started with most general rule (covers everything)\")\n",
    "print(\"   2. Greedily added conditions that improved accuracy\")\n",
    "print(\"   3. Stopped when accuracy threshold was reached\")\n",
    "print(\"   4. Result: A specific, accurate rule for detecting fraud!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Complete Sequential Covering Algorithm\n",
    "\n",
    "### Putting It All Together\n",
    "\n",
    "**Now we implement the outer loop that:**\n",
    "1. Calls `learn_one_rule` to get one rule\n",
    "2. Removes examples covered by that rule\n",
    "3. Repeats until stopping criterion met\n",
    "\n",
    "**Stopping Criteria:**\n",
    "- All positive examples covered\n",
    "- Rule quality drops below threshold\n",
    "- Maximum number of rules reached\n",
    "\n",
    "**Final Rule Set:**\n",
    "- A disjunction (OR) of rules: Rule1 OR Rule2 OR Rule3...\n",
    "- Each rule is independent and interpretable\n",
    "- Rules can be sorted by accuracy for priority ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Sequential Covering Algorithm\n",
    "print_section_header(\"Sequential Covering Algorithm\", \"üéØ\")\n",
    "\n",
    "def sequential_covering(examples, attributes, target_attr, target_class, \n",
    "                       max_rules=10, min_coverage=2, accuracy_threshold=0.9):\n",
    "    \"\"\"\n",
    "    Learn a set of rules using sequential covering.\n",
    "    \n",
    "    Args:\n",
    "        examples: List of training examples\n",
    "        attributes: List of attribute names\n",
    "        target_attr: Name of target attribute\n",
    "        target_class: Class to learn rules for\n",
    "        max_rules: Maximum number of rules to learn\n",
    "        min_coverage: Minimum coverage for a rule\n",
    "        accuracy_threshold: Minimum accuracy for a rule\n",
    "    \n",
    "    Returns:\n",
    "        List of learned rules\n",
    "    \"\"\"\n",
    "    learned_rules = []\n",
    "    remaining_examples = examples.copy()\n",
    "    \n",
    "    print(f\"üöÄ Starting Sequential Covering for class: {target_class}\")\n",
    "    print(f\"   Total examples: {len(examples)}\")\n",
    "    print(f\"   Positive examples: {sum(1 for ex in examples if ex[target_attr] == target_class)}\\n\")\n",
    "    \n",
    "    for rule_num in range(1, max_rules + 1):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Learning Rule {rule_num}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        # Check if we still have positive examples to cover\n",
    "        positive_remaining = sum(1 for ex in remaining_examples if ex[target_attr] == target_class)\n",
    "        \n",
    "        if positive_remaining == 0:\n",
    "            print(\"‚úÖ All positive examples covered!\\n\")\n",
    "            break\n",
    "        \n",
    "        print(f\"Remaining examples: {len(remaining_examples)}\")\n",
    "        print(f\"Positive examples remaining: {positive_remaining}\\n\")\n",
    "        \n",
    "        # Learn one rule\n",
    "        new_rule = learn_one_rule(\n",
    "            examples=remaining_examples,\n",
    "            attributes=attributes,\n",
    "            target_attr=target_attr,\n",
    "            target_class=target_class,\n",
    "            min_coverage=min_coverage,\n",
    "            accuracy_threshold=accuracy_threshold\n",
    "        )\n",
    "        \n",
    "        # Check if rule meets quality threshold\n",
    "        if new_rule.coverage < min_coverage or new_rule.accuracy < 0.5:\n",
    "            print(f\"‚ö†Ô∏è  Rule quality too low. Stopping.\\n\")\n",
    "            break\n",
    "        \n",
    "        # Add rule to learned set\n",
    "        learned_rules.append(new_rule)\n",
    "        \n",
    "        # Remove examples correctly covered by this rule\n",
    "        correctly_covered = [\n",
    "            ex for ex in remaining_examples \n",
    "            if new_rule.matches(ex) and ex[target_attr] == target_class\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\nüìä Rule {rule_num} covers {len(correctly_covered)} positive examples correctly\")\n",
    "        \n",
    "        # Remove covered examples\n",
    "        remaining_examples = [\n",
    "            ex for ex in remaining_examples \n",
    "            if ex not in correctly_covered\n",
    "        ]\n",
    "    \n",
    "    print(f\"\\n\\n{'='*80}\")\n",
    "    print(f\"Sequential Covering Complete!\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    print(f\"Total rules learned: {len(learned_rules)}\\n\")\n",
    "    \n",
    "    # Sort rules by accuracy (best first)\n",
    "    learned_rules.sort(key=lambda r: r.accuracy, reverse=True)\n",
    "    \n",
    "    return learned_rules\n",
    "\n",
    "# Run Sequential Covering on fraud detection\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Running Sequential Covering on Fraud Detection Dataset\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "fraud_rules = sequential_covering(\n",
    "    examples=fraud_data['training_data'],\n",
    "    attributes=fraud_attributes,\n",
    "    target_attr='is_fraud',\n",
    "    target_class='Yes',\n",
    "    max_rules=5,\n",
    "    min_coverage=1,\n",
    "    accuracy_threshold=0.9\n",
    ")\n",
    "\n",
    "# Display learned rules\n",
    "print(\"\\nüìã Learned Rule Set:\\n\")\n",
    "for i, rule in enumerate(fraud_rules, 1):\n",
    "    print(f\"Rule {i}: {rule}\")\n",
    "    print(f\"        Coverage: {rule.coverage}, Accuracy: {rule.accuracy*100:.1f}%\\n\")\n",
    "\n",
    "# Test on training data\n",
    "print(\"\\nüìä Testing on Training Data:\\n\")\n",
    "correct = 0\n",
    "total = len(fraud_data['training_data'])\n",
    "\n",
    "for ex in fraud_data['training_data']:\n",
    "    # Check if any rule matches\n",
    "    prediction = 'No'  # Default\n",
    "    for rule in fraud_rules:\n",
    "        if rule.matches(ex):\n",
    "            prediction = rule.prediction\n",
    "            break\n",
    "    \n",
    "    actual = ex['is_fraud']\n",
    "    if prediction == actual:\n",
    "        correct += 1\n",
    "    \n",
    "    match = '‚úì' if prediction == actual else '‚úó'\n",
    "    print(f\"{ex['transaction_id']}: Predicted={prediction:3s}, Actual={actual:3s} {match}\")\n",
    "\n",
    "accuracy = (correct / total) * 100\n",
    "print(f\"\\nüìà Training Accuracy: {correct}/{total} = {accuracy:.1f}%\")\n",
    "\n",
    "print(\"\\nüí° Key Observations:\")\n",
    "print(\"   ‚Ä¢ Each rule is independent and interpretable\")\n",
    "print(\"   ‚Ä¢ Rules are checked in order (best accuracy first)\")\n",
    "print(\"   ‚Ä¢ Easy to explain to stakeholders: 'Fraud because...'\")\n",
    "print(\"   ‚Ä¢ Can add/remove/modify individual rules easily\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: First-Order Rules with FOIL\n",
    "\n",
    "## üåü Slide 7: Moving to First-Order Rules\n",
    "\n",
    "### Why We Need Variables\n",
    "\n",
    "**The Limitation of Propositional Rules:**\n",
    "\n",
    "Propositional rules (what we just learned) can only talk about attributes of a single object:\n",
    "```\n",
    "IF age > 65 AND smoker = yes THEN high_risk\n",
    "```\n",
    "\n",
    "**But what about relationships between objects?**\n",
    "\n",
    "**Example: Family Relationships**\n",
    "\n",
    "**Propositional Approach (Impossible!):**\n",
    "```\n",
    "IF Alice.parent = Bob AND Bob.parent = Carol THEN Grandparent(Carol, Alice)\n",
    "IF Bob.parent = Dave AND Dave.parent = Emma THEN Grandparent(Emma, Bob)\n",
    "... (need a rule for every possible triple of people!)\n",
    "```\n",
    "\n",
    "**First-Order Approach (Elegant!):**\n",
    "```\n",
    "Grandparent(x, y) ‚Üê Parent(x, z) ‚àß Parent(z, y)\n",
    "```\n",
    "**ONE rule works for EVERYONE!**\n",
    "\n",
    "### Why First-Order Rules Are Powerful:\n",
    "\n",
    "1. **Expressiveness:** Can model complex relational structures\n",
    "2. **Recursion:** Rules can reference themselves\n",
    "   - `Ancestor(x,z) ‚Üê Parent(x,z)`\n",
    "   - `Ancestor(x,z) ‚Üê Parent(x,y) ‚àß Ancestor(y,z)`\n",
    "3. **Generalization:** One rule ‚Üí infinite instances\n",
    "4. **Programming:** First-order rules = PROLOG programs!\n",
    "\n",
    "### Real-World Applications:\n",
    "- Database queries (SQL-like reasoning)\n",
    "- Knowledge graphs (Google, Facebook)\n",
    "- Chemical structure analysis\n",
    "- Program synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load family relationships dataset\n",
    "print_section_header(\"First-Order Rules: Family Relationships\", \"üåü\")\n",
    "\n",
    "family_data = load_json_data('family_relationships_dataset.json')\n",
    "\n",
    "print(\"üë®‚Äçüë©‚Äçüëß‚Äçüë¶ Family Tree Description:\")\n",
    "print(f\"   {family_data['family_tree_description']}\\n\")\n",
    "\n",
    "print(\"üìä Available Predicates:\")\n",
    "for pred, desc in family_data['predicates'].items():\n",
    "    print(f\"   ‚Ä¢ {pred}: {desc}\")\n",
    "\n",
    "print(\"\\nüë• People in the family:\")\n",
    "print(f\"   {', '.join(family_data['people'])}\\n\")\n",
    "\n",
    "print(\"üìã Base Facts (What we know):\")\n",
    "print(f\"\\nFather relationships: {len(family_data['facts']['Father'])} facts\")\n",
    "for father, child in family_data['facts']['Father'][:5]:\n",
    "    print(f\"   Father({father}, {child})\")\n",
    "print(f\"   ... and {len(family_data['facts']['Father']) - 5} more\\n\")\n",
    "\n",
    "print(f\"Mother relationships: {len(family_data['facts']['Mother'])} facts\")\n",
    "for mother, child in family_data['facts']['Mother'][:5]:\n",
    "    print(f\"   Mother({mother}, {child})\")\n",
    "print(f\"   ... and {len(family_data['facts']['Mother']) - 5} more\\n\")\n",
    "\n",
    "print(f\"Male: {len(family_data['facts']['Male'])} people\")\n",
    "print(f\"   {', '.join(family_data['facts']['Male'])}\\n\")\n",
    "\n",
    "print(f\"Female: {len(family_data['facts']['Female'])} people\")\n",
    "print(f\"   {', '.join(family_data['facts']['Female'])}\\n\")\n",
    "\n",
    "# Visualize family tree\n",
    "print(\"üå≥ Family Tree Structure:\\n\")\n",
    "print(\"         George ‚ôÇ ‚ïê‚ïê‚ïê Mary ‚ôÄ\")\n",
    "print(\"              ‚îÇ\")\n",
    "print(\"      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\")\n",
    "print(\"      ‚îÇ               ‚îÇ\")\n",
    "print(\"   John ‚ôÇ          Susan ‚ôÄ     Bob ‚ôÇ ‚ïê‚ïê‚ïê Carol ‚ôÄ\")\n",
    "print(\"      ‚îÇ               ‚îÇ              ‚îÇ\")\n",
    "print(\"  ‚îå‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îê\")\n",
    "print(\"  ‚îÇ       ‚îÇ       ‚îÇ       ‚îÇ      ‚îÇ       ‚îÇ\")\n",
    "print(\"Tom ‚ôÇ  Alice ‚ôÄ  Tom ‚ôÇ  Alice ‚ôÄ  David ‚ôÇ  Emma ‚ôÄ\")\n",
    "print(\"  ‚îÇ       ‚îÇ                      ‚îÇ       ‚îÇ\")\n",
    "print(\"Frank‚ôÇ Grace‚ôÄ                 Henry‚ôÇ  Iris‚ôÄ\")\n",
    "\n",
    "print(\"\\nüí° Challenge: Learn rules like Grandparent(x,y) from these facts!\")\n",
    "print(\"   FOIL will discover: Grandparent(x,y) ‚Üê Parent(x,z) ‚àß Parent(z,y)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Slide 8-10: FOIL Algorithm\n",
    "\n",
    "### First-Order Inductive Learner\n",
    "\n",
    "**FOIL = Sequential Covering + Variables + Specialized Search**\n",
    "\n",
    "**Key Differences from Propositional Sequential Covering:**\n",
    "\n",
    "1. **Variable Introduction:** Each literal can add new variables\n",
    "   - Current rule: `GrandDaughter(x, y) ‚Üê`\n",
    "   - Can add: `Father(y, z)` where z is NEW\n",
    "\n",
    "2. **Binding Evaluation:** Must consider all possible variable bindings\n",
    "   - One example can create multiple bindings!\n",
    "   - Example: `Father(Tom, Bob), Father(Bob, Alice)`\n",
    "   - Creates bindings: {x=Alice, y=Bob, z=Tom} AND {x=Alice, y=Tom, z=Bob}\n",
    "\n",
    "3. **FoilGain Metric:** Different from information gain\n",
    "   - Accounts for variable bindings\n",
    "   - Formula: `FoilGain = t √ó (log‚ÇÇ(p‚ÇÅ/(p‚ÇÅ+n‚ÇÅ)) - log‚ÇÇ(p‚ÇÄ/(p‚ÇÄ+n‚ÇÄ)))`\n",
    "   - Where t = positive bindings still covered\n",
    "\n",
    "**Three Types of Literals FOIL Can Add:**\n",
    "\n",
    "**Type 1: New Predicate with Variables**\n",
    "```\n",
    "Q(v‚ÇÅ, v‚ÇÇ, ..., v·µ£)\n",
    "```\n",
    "- At least ONE variable must already exist in rule\n",
    "- Can introduce NEW variables\n",
    "- Example: `Father(y, z)` where y exists, z is new\n",
    "\n",
    "**Type 2: Equality Test**\n",
    "```\n",
    "Equal(x·µ¢, x‚±º)\n",
    "```\n",
    "- Both variables must already exist\n",
    "- Tests if two objects are the same\n",
    "\n",
    "**Type 3: Negations**\n",
    "```\n",
    "¬¨Q(...) or ¬¨Equal(...)\n",
    "```\n",
    "- Negation of Types 1 or 2\n",
    "- Example: `¬¨Friend(x, z)` ‚Äî x and z are NOT friends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified FOIL Implementation (Educational Version)\n",
    "print_section_header(\"FOIL Algorithm: Learning Grandparent(x, y)\", \"üîß\")\n",
    "\n",
    "print(\"üéØ Goal: Learn the rule for Grandparent(x, y)\\n\")\n",
    "print(\"üìö What we know (Background Knowledge):\")\n",
    "print(\"   ‚Ä¢ Father(x, y) - x is father of y\")\n",
    "print(\"   ‚Ä¢ Mother(x, y) - x is mother of y\")\n",
    "print(\"   ‚Ä¢ We can derive: Parent(x, y) from Father or Mother\\n\")\n",
    "\n",
    "# Create Parent predicate from Father and Mother\n",
    "parent_facts = []\n",
    "for father, child in family_data['facts']['Father']:\n",
    "    parent_facts.append([father, child])\n",
    "for mother, child in family_data['facts']['Mother']:\n",
    "    parent_facts.append([mother, child])\n",
    "\n",
    "print(f\"üìä Derived Parent facts: {len(parent_facts)} relationships\\n\")\n",
    "\n",
    "# Get positive and negative examples for Grandparent\n",
    "grandparent_concept = family_data['target_concepts']['Grandparent']\n",
    "positive_examples = grandparent_concept['positive_examples']\n",
    "negative_examples = grandparent_concept['negative_examples']\n",
    "\n",
    "print(f\"‚úÖ Positive examples: {len(positive_examples)}\")\n",
    "for ex in positive_examples[:5]:\n",
    "    print(f\"   Grandparent({ex[0]}, {ex[1]})\")\n",
    "print(f\"   ... and {len(positive_examples) - 5} more\\n\")\n",
    "\n",
    "print(f\"‚ùå Negative examples: {len(negative_examples)}\")\n",
    "for ex in negative_examples[:5]:\n",
    "    print(f\"   ¬¨Grandparent({ex[0]}, {ex[1]})\")\n",
    "print(f\"   ... and {len(negative_examples) - 5} more\\n\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FOIL Learning Process (Simplified)\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"üöÄ Iteration 1: Start with most general rule\")\n",
    "print(\"   Current rule: Grandparent(x, y) ‚Üê\")\n",
    "print(\"   This covers EVERYTHING (all positives AND all negatives!)\\n\")\n",
    "\n",
    "print(\"üîç Iteration 2: Add first literal\")\n",
    "print(\"   Candidates evaluated:\")\n",
    "print(\"   ‚Ä¢ Parent(x, y) - connects x directly to y\")\n",
    "print(\"   ‚Ä¢ Parent(y, x) - connects y directly to x\")\n",
    "print(\"   ‚Ä¢ Parent(x, z) - introduces new variable z ‚úì BEST\")\n",
    "print(\"   ‚Ä¢ Parent(y, z) - introduces new variable z\")\n",
    "print(\"   ‚Ä¢ Parent(z, x) - introduces new variable z\")\n",
    "print(\"   ‚Ä¢ Parent(z, y) - introduces new variable z\\n\")\n",
    "\n",
    "print(\"   Why Parent(x, z) wins?\")\n",
    "print(\"   - Creates chain: x is parent of someone (z)\")\n",
    "print(\"   - Eliminates many negatives (direct parent-child pairs)\")\n",
    "print(\"   - Keeps most positives (grandparents ARE parents of someone)\\n\")\n",
    "\n",
    "print(\"   Updated rule: Grandparent(x, y) ‚Üê Parent(x, z)\")\n",
    "print(\"   Still covers some negatives (e.g., George-John where George is parent of John)\\n\")\n",
    "\n",
    "print(\"üîç Iteration 3: Add second literal\")\n",
    "print(\"   Current variables: x, y, z\")\n",
    "print(\"   Candidates evaluated:\")\n",
    "print(\"   ‚Ä¢ Parent(z, y) - completes the chain! ‚úì BEST\")\n",
    "print(\"   ‚Ä¢ Parent(z, x) - creates wrong relationship\")\n",
    "print(\"   ‚Ä¢ Parent(y, z) - creates wrong relationship\")\n",
    "print(\"   ‚Ä¢ Equal(x, y) - not helpful\")\n",
    "print(\"   ‚Ä¢ Equal(z, y) - not helpful\\n\")\n",
    "\n",
    "print(\"   Why Parent(z, y) wins?\")\n",
    "print(\"   - Completes chain: x parent of z, z parent of y\")\n",
    "print(\"   - This IS the grandparent relationship!\")\n",
    "print(\"   - Eliminates all remaining negatives\")\n",
    "print(\"   - Keeps all positives\\n\")\n",
    "\n",
    "print(\"‚úÖ Final rule: Grandparent(x, y) ‚Üê Parent(x, z) ‚àß Parent(z, y)\")\n",
    "print(\"   Coverage: All positive examples, zero negative examples\")\n",
    "print(\"   Accuracy: 100%\\n\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Rule Interpretation\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "print(\"In plain English:\")\n",
    "print(\"   'x is a grandparent of y IF:\")\n",
    "print(\"    - x is a parent of some person z, AND\")\n",
    "print(\"    - that person z is a parent of y'\\n\")\n",
    "\n",
    "print(\"Example application:\")\n",
    "print(\"   Is George a grandparent of Tom?\")\n",
    "print(\"   1. Find z where Parent(George, z): z = John ‚úì\")\n",
    "print(\"   2. Check Parent(John, Tom): TRUE ‚úì\")\n",
    "print(\"   3. Therefore: Grandparent(George, Tom) = TRUE ‚úì\\n\")\n",
    "\n",
    "# Verify the rule\n",
    "print(\"üß™ Testing learned rule:\\n\")\n",
    "\n",
    "def check_grandparent(x, y, parent_facts):\n",
    "    \"\"\"Check if x is grandparent of y using learned rule.\"\"\"\n",
    "    # Find all z where Parent(x, z)\n",
    "    for p1, c1 in parent_facts:\n",
    "        if p1 == x:\n",
    "            z = c1\n",
    "            # Check if Parent(z, y)\n",
    "            for p2, c2 in parent_facts:\n",
    "                if p2 == z and c2 == y:\n",
    "                    return True\n",
    "    return False\n",
    "\n",
    "# Test on positive examples\n",
    "print(\"Testing on positive examples:\")\n",
    "correct_pos = 0\n",
    "for x, y in positive_examples[:5]:\n",
    "    result = check_grandparent(x, y, parent_facts)\n",
    "    correct_pos += (1 if result else 0)\n",
    "    print(f\"   Grandparent({x}, {y}): {result} {'‚úì' if result else '‚úó'}\")\n",
    "\n",
    "# Test on negative examples\n",
    "print(\"\\nTesting on negative examples:\")\n",
    "correct_neg = 0\n",
    "for x, y in negative_examples[:5]:\n",
    "    result = check_grandparent(x, y, parent_facts)\n",
    "    correct_neg += (0 if result else 1)\n",
    "    print(f\"   Grandparent({x}, {y}): {result} {'‚úó' if result else '‚úì'}\")\n",
    "\n",
    "print(f\"\\nüìà Accuracy: {(correct_pos + correct_neg) / 10 * 100:.0f}% on sample\")\n",
    "\n",
    "print(\"\\nüí° Key Insights about FOIL:\")\n",
    "print(\"   1. Variables allow ONE rule to cover infinite instances\")\n",
    "print(\"   2. Intermediate variables (z) create chains of relationships\")\n",
    "print(\"   3. FoilGain guides search toward discriminative literals\")\n",
    "print(\"   4. Result is human-readable and logically sound\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Explanation-Based Learning (EBL)\n",
    "\n",
    "## üß¨ Slide 13-14: Analytical Learning - A Different Paradigm\n",
    "\n",
    "### Learning with Prior Knowledge\n",
    "\n",
    "**Two Fundamentally Different Approaches:**\n",
    "\n",
    "**Inductive Learning (What We've Done So Far):**\n",
    "```\n",
    "Given:\n",
    "    ‚Ä¢ Training examples (data)\n",
    "    ‚Ä¢ Hypothesis space\n",
    "    \n",
    "Find:\n",
    "    ‚Ä¢ Hypothesis that fits the data\n",
    "\n",
    "Problem:\n",
    "    ‚Ä¢ Many hypotheses might fit!\n",
    "    ‚Ä¢ Need lots of data\n",
    "    ‚Ä¢ Pure pattern matching\n",
    "```\n",
    "\n",
    "**Analytical Learning (Using Knowledge!):**\n",
    "```\n",
    "Given:\n",
    "    ‚Ä¢ Training examples (data)\n",
    "    ‚Ä¢ Hypothesis space\n",
    "    ‚Ä¢ Domain theory (prior knowledge!) ‚Üê NEW!\n",
    "    \n",
    "Find:\n",
    "    ‚Ä¢ Hypothesis consistent with BOTH data AND theory\n",
    "\n",
    "Advantage:\n",
    "    ‚Ä¢ Less ambiguity\n",
    "    ‚Ä¢ Learn from fewer examples\n",
    "    ‚Ä¢ Justified decisions\n",
    "```\n",
    "\n",
    "### Real-World Analogy: Learning Chess\n",
    "\n",
    "**Pure Inductive Learning:**\n",
    "- Watch millions of chess games\n",
    "- Try to figure out patterns from scratch\n",
    "- \"Hmm, this L-shaped piece moves in weird ways...\"\n",
    "- Takes forever, unclear why moves are good\n",
    "\n",
    "**Analytical Learning (With Domain Theory):**\n",
    "- Domain theory: Legal moves, piece values, checkmate rules\n",
    "- Now just learn: \"Which legal moves are GOOD in different positions?\"\n",
    "- Much faster! Can explain based on principles\n",
    "- \"This move controls the center\" (theory-justified)\n",
    "\n",
    "### Perfect Domain Theories\n",
    "\n",
    "A domain theory is **perfect** if it is:\n",
    "\n",
    "1. **Correct:** Every assertion in the theory is TRUE about the world\n",
    "2. **Complete:** Every positive example can be proven using the theory\n",
    "\n",
    "**Examples:**\n",
    "- Chess: Legal move rules (perfect!)\n",
    "- Physics: Newton's laws F=ma (perfect!)\n",
    "- Logic Circuits: Gate behaviors (perfect!)\n",
    "\n",
    "**The Paradox: If We Know Everything, Why Learn?**\n",
    "\n",
    "**The Answer:** Difference between what we \"know in principle\" vs. what we can \"compute efficiently\"\n",
    "\n",
    "- ‚úì We know F = ma perfectly\n",
    "- ‚úó But solving complex physics problems from first principles takes forever!\n",
    "- **Learning:** Transform deep knowledge into operational shortcuts\n",
    "\n",
    "**What EBL Does:**\n",
    "```\n",
    "Deep, Principled Knowledge (slow to apply)\n",
    "            ‚Üì\n",
    "        Learning\n",
    "            ‚Üì\n",
    "Shallow, Operational Rules (fast to apply)\n",
    "```\n",
    "\n",
    "Like memorizing \"9 √ó 7 = 63\" instead of adding 9 seven times every time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SafeToStack domain for EBL\n",
    "print_section_header(\"Explanation-Based Learning: SafeToStack Domain\", \"üß¨\")\n",
    "\n",
    "stack_data = load_json_data('safe_to_stack_dataset.json')\n",
    "\n",
    "print(\"üìö Domain Theory (Perfect Knowledge):\\n\")\n",
    "for rule in stack_data['domain_theory']['rules']:\n",
    "    print(f\"{rule['rule_id']}: {rule['rule']}\")\n",
    "    print(f\"    {rule['description']}\\n\")\n",
    "\n",
    "print(\"\\nüì¶ Objects in the Domain:\\n\")\n",
    "for obj in stack_data['objects'][:4]:\n",
    "    print(f\"{obj['object_id']}: {obj['name']}\")\n",
    "    print(f\"   Material: {obj['material']}, Density: {obj['density']}, Volume: {obj['volume']}, Weight: {obj['weight']}\\n\")\n",
    "\n",
    "print(\"\\nüìã Training Example:\\n\")\n",
    "example = stack_data['training_examples'][0]\n",
    "print(f\"Example: SafeToStack({example['top_object']}, {example['bottom_object']})\")\n",
    "print(f\"Top: {stack_data['objects'][0]['name']} (weight={stack_data['objects'][0]['weight']})\")\n",
    "print(f\"Bottom: {stack_data['objects'][1]['name']} (weight={stack_data['objects'][1]['weight']})\")\n",
    "print(f\"Safe? {example['safe_to_stack']}\")\n",
    "print(f\"Why? {example['explanation']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Slide 15-17: The EBL Process\n",
    "\n",
    "### Three Steps to Learning from Explanations\n",
    "\n",
    "**Step 1: EXPLAIN**\n",
    "- Prove why the training example satisfies the target concept\n",
    "- Use domain theory to build logical derivation\n",
    "- Create a \"proof tree\" showing reasoning\n",
    "\n",
    "**Step 2: ANALYZE**\n",
    "- Determine general conditions under which explanation holds\n",
    "- Find the \"weakest preimage\" ‚Äî most general conditions\n",
    "- Abstract away specific details\n",
    "\n",
    "**Step 3: REFINE**\n",
    "- Add new rule to hypothesis capturing these conditions\n",
    "- Create operational rule that skips future explanations\n",
    "- Cache the pattern for reuse\n",
    "\n",
    "### Complete EBL Walkthrough: SafeToStack(Obj1, Obj2)\n",
    "\n",
    "**Given:**\n",
    "- Obj1: Plastic Box (density=0.5, volume=10, weight=5.0)\n",
    "- Obj2: Wooden Crate (density=0.6, volume=20, weight=12.0)\n",
    "- Goal: Prove SafeToStack(Obj1, Obj2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EBL Walkthrough\n",
    "print_section_header(\"EBL Walkthrough: SafeToStack(Obj1, Obj2)\", \"üéì\")\n",
    "\n",
    "walkthrough = stack_data['ebl_walkthrough_example']\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STEP 1: EXPLAIN (Build Proof Tree)\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(f\"Goal: {walkthrough['example']}\\n\")\n",
    "print(\"Given Facts:\")\n",
    "for obj, facts in walkthrough['given_facts'].items():\n",
    "    print(f\"  {obj}:\")\n",
    "    for attr, value in facts.items():\n",
    "        print(f\"    {attr}({obj}, {value})\")\n",
    "print()\n",
    "\n",
    "print(\"Proof Tree Construction:\\n\")\n",
    "print(\"SafeToStack(Obj1, Obj2)\")\n",
    "print(\"    ‚Üì [Apply Rule 1: SafeToStack(x,y) ‚Üê Lighter(x,y)]\")\n",
    "print(\"Lighter(Obj1, Obj2)\")\n",
    "print(\"    ‚Üì [Apply Rule 3: Lighter(x,y) ‚Üê Weight(x,wx) ‚àß Weight(y,wy) ‚àß LessThan(wx,wy)]\")\n",
    "print(\"Weight(Obj1, w1) ‚àß Weight(Obj2, w2) ‚àß LessThan(w1, w2)\")\n",
    "print(\"    ‚Üì [Apply Rule 4 twice: Weight(x,w) ‚Üê Volume(x,v) ‚àß Density(x,d) ‚àß Equal(w, v√ód)]\")\n",
    "print(\"Volume(Obj1, 10) ‚àß Density(Obj1, 0.5) ‚àß Equal(w1, 10√ó0.5)\")\n",
    "print(\"‚àß Volume(Obj2, 20) ‚àß Density(Obj2, 0.6) ‚àß Equal(w2, 20√ó0.6)\")\n",
    "print(\"‚àß LessThan(5, 12)\")\n",
    "print(\"    ‚Üì [Evaluate arithmetic]\")\n",
    "print(\"TRUE ‚úì\\n\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STEP 2: ANALYZE (Extract General Conditions)\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"Look at proof tree and replace specific values with variables:\\n\")\n",
    "print(\"Specific values ‚Üí Variables:\")\n",
    "print(\"  Obj1 ‚Üí x (any object)\")\n",
    "print(\"  Obj2 ‚Üí y (any object)\")\n",
    "print(\"  10, 0.5, 5 ‚Üí vx, dx, (vx√ódx)\")\n",
    "print(\"  20, 0.6, 12 ‚Üí vy, dy, (vy√ódy)\\n\")\n",
    "\n",
    "print(\"Weakest Preimage (Most General Conditions):\")\n",
    "print(\"  'The explanation holds for ANY x and y where x's weight < y's weight'\\n\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STEP 3: REFINE (Create Operational Rule)\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"Learned Rule:\")\n",
    "print(f\"  {walkthrough['weakest_preimage']}\\n\")\n",
    "\n",
    "print(\"Interpretation:\")\n",
    "print(f\"  {walkthrough['learned_rule_interpretation']}\\n\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"What We Achieved\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"BEFORE EBL (Using Domain Theory):\")\n",
    "print(\"  SafeToStack ‚Üí Lighter ‚Üí Weight ‚Üí Volume√óDensity ‚Üí Calculate\")\n",
    "print(\"  (3-step reasoning chain)\\n\")\n",
    "\n",
    "print(\"AFTER EBL (Using Learned Rule):\")\n",
    "print(\"  SafeToStack ‚Üí Direct check: vx√ódx < vy√ódy\")\n",
    "print(\"  (1-step direct computation!)\\n\")\n",
    "\n",
    "print(\"Benefits:\")\n",
    "print(\"  ‚úì Flattened multi-step reasoning into single rule\")\n",
    "print(\"  ‚úì Works for ANY objects with lighter-than relationship\")\n",
    "print(\"  ‚úì Bypasses intermediate concepts (Lighter, Weight)\")\n",
    "print(\"  ‚úì Much faster to apply!\")\n",
    "print(\"  ‚úì Still logically sound (derived from correct theory)\\n\")\n",
    "\n",
    "# Visualize the speedup\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "methods = ['Domain Theory\\n(Deep Reasoning)', 'Learned Rule\\n(EBL)']\n",
    "steps = [3, 1]  # Number of reasoning steps\n",
    "time = [100, 10]  # Relative time (arbitrary units)\n",
    "\n",
    "x = np.arange(len(methods))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, steps, width, label='Reasoning Steps', color='#3498db', alpha=0.7)\n",
    "ax.bar(x + width/2, time, width, label='Relative Time', color='#e74c3c', alpha=0.7)\n",
    "\n",
    "ax.set_ylabel('Count / Time', fontsize=12)\n",
    "ax.set_title('EBL Speedup: Deep Knowledge ‚Üí Operational Rules', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(methods)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, (s, t) in enumerate(zip(steps, time)):\n",
    "    ax.text(i - width/2, s + 0.1, str(s), ha='center', fontsize=10, fontweight='bold')\n",
    "    ax.text(i + width/2, t + 2, str(t), ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Insight: EBL doesn't discover new knowledge‚Äîit reformulates\")\n",
    "print(\"   existing knowledge into more usable form!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Comparisons and Applications\n",
    "\n",
    "## üìä Slide 18: Inductive vs Analytical Learning\n",
    "\n",
    "### Comparing the Two Paradigms\n",
    "\n",
    "| Aspect | Inductive Learning | Analytical Learning |\n",
    "|--------|-------------------|---------------------|\n",
    "| **Input** | Data + Hypothesis space | Data + Hypothesis space + **Domain theory** |\n",
    "| **Learning Style** | Pattern matching from data | Explanation + generalization |\n",
    "| **Sample Complexity** | Needs many examples | Can learn from few examples |\n",
    "| **Guarantee** | Statistical (probably correct) | Deductive (logically correct if theory correct) |\n",
    "| **Hypothesis Space** | Searches entire space | Constrained by theory |\n",
    "| **Explainability** | Often black box | Fully explainable |\n",
    "| **New Knowledge** | Can discover truly novel patterns | Reformulates existing knowledge |\n",
    "| **Robustness** | Handles noisy data well | Sensitive to theory errors |\n",
    "| **When Theory Wrong** | Still learns from data | May learn incorrect rules |\n",
    "\n",
    "### When to Use Each?\n",
    "\n",
    "**Use Inductive Learning When:**\n",
    "- ‚úì Lots of training data available\n",
    "- ‚úì No good domain theory exists\n",
    "- ‚úì Discovering novel patterns\n",
    "- ‚úì Data is noisy or messy\n",
    "- ‚úì Want to find surprising insights\n",
    "\n",
    "**Examples:** Image recognition, spam detection, recommendation systems\n",
    "\n",
    "**Use Analytical Learning When:**\n",
    "- ‚úì Limited training data\n",
    "- ‚úì Strong prior knowledge exists\n",
    "- ‚úì Need explainable decisions\n",
    "- ‚úì Rules of domain are well-known\n",
    "- ‚úì Correctness is critical\n",
    "\n",
    "**Examples:** Chess, physics problems, logic puzzles, planning\n",
    "\n",
    "**The Best Approach: Combine Both!**\n",
    "\n",
    "```\n",
    "Inductive-Analytical Hybrid:\n",
    "1. Start with domain theory (analytical)\n",
    "2. Use data to refine/correct theory (inductive)\n",
    "3. Use theory to guide search (analytical)\n",
    "4. Handle exceptions with data (inductive)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 6: Self-Assessment and Practice\n",
    "\n",
    "## üìù Test Your Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-Assessment Questions\n",
    "print_section_header(\"Self-Assessment Questions\", \"üìù\")\n",
    "\n",
    "questions = [\n",
    "    {\n",
    "        'id': 1,\n",
    "        'type': 'Multiple Choice',\n",
    "        'question': 'What is the main difference between sequential covering and decision tree learning?',\n",
    "        'options': [\n",
    "            'A) Sequential covering is faster',\n",
    "            'B) Sequential covering learns rules one at a time, decision trees learn all rules simultaneously',\n",
    "            'C) Decision trees are more accurate',\n",
    "            'D) Sequential covering cannot handle categorical data'\n",
    "        ],\n",
    "        'answer': 'B',\n",
    "        'explanation': 'Sequential covering learns rules independently one at a time, removing covered examples '\n",
    "                      'after each rule. Decision trees learn all rules simultaneously as they share decisions at top nodes.'\n",
    "    },\n",
    "    {\n",
    "        'id': 2,\n",
    "        'type': 'Multiple Choice',\n",
    "        'question': 'In FOIL, what does the FoilGain metric measure?',\n",
    "        'options': [\n",
    "            'A) The number of variables in a rule',\n",
    "            'B) The reduction in entropy from adding a literal',\n",
    "            'C) The information gain accounting for variable bindings',\n",
    "            'D) The computational cost of evaluating a rule'\n",
    "        ],\n",
    "        'answer': 'C',\n",
    "        'explanation': 'FoilGain measures information gain but accounts for variable bindings. '\n",
    "                      'It considers how many positive bindings are still covered after adding a literal.'\n",
    "    },\n",
    "    {\n",
    "        'id': 3,\n",
    "        'type': 'Multiple Choice',\n",
    "        'question': 'What is the \"weakest preimage\" in EBL?',\n",
    "        'options': [\n",
    "            'A) The least accurate rule',\n",
    "            'B) The most general conditions under which the explanation holds',\n",
    "            'C) The simplest domain theory',\n",
    "            'D) The minimum number of training examples needed'\n",
    "        ],\n",
    "        'answer': 'B',\n",
    "        'explanation': 'The weakest preimage represents the most general conditions under which the proof '\n",
    "                      'tree holds. It abstracts away specific details to create a broadly applicable rule.'\n",
    "    },\n",
    "    {\n",
    "        'id': 4,\n",
    "        'type': 'True/False',\n",
    "        'question': 'First-order rules can express relationships that propositional rules cannot.',\n",
    "        'answer': 'True',\n",
    "        'explanation': 'TRUE. First-order rules use variables and can express relationships between objects '\n",
    "                      '(e.g., Grandparent(x,y) ‚Üê Parent(x,z) ‚àß Parent(z,y)). Propositional rules can only '\n",
    "                      'describe attributes of single objects.'\n",
    "    },\n",
    "    {\n",
    "        'id': 5,\n",
    "        'type': 'True/False',\n",
    "        'question': 'EBL can discover knowledge that is not already in the domain theory.',\n",
    "        'answer': 'False',\n",
    "        'explanation': 'FALSE. EBL reformulates existing knowledge into more operational form. It cannot '\n",
    "                      'discover truly novel knowledge‚Äîit only makes existing knowledge more efficient to use.'\n",
    "    },\n",
    "    {\n",
    "        'id': 6,\n",
    "        'type': 'True/False',\n",
    "        'question': 'Sequential covering removes examples after learning each rule.',\n",
    "        'answer': 'True',\n",
    "        'explanation': 'TRUE. After learning each rule, sequential covering removes the positive examples '\n",
    "                      'correctly covered by that rule, then learns the next rule on remaining examples.'\n",
    "    },\n",
    "    {\n",
    "        'id': 7,\n",
    "        'type': 'Conceptual',\n",
    "        'question': 'Why might rule-based learning be preferred over neural networks in medical diagnosis?',\n",
    "        'answer': 'Rule-based learning produces explainable, human-readable rules that doctors can understand '\n",
    "                 'and verify. In medical diagnosis, it\\'s critical to explain WHY a diagnosis was made, not just '\n",
    "                 'what the diagnosis is. Rules like \"IF fever=VeryHigh AND breathing=Severe THEN COVID-19\" can '\n",
    "                 'be validated by medical experts and explained to patients. Neural networks are black boxes '\n",
    "                 'that cannot provide such explanations, which is problematic for medical liability and trust.'\n",
    "    },\n",
    "    {\n",
    "        'id': 8,\n",
    "        'type': 'Conceptual',\n",
    "        'question': 'Explain the paradox: \"If we have a perfect domain theory, why do we need to learn?\"',\n",
    "        'answer': 'A perfect domain theory tells us what is true in principle, but may be computationally '\n",
    "                 'expensive to apply. For example, we know physics laws perfectly (F=ma), but solving complex '\n",
    "                 'physics problems from first principles is slow. EBL learns operational shortcuts that '\n",
    "                 'transform deep, principled knowledge into fast, directly applicable rules. It\\'s like '\n",
    "                 'memorizing multiplication tables instead of adding repeatedly‚Äîsame result, much faster.'\n",
    "    },\n",
    "    {\n",
    "        'id': 9,\n",
    "        'type': 'Conceptual',\n",
    "        'question': 'What are the three types of literals FOIL can add to a rule?',\n",
    "        'answer': '1) New predicate with variables Q(v1, v2, ..., vr) where at least one variable already '\n",
    "                 'exists in the rule and new variables can be introduced. '\n",
    "                 '2) Equality test Equal(xi, xj) where both variables already exist in the rule. '\n",
    "                 '3) Negations of the above two types, e.g., ¬¨Q(...) or ¬¨Equal(...).'\n",
    "    },\n",
    "    {\n",
    "        'id': 10,\n",
    "        'type': 'Conceptual',\n",
    "        'question': 'Compare the sample complexity of inductive vs analytical learning.',\n",
    "        'answer': 'Inductive learning requires many training examples because it must search a large hypothesis '\n",
    "                 'space with only data as guidance. Many hypotheses may fit small datasets. Analytical learning '\n",
    "                 'can learn from very few examples (even one!) because the domain theory constrains the '\n",
    "                 'hypothesis space dramatically. The theory provides additional information beyond the data, '\n",
    "                 'reducing ambiguity. However, analytical learning requires a good domain theory, which may '\n",
    "                 'not always be available.'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Display questions\n",
    "for q in questions:\n",
    "    print(f\"\\nQuestion {q['id']} ({q['type']}):\")\n",
    "    print(f\"  {q['question']}\")\n",
    "    \n",
    "    if 'options' in q:\n",
    "        for opt in q['options']:\n",
    "            print(f\"    {opt}\")\n",
    "    \n",
    "    print(f\"\\n  ‚úÖ Answer: {q['answer']}\")\n",
    "    print(f\"  üí° Explanation: {q['explanation']}\")\n",
    "    print(\"  \" + \"-\"*70)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä How did you do? Review any concepts you found challenging!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 7: Summary and Real-World Applications\n",
    "\n",
    "## üåç Slide 19: Real-World Applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-world applications\n",
    "print_section_header(\"Real-World Applications of Rule Learning\", \"üåç\")\n",
    "\n",
    "applications = [\n",
    "    {\n",
    "        'name': 'SOAR (State, Operator And Result)',\n",
    "        'domain': 'Cognitive Architecture',\n",
    "        'technique': 'EBL + Chunking',\n",
    "        'description': 'Learns problem-solving strategies by chunking successful solution paths. '\n",
    "                      'Used in robotics, game AI, and intelligent tutoring systems.',\n",
    "        'impact': 'Enables agents to learn from experience and improve performance over time.'\n",
    "    },\n",
    "    {\n",
    "        'name': 'PRODIGY',\n",
    "        'domain': 'Planning and Problem Solving',\n",
    "        'technique': 'EBL for control knowledge',\n",
    "        'description': 'Learns search control rules to guide planning. Analyzes successful plans '\n",
    "                      'to extract general strategies.',\n",
    "        'impact': 'Dramatically speeds up planning by avoiding futile search paths.'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Chess Programs (Deep Blue era)',\n",
    "        'domain': 'Game Playing',\n",
    "        'technique': 'EBL for opening book',\n",
    "        'description': 'Analyzes master games to extract opening principles and endgame patterns. '\n",
    "                      'Converts deep analysis into fast lookup rules.',\n",
    "        'impact': 'Enables rapid opening play and endgame recognition.'\n",
    "    },\n",
    "    {\n",
    "        'name': 'DENDRAL',\n",
    "        'domain': 'Chemical Structure Analysis',\n",
    "        'technique': 'Rule-based expert system',\n",
    "        'description': 'Infers molecular structure from mass spectrometry data using chemical rules. '\n",
    "                      'One of the first successful AI systems.',\n",
    "        'impact': 'Matched human expert performance in identifying organic molecules.'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Fraud Detection Systems',\n",
    "        'domain': 'Financial Services',\n",
    "        'technique': 'Sequential Covering',\n",
    "        'description': 'Learns interpretable rules for detecting fraudulent transactions. '\n",
    "                      'Rules can be audited and explained to regulators.',\n",
    "        'impact': 'Saves billions annually while maintaining regulatory compliance.'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Medical Diagnosis (MYCIN)',\n",
    "        'domain': 'Healthcare',\n",
    "        'technique': 'Rule-based reasoning',\n",
    "        'description': 'Diagnoses bacterial infections using ~600 rules. Explains reasoning '\n",
    "                      'to doctors in natural language.',\n",
    "        'impact': 'Achieved expert-level diagnostic accuracy with full explainability.'\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, app in enumerate(applications, 1):\n",
    "    print(f\"\\n{i}. {app['name']}\")\n",
    "    print(f\"   Domain: {app['domain']}\")\n",
    "    print(f\"   Technique: {app['technique']}\")\n",
    "    print(f\"   Description: {app['description']}\")\n",
    "    print(f\"   Impact: {app['impact']}\")\n",
    "    print(\"   \" + \"-\"*70)\n",
    "\n",
    "print(\"\\nüí° Common Thread: All these applications need EXPLAINABLE decisions!\")\n",
    "print(\"   Rule-based learning provides transparency that black-box methods cannot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Slide 20: Key Takeaways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module summary\n",
    "print_section_header(\"Module 2: Key Takeaways\", \"üéØ\")\n",
    "\n",
    "print(\"\\nüìö What We Learned:\\n\")\n",
    "\n",
    "takeaways = [\n",
    "    {\n",
    "        'topic': 'Sequential Covering',\n",
    "        'key_points': [\n",
    "            'Learns rules ONE AT A TIME (vs. decision trees learning all at once)',\n",
    "            'Removes covered examples after each rule',\n",
    "            'Each rule is independent and interpretable',\n",
    "            'Natural for disjunctive concepts (Rule1 OR Rule2 OR ...)'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'topic': 'FOIL Algorithm',\n",
    "        'key_points': [\n",
    "            'Extends sequential covering to first-order logic',\n",
    "            'Uses variables to express relationships between objects',\n",
    "            'One rule can cover infinite instances',\n",
    "            'FoilGain metric accounts for variable bindings',\n",
    "            'Three types of literals: new predicates, equality, negations'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'topic': 'Explanation-Based Learning',\n",
    "        'key_points': [\n",
    "            'Uses prior knowledge (domain theory) to guide learning',\n",
    "            'Three steps: EXPLAIN ‚Üí ANALYZE ‚Üí REFINE',\n",
    "            'Transforms deep knowledge into operational rules',\n",
    "            'Can learn from very few examples',\n",
    "            'Does not discover new knowledge‚Äîreformulates existing knowledge'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'topic': 'Inductive vs Analytical',\n",
    "        'key_points': [\n",
    "            'Inductive: Learn from data alone (many examples needed)',\n",
    "            'Analytical: Learn from data + theory (few examples needed)',\n",
    "            'Best approach: Combine both (hybrid methods)',\n",
    "            'Choose based on: data availability, domain knowledge, explainability needs'\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, section in enumerate(takeaways, 1):\n",
    "    print(f\"{i}. {section['topic']}:\")\n",
    "    for point in section['key_points']:\n",
    "        print(f\"   ‚Ä¢ {point}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üåü The Big Picture\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"Rule-based learning sits at the intersection of:\")\n",
    "print(\"   ‚Ä¢ Machine Learning (data-driven pattern discovery)\")\n",
    "print(\"   ‚Ä¢ Logic and Knowledge Representation (symbolic reasoning)\")\n",
    "print(\"   ‚Ä¢ Explainable AI (transparent, interpretable decisions)\\n\")\n",
    "\n",
    "print(\"When to use rule-based learning:\")\n",
    "print(\"   ‚úì Explainability is critical (medical, legal, financial)\")\n",
    "print(\"   ‚úì Domain experts need to validate/modify rules\")\n",
    "print(\"   ‚úì Regulatory compliance requires transparency\")\n",
    "print(\"   ‚úì Relational data (databases, knowledge graphs)\")\n",
    "print(\"   ‚úì Prior knowledge available (domain theories)\\n\")\n",
    "\n",
    "print(\"When NOT to use rule-based learning:\")\n",
    "print(\"   ‚úó High-dimensional perceptual data (images, audio)\")\n",
    "print(\"   ‚úó Complex non-linear patterns (deep learning excels)\")\n",
    "print(\"   ‚úó Massive datasets where interpretability not needed\")\n",
    "print(\"   ‚úó Rapidly changing domains (rules become outdated)\\n\")\n",
    "\n",
    "# Create visual summary\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left: Algorithm comparison\n",
    "ax1 = axes[0]\n",
    "algorithms = ['Sequential\\nCovering', 'FOIL', 'EBL']\n",
    "expressiveness = [6, 9, 7]  # Arbitrary scale\n",
    "explainability = [9, 8, 10]\n",
    "data_efficiency = [5, 6, 9]\n",
    "\n",
    "x = np.arange(len(algorithms))\n",
    "width = 0.25\n",
    "\n",
    "ax1.bar(x - width, expressiveness, width, label='Expressiveness', color='#3498db', alpha=0.8)\n",
    "ax1.bar(x, explainability, width, label='Explainability', color='#2ecc71', alpha=0.8)\n",
    "ax1.bar(x + width, data_efficiency, width, label='Data Efficiency', color='#e74c3c', alpha=0.8)\n",
    "\n",
    "ax1.set_ylabel('Score (0-10)', fontsize=11)\n",
    "ax1.set_title('Algorithm Comparison', fontsize=13, fontweight='bold')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(algorithms)\n",
    "ax1.legend(fontsize=9)\n",
    "ax1.set_ylim(0, 11)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Right: Learning paradigms\n",
    "ax2 = axes[1]\n",
    "paradigms = ['Pure\\nInductive', 'Pure\\nAnalytical', 'Hybrid\\nApproach']\n",
    "sample_complexity = [9, 2, 5]  # Higher = needs more samples\n",
    "novelty_discovery = [9, 1, 7]  # Can discover new patterns\n",
    "theory_dependence = [1, 10, 6]  # Needs domain theory\n",
    "\n",
    "x2 = np.arange(len(paradigms))\n",
    "\n",
    "ax2.bar(x2 - width, sample_complexity, width, label='Sample Complexity', color='#9b59b6', alpha=0.8)\n",
    "ax2.bar(x2, novelty_discovery, width, label='Novelty Discovery', color='#f39c12', alpha=0.8)\n",
    "ax2.bar(x2 + width, theory_dependence, width, label='Theory Dependence', color='#1abc9c', alpha=0.8)\n",
    "\n",
    "ax2.set_ylabel('Score (0-10)', fontsize=11)\n",
    "ax2.set_title('Learning Paradigm Comparison', fontsize=13, fontweight='bold')\n",
    "ax2.set_xticks(x2)\n",
    "ax2.set_xticklabels(paradigms)\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.set_ylim(0, 11)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üöÄ Next Steps\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"To deepen your understanding:\")\n",
    "print(\"   1. Implement FOIL for other family relationships (Uncle, Cousin, Sibling)\")\n",
    "print(\"   2. Apply sequential covering to the medical diagnosis dataset\")\n",
    "print(\"   3. Create your own domain theory and apply EBL\")\n",
    "print(\"   4. Compare rule-based vs decision tree performance on same dataset\")\n",
    "print(\"   5. Read: Mitchell Chapter 10-11 for deeper mathematical treatment\\n\")\n",
    "\n",
    "print(\"Prepare for Module 3:\")\n",
    "print(\"   ‚Ä¢ Review Bayesian probability (prior, posterior, likelihood)\")\n",
    "print(\"   ‚Ä¢ Understand conditional independence\")\n",
    "print(\"   ‚Ä¢ Practice with probability calculations\\n\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üéâ Congratulations! You've completed Module 2!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nYou now understand how to:\")\n",
    "print(\"   ‚úÖ Learn interpretable rules from data\")\n",
    "print(\"   ‚úÖ Express complex relationships with first-order logic\")\n",
    "print(\"   ‚úÖ Leverage prior knowledge to learn efficiently\")\n",
    "print(\"   ‚úÖ Choose the right learning paradigm for your problem\\n\")\n",
    "print(\"Keep learning, keep building, and remember: Explainability matters! üåü\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

