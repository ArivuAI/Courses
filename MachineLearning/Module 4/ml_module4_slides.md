# Module 4: Unsupervised Learning & MCMC Methods
### Machine Learning Course - Educational Slides

---

## Slide 1: Why Should You Care About Unsupervised Learning?

**Real-World Problem:**

Imagine you're Netflix with 200 million users watching thousands of movies. How do you organize users into groups for personalized recommendations WITHOUT manually labeling each user's preferences?

**Business Impact:**
- Manual labeling: $500,000+ annually (hiring teams to categorize)
- Lost revenue: 15-20% customers leave due to poor recommendations
- Solution cost: Automated clustering reduces costs by 90%

**What You'll Learn:**
- Discover hidden patterns in data without labels
- Group similar items automatically
- Reduce data complexity while preserving information
- Master techniques used by Netflix, Spotify, and Amazon

**Key Question:** What if your data has NO labels - can machines still learn?

---

## Slide 2: How Humans Learn Patterns (The Clustering Analogy)

**Imagine Walking Into a Party:**

**Trial 1:** "Let me observe people..."
- You notice: Some people cluster near the music (dancers)
- Others gather around food (foodies)  
- Some sit quietly in corners (introverts)
- **Learning:** "People naturally form groups by behavior"

**Trial 2:** "I'll join a group..."
- You move closer to the dancers
- **Error check:** "Am I similar to them? Do I like dancing?"
- **Adjustment:** "Actually, I prefer food!" (move to foodies)

**Trial 3:** "Found my group!"
- You've clustered yourself with similar people
- **Memory:** "I belong with the foodies"

**Neural Network Parallel:**
- Your observations = Input data
- Groups you identified = Clusters
- Your movement = Algorithm iterations
- Finding your place = Convergence

---

## Slide 3: The k-Means Algorithm - Finding Natural Groups

**What It Is:**
A clustering algorithm that groups similar data points by finding k "center points" (means) in your data space.

**Everyday Analogy:**
Think of organizing your city's coffee shops. If you want to place 3 delivery hubs (centers) to minimize average delivery distance, k-means finds the optimal locations!

**The Algorithm Works in 3 Steps:**

**Step 1: Initialize**
- Choose k (number of groups you want)
- Place k "cluster centers" randomly in your data space
- Think: "Put 3 flags randomly on a map of coffee shops"

**Step 2: Assign**
- Every data point joins the nearest center
- Distance formula: Euclidean distance = ‚àö[(x‚ÇÅ-x‚ÇÇ)¬≤ + (y‚ÇÅ-y‚ÇÇ)¬≤]
- Think: "Each coffee shop joins its nearest delivery hub"

**Step 3: Update**
- Move each center to the average position of its members
- New center = mean of all assigned points
- Think: "Relocate hub to the center of its coffee shops"

**Repeat Steps 2-3 until centers stop moving!**

---

## Slide 4: k-Means Mathematical Foundation

**The Objective Function:**
Minimize the sum of squared distances from each point to its cluster center:

```
Error = Œ£‚±º Œ£·µ¢ ||x·µ¢ - Œº‚±º||¬≤

Where:
- x·µ¢ = data point i
- Œº‚±º = center of cluster j  
- ||.|| = Euclidean distance
- Goal: Make this error as small as possible
```

**Simple Example with Numbers:**

Suppose we have 6 data points: [2, 3, 3, 8, 9, 10]
Want k=2 clusters

**Iteration 1:**
- Initialize centers: Œº‚ÇÅ=2, Œº‚ÇÇ=10
- Assign: Cluster 1={2,3,3}, Cluster 2={8,9,10}
- Update: Œº‚ÇÅ=(2+3+3)/3=2.67, Œº‚ÇÇ=(8+9+10)/3=9

**Iteration 2:**
- Assign: Cluster 1={2,3,3}, Cluster 2={8,9,10} (same!)
- **Converged!** Centers stopped moving

**Key Insight:** The algorithm always converges, but may find local minima depending on initialization.

---

## Slide 5: k-Means Implementation with Code

```python
# Import required library
import numpy as np

# Step 1: Initialize k cluster centers randomly
def initialize_centers(data, k):
    # Randomly select k data points as initial centers
    indices = np.random.choice(len(data), k, replace=False)
    return data[indices]

# Step 2: Assign each point to nearest center
def assign_clusters(data, centers):
    clusters = []
    for point in data:
        # Calculate distance to each center
        distances = [np.linalg.norm(point - center) 
                    for center in centers]
        # Assign to nearest center
        clusters.append(np.argmin(distances))
    return np.array(clusters)

# Step 3: Update centers to mean of assigned points
def update_centers(data, clusters, k):
    new_centers = []
    for i in range(k):
        # Find all points in cluster i
        cluster_points = data[clusters == i]
        # Calculate mean position
        new_centers.append(np.mean(cluster_points, axis=0))
    return np.array(new_centers)

# Main k-means algorithm
def kmeans(data, k, max_iterations=100):
    centers = initialize_centers(data, k)
    
    for iteration in range(max_iterations):
        # Assign and update
        clusters = assign_clusters(data, centers)
        new_centers = update_centers(data, clusters, k)
        
        # Check convergence (centers stopped moving)
        if np.allclose(centers, new_centers):
            break
        centers = new_centers
    
    return centers, clusters

# Expected output: Cluster centers and assignments
```

---

## Slide 6: Real Project Story - Customer Segmentation at E-commerce

**Project:** Customer Clustering for TechMart (2023)

**Background:**
- E-commerce platform with 500,000 active customers
- Challenge: Generic marketing emails had 2% click rate
- Current cost: $50,000/month in email campaigns with poor ROI

**Technical Challenge:**
- Dataset: Customer purchase history (50+ features)
- Tried: Manual segmentation (too slow, only 5 categories)
- Initial k-means: Chose k=10 randomly, got poor results

**Solution:**
- Used "Elbow Method" to find optimal k=7 clusters
- Features: Purchase frequency, average order value, category preferences
- Normalized all features to same scale (critical!)
- Ran k-means for 50 iterations until convergence

**Results:**
- **Before:** 2% click rate, $0.50 revenue per email
- **After:** 12% click rate, $3.20 revenue per email (540% increase!)
- **Business Impact:** Increased annual revenue by $2.8M
- **Implementation time:** 2 weeks with Python/scikit-learn

**Lesson Learned:**
Feature normalization is CRUCIAL! Initially forgot to normalize - results were terrible because "price" (0-1000) dominated "frequency" (0-10).

---

## Slide 7: Quick Check - Test Your Understanding

**Question 1 (Conceptual):**
Why does k-means require you to specify k (number of clusters) beforehand? What happens if you choose k incorrectly?

**Question 2 (Predictive):**
You run k-means twice on the same dataset with k=3. Will you get the same clusters both times? Why or why not?

**Question 3 (Practical):**
Your dataset has features: age (20-80), income ($20k-$200k), and purchase count (1-1000). What preprocessing step is essential before running k-means?

**Answers:**
1. k-means needs k to initialize centers. Wrong k ‚Üí under/over-fitting (too few=missed patterns, too many=overfitting noise)
2. No! Random initialization means different starting points may converge to different local optima
3. **Normalization/Standardization!** Otherwise income dominates the distance calculation

---

## Slide 8: Introduction to Self-Organizing Maps (SOM)

**What It Is:**
A neural network that maps high-dimensional data onto a 2D grid while preserving topological relationships (nearby inputs ‚Üí nearby neurons).

**Why It Matters:**
- **Dimensionality Reduction:** Visualize complex data (100D ‚Üí 2D)
- **Feature Mapping:** Automatically organize features spatially
- **Preserves Structure:** Similar data stays together

**Real-World Applications:**
1. **Banking:** Fraud detection - similar transaction patterns cluster together
2. **Healthcare:** Patient diagnosis - similar symptoms group spatially
3. **Manufacturing:** Quality control - defect patterns revealed visually

**Difference from k-Means:**
- k-means: Only finds cluster centers
- SOM: Creates ordered map where neighbors are similar (topology preservation)

**Think of it as:** Creating a "map of concepts" where related ideas are physically close to each other!

---

## Slide 9: How SOM Works - The Neighborhood Effect

**Core Concept: Competitive Learning with Cooperation**

**Step-by-Step Process:**

**Step 1: Setup**
- Create a 2D grid of neurons (e.g., 10√ó10 = 100 neurons)
- Each neuron has weights = position in input space
- Initialize weights randomly or using PCA

**Step 2: Competition**
- Present an input pattern x
- Find "winning neuron" (Best Matching Unit - BMU)
- BMU = neuron with smallest distance: d = ||x - w‚±º||

**Step 3: Cooperation**
- Update BMU AND its neighbors!
- Neighborhood function: h(distance) = e^(-distance¬≤/2œÉ¬≤)
- Close neighbors update more than distant ones

**Step 4: Update Weights**
```
w‚±º(new) = w‚±º(old) + Œ∑(t) √ó h(n·µ¶,t) √ó (x - w‚±º(old))

Where:
- Œ∑(t) = learning rate (decreases over time)
- h(n·µ¶,t) = neighborhood function
- Moves neuron towards input
```

**Key Innovation:** Neighbors move together, creating smooth topology!

---

## Slide 10: SOM Algorithm Implementation

```python
import numpy as np

# Initialize SOM grid
class SOM:
    def __init__(self, grid_size, input_dim, learning_rate=0.5):
        self.grid_size = grid_size  # e.g., (10, 10)
        self.input_dim = input_dim
        self.learning_rate = learning_rate
        
        # Initialize weights randomly for each neuron
        self.weights = np.random.rand(
            grid_size[0], grid_size[1], input_dim
        )
    
    def find_bmu(self, input_vector):
        """Find Best Matching Unit (winning neuron)"""
        # Calculate distance to all neurons
        distances = np.linalg.norm(
            self.weights - input_vector, axis=2
        )
        # Return coordinates of minimum distance
        return np.unravel_index(
            np.argmin(distances), distances.shape
        )
    
    def neighborhood_function(self, bmu, sigma):
        """Calculate influence of BMU on neighbors"""
        # Create coordinate grid
        rows, cols = np.indices(self.grid_size)
        # Distance from BMU
        distances = ((rows - bmu[0])**2 + 
                    (cols - bmu[1])**2)**0.5
        # Gaussian neighborhood
        return np.exp(-distances**2 / (2*sigma**2))
    
    def train_step(self, input_vector, iteration, max_iter):
        """One training iteration"""
        # Find winning neuron
        bmu = self.find_bmu(input_vector)
        
        # Decay learning rate and neighborhood
        lr = self.learning_rate * (1 - iteration/max_iter)
        sigma = max(1, self.grid_size[0]/2 * 
                   (1 - iteration/max_iter))
        
        # Get neighborhood influence
        influence = self.neighborhood_function(bmu, sigma)
        
        # Update all neurons
        for i in range(self.grid_size[0]):
            for j in range(self.grid_size[1]):
                self.weights[i,j] += (lr * influence[i,j] * 
                    (input_vector - self.weights[i,j]))

# Expected output: Organized topological map
```

---

## Slide 11: Introduction to MCMC - The Monte Carlo Principle

**The Casino Connection:**

Monte Carlo (famous for casinos) ‚Üí Monte Carlo methods (using randomness to solve problems)

**Core Idea:**
If you can't calculate something exactly, sample from it randomly many times and the samples will converge to the true answer!

**Simple Example - Estimating Pi:**

```
Draw a square (side = 2r)
Draw a circle inside (radius = r)
Throw darts randomly at square

Circle area / Square area = œÄr¬≤ / 4r¬≤ = œÄ/4

Therefore: œÄ ‚âà 4 √ó (darts in circle / total darts)

With 1,000 darts: œÄ ‚âà 3.14
With 1,000,000 darts: œÄ ‚âà 3.14159 (better!)
```

**The Monte Carlo Principle:**
```
True Value = lim(N‚Üí‚àû) [1/N Œ£·µ¢ f(x‚Å±)]

Translation: Average of many samples 
            approaches true expectation
```

**Why This Matters:**
Many problems in ML have complex distributions we can't solve analytically. Monte Carlo lets us sample instead!

---

## Slide 12: Why We Need MCMC in Machine Learning

**The Problem:**
Many ML problems require computing expectations over complex, high-dimensional probability distributions.

**Example - Bayesian Neural Networks:**
```
We want: p(weights | data)
But need: ‚à´ p(weights | data) dweights
Problem: Can't compute this integral!
```

**Three Challenges:**

**1. High Dimensionality**
- Neural network with 1000 parameters
- Can't explore all combinations (2^1000 possibilities!)

**2. Complex Distributions**
- Multi-modal (multiple peaks)
- Non-standard shapes (not Gaussian)

**3. Unknown Normalizing Constants**
- We know p(x) ‚àù f(x) but not the constant Z
- Computing Z requires integration (intractable!)

**MCMC Solution:**
Don't compute the distribution exactly - **sample** from it intelligently!

**Business Impact Example:**
- Problem: Uncertainty estimation in medical diagnosis
- Without MCMC: Single-point prediction (70% cancer risk)
- With MCMC: Distribution of possibilities (60-80% with confidence)
- Result: Doctors make better decisions with uncertainty quantification

---

## Slide 13: The Metropolis-Hastings Algorithm

**What It Is:**
The most popular MCMC algorithm that samples from ANY distribution by intelligently accepting/rejecting proposals.

**How It Works (The Proposal Dance):**

**Step 1: Start Somewhere**
- Begin with initial value x‚ÇÄ (random guess)

**Step 2: Propose a Move**
- Generate candidate x* from proposal distribution q(x*|x·µ¢)
- Example: x* = x·µ¢ + Gaussian_noise (random walk)

**Step 3: Decide to Accept or Reject**
```python
# Calculate acceptance probability
alpha = min(1, p(x*)/p(x·µ¢) √ó q(x·µ¢|x*)/q(x*|x·µ¢))

# Flip a coin with probability alpha
if random(0,1) < alpha:
    x_{i+1} = x*  # Accept: Move to new position
else:
    x_{i+1} = x·µ¢  # Reject: Stay at current position
```

**Step 4: Repeat**
- Continue proposing and accepting/rejecting
- Over time, samples converge to target distribution!

**Beautiful Property:**
Even if proposal is simple (Gaussian), samples explore the true complex distribution!

---

## Slide 14: Metropolis-Hastings Implementation

```python
import numpy as np

def metropolis_hastings(target_pdf, proposal_sampler, 
                       initial_value, n_samples):
    """
    MCMC sampling using Metropolis-Hastings
    
    Args:
        target_pdf: Function p(x) we want to sample from
        proposal_sampler: Function to generate proposals
        initial_value: Starting point x‚ÇÄ
        n_samples: Number of samples to generate
    """
    # Initialize sample chain
    samples = np.zeros(n_samples)
    samples[0] = initial_value
    
    # Track acceptance rate
    n_accepted = 0
    
    for i in range(1, n_samples):
        current = samples[i-1]
        
        # Step 1: Generate proposal
        proposal = proposal_sampler(current)
        
        # Step 2: Calculate acceptance probability
        # alpha = min(1, p(proposal)/p(current))
        # For symmetric proposals, q cancels out
        acceptance_prob = min(1, 
            target_pdf(proposal) / target_pdf(current)
        )
        
        # Step 3: Accept or reject
        if np.random.rand() < acceptance_prob:
            samples[i] = proposal  # Accept
            n_accepted += 1
        else:
            samples[i] = current   # Reject (stay)
    
    print(f"Acceptance rate: {n_accepted/n_samples:.2%}")
    return samples

# Example: Sample from mixture of Gaussians
def target(x):
    # Mixture: 0.3√óN(0,1) + 0.7√óN(5,1)
    return (0.3 * np.exp(-x**2/2) + 
            0.7 * np.exp(-(x-5)**2/2))

def proposal(x):
    # Random walk: add Gaussian noise
    return x + np.random.normal(0, 1)

# Generate 10,000 samples
samples = metropolis_hastings(target, proposal, 0, 10000)

# Expected output: Samples from mixture distribution
```

---

## Slide 15: Gibbs Sampling - The Conditional Sampler

**What It Is:**
A special MCMC method that samples from joint distributions by sampling each variable conditionally (holding others fixed).

**When to Use:**
When you know conditional distributions p(x‚±º|x‚ÇÅ,...,x‚±º‚Çã‚ÇÅ,x‚±º‚Çä‚ÇÅ,...,x‚Çô) but not the joint p(x‚ÇÅ,...,x‚Çô)

**How It Works:**

**The Gibbs Sampling Algorithm:**
```
1. Initialize all variables: x‚ÇÅ‚ÅΩ‚Å∞‚Åæ, x‚ÇÇ‚ÅΩ‚Å∞‚Åæ, ..., x‚Çô‚ÅΩ‚Å∞‚Åæ

2. For iteration t = 1 to N:
   - Sample x‚ÇÅ‚ÅΩ·µó‚Åæ from p(x‚ÇÅ|x‚ÇÇ‚ÅΩ·µó‚Åª¬π‚Åæ, x‚ÇÉ‚ÅΩ·µó‚Åª¬π‚Åæ, ..., x‚Çô‚ÅΩ·µó‚Åª¬π‚Åæ)
   - Sample x‚ÇÇ‚ÅΩ·µó‚Åæ from p(x‚ÇÇ|x‚ÇÅ‚ÅΩ·µó‚Åæ, x‚ÇÉ‚ÅΩ·µó‚Åª¬π‚Åæ, ..., x‚Çô‚ÅΩ·µó‚Åª¬π‚Åæ)
   - ...
   - Sample x‚Çô‚ÅΩ·µó‚Åæ from p(x‚Çô|x‚ÇÅ‚ÅΩ·µó‚Åæ, x‚ÇÇ‚ÅΩ·µó‚Åæ, ..., x‚Çô‚Çã‚ÇÅ‚ÅΩ·µó‚Åæ)

3. Repeat until samples converge
```

**Key Advantage:**
- No accept/reject step (acceptance rate = 100%)!
- Simpler than Metropolis-Hastings when conditionals are known

**Real Example - Weather Prediction:**
```
Variables: Temperature, Humidity, Wind
- Sample Temperature given current Humidity, Wind
- Sample Humidity given new Temperature, current Wind  
- Sample Wind given new Temperature, new Humidity
- Repeat ‚Üí Get joint distribution of weather conditions!
```

---

## Slide 16: Real Project Story - Bayesian A/B Testing with MCMC

**Project:** Conversion Rate Optimization for AdTech Startup (2024)

**Background:**
- Online advertising platform testing new ad formats
- Problem: Standard A/B test says "Version B wins" but by how much?
- Management needs: Probability that B > A, not just p-value

**Technical Challenge:**
- Data: Version A (5,000 impressions, 250 clicks)
         Version B (5,000 impressions, 290 clicks)
- Want: Full posterior distribution of conversion rates
- Classic stats: Only gives p-value (p=0.04, "significant")
- Bayesian MCMC: Gives probability distribution!

**Solution Implemented:**
```python
# Model: Conversion rate ~ Beta distribution
# Used Gibbs sampling (beta-binomial conjugate)

# Results from 10,000 MCMC samples:
# P(conversion_B > conversion_A) = 94.2%
# Expected lift: 12-18% (95% credible interval)
# Probability lift > 10%: 87%
```

**Results:**
- **Before:** "B is better" (vague, p-value only)
- **After:** "94% sure B is better, expect 12-18% lift"
- **Business Decision:** Rolled out Version B confidently
- **Outcome:** 15% actual lift, $850K additional annual revenue
- **Time:** 3 days for MCMC implementation in Python

**Lesson Learned:**
MCMC gives decision-makers actionable probabilities, not just "statistically significant" binary results!

---

## Slide 17: Interview Questions - Be Prepared!

**Junior Level:**

**Q1:** Explain the difference between k-means and hierarchical clustering.
**A:** k-means requires specifying k upfront and finds flat clusters. Hierarchical creates a tree of clusters without pre-specifying count. k-means is faster (O(nk)) vs hierarchical (O(n¬≤)).

**Q2:** Why do we need to normalize features before k-means?
**A:** k-means uses Euclidean distance. Without normalization, features with larger ranges dominate. Example: income ($20K-$200K) overwhelms age (20-80), making age irrelevant to clustering.

**Mid Level:**

**Q3:** How would you choose the optimal k in k-means?
**A:** Use Elbow Method (plot error vs k, find "elbow"), Silhouette Score (measure cluster quality), or domain knowledge. No single "correct" answer - balance between complexity and interpretability.

**Q4:** Explain why MCMC samples eventually converge to the target distribution.
**A:** Due to detailed balance condition: p(x)T(x‚Üíx') = p(x')T(x'‚Üíx). This ensures the Markov chain is reversible and has p(x) as its equilibrium distribution.

**Senior Level:**

**Q5:** Design a system to detect anomalous network traffic using unsupervised learning at scale (millions of requests/second).
**A:** Use online k-means or mini-batch SOM for real-time clustering. Stream data through incremental updates. Anomalies = points far from all clusters. Challenges: concept drift (update clusters periodically), high-dimensional data (PCA preprocessing), scalability (distributed processing).

---

## Slide 18: Common Challenges and Solutions

**Problem 1: k-Means Stuck in Local Minimum**

**Symptoms:**
- Different runs give different results
- Some clusters are empty or very small
- High total error despite convergence

**Solutions:**
- Run k-means multiple times (10-50) with different initializations
- Use k-means++ initialization (smarter starting points)
- Try different values of k
- Check if data is actually clusterable (Hopkins statistic)

---

**Problem 2: SOM Not Preserving Topology**

**Symptoms:**
- Similar inputs map to distant neurons
- Map looks random, no clear organization

**Solutions:**
- Increase grid size (more neurons = better resolution)
- Train longer (more iterations)
- Start with large neighborhood, decay slowly
- Use proper data normalization
- Initialize with PCA (better starting point)

---

**Problem 3: MCMC Chains Not Mixing**

**Symptoms:**
- Samples stay in one region
- High autocorrelation between consecutive samples
- Multiple chains converge to different values

**Solutions:**
- Increase proposal variance (explore more)
- Run longer (more samples = better mixing)
- Use multiple chains, check convergence (Gelman-Rubin statistic)
- Try different MCMC variant (HMC, NUTS)
- Improve proposal distribution

---

## Slide 19: When to Use Which Method?

**Decision Framework:**

**Use k-Means When:**
‚úÖ You know approximate number of clusters
‚úÖ Clusters are roughly spherical/compact
‚úÖ Need fast, scalable solution
‚úÖ Working with numerical data only
‚ùå Don't use: Complex cluster shapes, mixed data types

**Use SOM When:**
‚úÖ Need dimensionality reduction + clustering
‚úÖ Want to visualize high-dimensional data
‚úÖ Topology preservation is important
‚úÖ Have time for longer training
‚ùå Don't use: Need fast results, have very sparse data

**Use MCMC When:**
‚úÖ Need full probability distributions (not just point estimates)
‚úÖ Working with Bayesian models
‚úÖ Have complex, high-dimensional posteriors
‚úÖ Need uncertainty quantification
‚ùå Don't use: Need real-time results, have simple distributions (use analytical solutions)

**Real-World Scenarios:**
- Customer segmentation ‚Üí **k-Means** (fast, interpretable)
- Gene expression visualization ‚Üí **SOM** (high-dimensional patterns)
- Clinical trial analysis ‚Üí **MCMC** (uncertainty crucial for medical decisions)

---

## Slide 20: Key Takeaways & Next Steps

**üéØ Core Concepts Mastered:**

1. **Unsupervised Learning**
   - Finding patterns without labels
   - k-Means: Fast clustering via iterative center updates
   - SOM: Topology-preserving maps for visualization

2. **MCMC Methods**
   - Sampling from complex distributions
   - Metropolis-Hastings: Accept/reject proposals
   - Gibbs: Conditional sampling for joint distributions

3. **Practical Skills**
   - Feature normalization is CRITICAL
   - Always run multiple initializations
   - Check convergence diagnostics
   - Visualize results to validate

---

**üí° Real-World Impact:**
- Customer segmentation: $2.8M revenue increase
- Bayesian A/B testing: 94% confidence in decisions
- Fraud detection: Real-time anomaly detection

---

**üìö Next Module Preview:**
Module 5 will cover **Deep Learning Architectures**:
- Convolutional Neural Networks (CNNs)
- Recurrent Neural Networks (RNNs)
- Transfer Learning & Pre-trained Models

---

**üîß Hands-On Assignment (Due Next Week):**

**Task 1:** Implement k-means from scratch (no libraries) on Iris dataset
- Objective: Group flowers into species
- Requirements: 
  - Code initialization, assignment, update steps
  - Visualize clusters with different k values
  - Calculate silhouette scores

**Task 2:** Use SOM to visualize MNIST digit clusters
- Objective: Create 2D map of handwritten digits
- Requirements:
  - Train 10√ó10 SOM on MNIST data
  - Visualize which neurons respond to which digits
  - Analyze topology preservation

**Challenge (Optional +10 points):**
Implement Metropolis-Hastings to sample from a custom bimodal distribution and compare convergence rates with different proposal distributions.

**Deliverables:** Jupyter notebook + 2-page report
**Deadline:** October 25, 2025, 11:59 PM

---

**Questions? Office Hours: Wednesdays 2-4 PM**

---

*"In God we trust, all others must bring data... and sometimes, we don't even have labels for that data!" - Adapted from W. Edwards Deming*