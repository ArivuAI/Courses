{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1: Introduction & Concept Learning\n",
    "**Course:** Arivu AI Machine Learning Course  \n",
    "**Duration:** 4-6 hours  \n",
    "**Prerequisites:** Basic Python programming, understanding of basic mathematics\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this module, you will be able to:\n",
    "\n",
    "1. **Define** well-posed learning problems with task, performance, and experience\n",
    "2. **Design** the four key components of any learning system\n",
    "3. **Explain** concept learning as a search through hypothesis space\n",
    "4. **Apply** the Find-S algorithm to find maximally specific hypotheses\n",
    "5. **Implement** the Candidate-Elimination algorithm using version spaces\n",
    "6. **Analyze** the role of inductive bias in learning\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Why This Matters - The $2 Million Question\n",
    "\n",
    "### Real-World Impact\n",
    "\n",
    "**Think About This:**\n",
    "- Netflix recommends shows you'll love‚Äîhow does it know?\n",
    "- Gmail blocks spam emails automatically‚Äîwho taught it?\n",
    "- Self-driving cars recognize pedestrians‚Äîbut never took a driving lesson\n",
    "\n",
    "**The Business Impact:**\n",
    "- Companies lose $62 billion annually due to poor customer understanding\n",
    "- Machine Learning reduces fraud detection costs by 40%\n",
    "- Personalization engines increase revenue by 15-20%\n",
    "\n",
    "**The Challenge:** Can we teach computers to improve from experience without explicitly programming every scenario?\n",
    "\n",
    "### Personal Experience Story\n",
    "\n",
    "**Project:** Email Spam Filter for Enterprise Client  \n",
    "**Challenge:** Company was losing 2 hours per employee per week to spam emails (500 employees = 1,000 hours/week wasted)  \n",
    "**Solution:** Implemented a machine learning-based spam filter using concept learning principles  \n",
    "**Impact:** \n",
    "- Reduced spam by 98.5%\n",
    "- Saved approximately $1.2M annually in productivity\n",
    "- False positive rate < 0.1% (critical for business emails)\n",
    "\n",
    "This module teaches you the **fundamental concepts** that power such systems!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Setup & Dependencies\n",
    "\n",
    "Let's start by importing the necessary libraries and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Set\n",
    "from copy import deepcopy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configure display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"üìä NumPy version: {np.__version__}\")\n",
    "print(f\"üìä Pandas version: {pd.__version__}\")\n",
    "print(f\"üìä Matplotlib version: {plt.matplotlib.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Helper Functions\n",
    "\n",
    "Let's define some utility functions we'll use throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_data(filename: str) -> dict:\n",
    "    \"\"\"\n",
    "    Load JSON data from the data folder.\n",
    "    \n",
    "    Args:\n",
    "        filename: Name of the JSON file\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing the loaded data\n",
    "    \"\"\"\n",
    "    data_path = Path('data') / filename\n",
    "    with open(data_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def print_section_header(title: str, emoji: str = \"üìö\"):\n",
    "    \"\"\"\n",
    "    Print a formatted section header.\n",
    "    \n",
    "    Args:\n",
    "        title: Section title\n",
    "        emoji: Emoji to display\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"{emoji} {title}\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "def visualize_hypothesis(hypothesis: list, attribute_names: list, title: str = \"Hypothesis\"):\n",
    "    \"\"\"\n",
    "    Visualize a hypothesis as a formatted table.\n",
    "    \n",
    "    Args:\n",
    "        hypothesis: List representing the hypothesis\n",
    "        attribute_names: Names of attributes\n",
    "        title: Title for the visualization\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame([hypothesis], columns=attribute_names)\n",
    "    print(f\"\\n{title}:\")\n",
    "    print(df.to_string(index=False))\n",
    "    print()\n",
    "\n",
    "print(\"‚úÖ Helper functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Understanding Machine Learning Fundamentals\n",
    "\n",
    "## üß† Slide 2: Learning Like Humans Do\n",
    "\n",
    "### The Human Learning Process\n",
    "\n",
    "Let's understand how humans learn by solving a simple equation, and then see how this parallels machine learning!\n",
    "\n",
    "**Problem:** Solve 2x + 3 = 9\n",
    "\n",
    "**Human Approach:**\n",
    "1. **Trial 1:** Try x = 2 ‚Üí Result: 2(2) + 3 = 7 ‚Üí Error: Off by 2 ‚Üí Learning: \"x needs to be bigger\"\n",
    "2. **Trial 2:** Try x = 3 ‚Üí Result: 2(3) + 3 = 9 ‚úì ‚Üí Success!\n",
    "\n",
    "**Machine Learning Parallel:**\n",
    "- Trial = Iteration/Epoch\n",
    "- Error = Loss Function\n",
    "- Adjustment = Gradient Descent\n",
    "- Memory = Learned Weights\n",
    "\n",
    "Let's simulate this learning process!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulating Human-Like Learning for Equation Solving\n",
    "print_section_header(\"Human-Like Learning Simulation\", \"üß†\")\n",
    "\n",
    "def equation_value(x):\n",
    "    \"\"\"Calculate 2x + 3\"\"\"\n",
    "    return 2 * x + 3\n",
    "\n",
    "def calculate_error(predicted, target=9):\n",
    "    \"\"\"Calculate how far off we are from the target\"\"\"\n",
    "    return abs(target - predicted)\n",
    "\n",
    "# Learning process\n",
    "target = 9\n",
    "trials = []\n",
    "\n",
    "print(\"üéØ Goal: Find x such that 2x + 3 = 9\\n\")\n",
    "\n",
    "# Trial 1\n",
    "x_trial1 = 2\n",
    "result1 = equation_value(x_trial1)\n",
    "error1 = calculate_error(result1, target)\n",
    "trials.append({'Trial': 1, 'x': x_trial1, 'Result': result1, 'Error': error1, 'Learning': 'x needs to be bigger'})\n",
    "print(f\"Trial 1: x = {x_trial1}\")\n",
    "print(f\"  Result: 2({x_trial1}) + 3 = {result1}\")\n",
    "print(f\"  Error: {error1}\")\n",
    "print(f\"  Learning: x needs to be bigger\\n\")\n",
    "\n",
    "# Trial 2\n",
    "x_trial2 = 3\n",
    "result2 = equation_value(x_trial2)\n",
    "error2 = calculate_error(result2, target)\n",
    "trials.append({'Trial': 2, 'x': x_trial2, 'Result': result2, 'Error': error2, 'Learning': 'Success!'})\n",
    "print(f\"Trial 2: x = {x_trial2}\")\n",
    "print(f\"  Result: 2({x_trial2}) + 3 = {result2}\")\n",
    "print(f\"  Error: {error2}\")\n",
    "print(f\"  Learning: Success! ‚úì\\n\")\n",
    "\n",
    "# Visualize the learning process\n",
    "trials_df = pd.DataFrame(trials)\n",
    "print(\"üìä Learning Progress:\")\n",
    "print(trials_df.to_string(index=False))\n",
    "\n",
    "# Plot the learning curve\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(trials_df['Trial'], trials_df['Error'], marker='o', linewidth=2, markersize=10)\n",
    "plt.xlabel('Trial Number', fontsize=12)\n",
    "plt.ylabel('Error', fontsize=12)\n",
    "plt.title('Learning Curve: Error Reduction', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(trials_df['Trial'], trials_df['Result'], marker='s', linewidth=2, markersize=10, label='Predicted')\n",
    "plt.axhline(y=target, color='r', linestyle='--', linewidth=2, label='Target')\n",
    "plt.xlabel('Trial Number', fontsize=12)\n",
    "plt.ylabel('Result', fontsize=12)\n",
    "plt.title('Convergence to Target', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Insight: Machine learning algorithms use the same principle!\")\n",
    "print(\"   They make predictions, measure errors, and adjust to improve.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Slide 3: The ML Learning Cycle\n",
    "\n",
    "### Understanding the Complete Learning Process\n",
    "\n",
    "Machine learning follows a cyclical process:\n",
    "\n",
    "```\n",
    "RAW DATA ‚Üí LEARNING ALGORITHM ‚Üí HYPOTHESIS ‚Üí PREDICTIONS ‚Üí FEEDBACK/ERROR ‚Üí (Improve)\n",
    "```\n",
    "\n",
    "Let's visualize this cycle with a real example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the ML Learning Cycle\n",
    "print_section_header(\"The ML Learning Cycle\", \"üìä\")\n",
    "\n",
    "# Create a visual representation\n",
    "from matplotlib.patches import FancyBboxPatch, FancyArrowPatch\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.axis('off')\n",
    "\n",
    "# Define components\n",
    "components = [\n",
    "    {'name': 'RAW DATA', 'pos': (1, 8), 'color': '#3498db', 'example': 'Emails, Images, Sensors'},\n",
    "    {'name': 'LEARNING\\nALGORITHM', 'pos': (1, 5.5), 'color': '#e74c3c', 'example': 'Find-S, Neural Nets'},\n",
    "    {'name': 'HYPOTHESIS', 'pos': (5, 5.5), 'color': '#2ecc71', 'example': 'Learned Rules/Patterns'},\n",
    "    {'name': 'PREDICTIONS', 'pos': (8, 5.5), 'color': '#f39c12', 'example': 'Test on New Data'},\n",
    "    {'name': 'FEEDBACK/\\nERROR', 'pos': (8, 2), 'color': '#9b59b6', 'example': 'Improve'}\n",
    "]\n",
    "\n",
    "# Draw boxes\n",
    "for comp in components:\n",
    "    box = FancyBboxPatch((comp['pos'][0]-0.6, comp['pos'][1]-0.4), 1.2, 0.8,\n",
    "                          boxstyle=\"round,pad=0.1\", edgecolor=comp['color'],\n",
    "                          facecolor=comp['color'], alpha=0.3, linewidth=3)\n",
    "    ax.add_patch(box)\n",
    "    ax.text(comp['pos'][0], comp['pos'][1], comp['name'],\n",
    "            ha='center', va='center', fontsize=11, fontweight='bold')\n",
    "    ax.text(comp['pos'][0], comp['pos'][1]-0.7, f\"({comp['example']})\",\n",
    "            ha='center', va='center', fontsize=8, style='italic')\n",
    "\n",
    "# Draw arrows\n",
    "arrows = [\n",
    "    ((1, 7.5), (1, 6.3)),      # Data to Algorithm\n",
    "    ((1.6, 5.5), (4.4, 5.5)),  # Algorithm to Hypothesis\n",
    "    ((5.6, 5.5), (7.4, 5.5)),  # Hypothesis to Predictions\n",
    "    ((8, 4.7), (8, 2.8)),      # Predictions to Feedback\n",
    "    ((7.4, 2), (1.6, 5.1))     # Feedback back to Algorithm\n",
    "]\n",
    "\n",
    "for start, end in arrows:\n",
    "    arrow = FancyArrowPatch(start, end, arrowstyle='->', mutation_scale=30,\n",
    "                           linewidth=2.5, color='#34495e', alpha=0.7)\n",
    "    ax.add_patch(arrow)\n",
    "\n",
    "plt.title('The Machine Learning Cycle', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüîÑ Key Components:\")\n",
    "print(\"  1. Data feeds the algorithm\")\n",
    "print(\"  2. Algorithm searches for patterns\")\n",
    "print(\"  3. Creates a hypothesis (model)\")\n",
    "print(\"  4. Tests and improves iteratively\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Well-Posed Learning Problems\n",
    "\n",
    "## üéØ Slide 5: What is a Well-Posed Learning Problem?\n",
    "\n",
    "### The Three Essential Elements\n",
    "\n",
    "**Definition:** A computer program learns from experience **E** with respect to some task **T** and performance measure **P**, if its performance at **T** (measured by **P**) improves with experience **E**.\n",
    "\n",
    "**The Three Components:**\n",
    "1. **Task (T)** - What are we trying to do?\n",
    "2. **Performance Measure (P)** - How do we measure success?\n",
    "3. **Experience (E)** - What data do we learn from?\n",
    "\n",
    "Let's explore this with real examples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples of Well-Posed Learning Problems\n",
    "print_section_header(\"Well-Posed Learning Problems\", \"üéØ\")\n",
    "\n",
    "# Define several learning problems\n",
    "learning_problems = [\n",
    "    {\n",
    "        'Domain': 'Email Spam Filter',\n",
    "        'Task (T)': 'Classify emails as spam or not spam',\n",
    "        'Performance (P)': 'Accuracy % (correctly classified)',\n",
    "        'Experience (E)': 'Database of labeled emails',\n",
    "        'Business Impact': 'Save 2 hours/employee/week'\n",
    "    },\n",
    "    {\n",
    "        'Domain': 'House Price Prediction',\n",
    "        'Task (T)': 'Predict house sale price',\n",
    "        'Performance (P)': 'Mean Absolute Error ($ difference)',\n",
    "        'Experience (E)': 'Historical house sales data',\n",
    "        'Business Impact': 'Better pricing decisions'\n",
    "    },\n",
    "    {\n",
    "        'Domain': 'Fraud Detection',\n",
    "        'Task (T)': 'Identify fraudulent transactions',\n",
    "        'Performance (P)': 'F1-Score (balance precision/recall)',\n",
    "        'Experience (E)': 'Past transactions with fraud labels',\n",
    "        'Business Impact': 'Reduce fraud losses by 40%'\n",
    "    },\n",
    "    {\n",
    "        'Domain': 'Medical Diagnosis',\n",
    "        'Task (T)': 'Diagnose disease from symptoms',\n",
    "        'Performance (P)': 'Diagnostic accuracy %',\n",
    "        'Experience (E)': 'Patient records with diagnoses',\n",
    "        'Business Impact': 'Earlier disease detection'\n",
    "    },\n",
    "    {\n",
    "        'Domain': 'Checkers Game',\n",
    "        'Task (T)': 'Play checkers',\n",
    "        'Performance (P)': '% of games won',\n",
    "        'Experience (E)': 'Games played against itself',\n",
    "        'Business Impact': 'World championship level play'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Display as a formatted table\n",
    "df_problems = pd.DataFrame(learning_problems)\n",
    "print(\"üìã Examples of Well-Posed Learning Problems:\\n\")\n",
    "print(df_problems.to_string(index=False))\n",
    "\n",
    "# Visualize the components\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "components = ['Task (T)', 'Performance (P)', 'Experience (E)']\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "\n",
    "for idx, (component, color) in enumerate(zip(components, colors)):\n",
    "    ax = axes[idx]\n",
    "    values = df_problems[component].tolist()\n",
    "    domains = df_problems['Domain'].tolist()\n",
    "    \n",
    "    y_pos = np.arange(len(domains))\n",
    "    ax.barh(y_pos, [1]*len(domains), color=color, alpha=0.6)\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(domains, fontsize=9)\n",
    "    ax.set_xlim(0, 1.2)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_title(component, fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i, (domain, value) in enumerate(zip(domains, values)):\n",
    "        ax.text(0.05, i, value, va='center', fontsize=8, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Insight: A well-defined problem is half solved!\")\n",
    "print(\"   Always clearly define T, P, and E before building a solution.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìß Slide 6: Real-World Example - Email Spam Filter\n",
    "\n",
    "### Deep Dive into a Practical Application\n",
    "\n",
    "Let's load our spam email dataset and explore how we can frame this as a well-posed learning problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and explore the spam email dataset\n",
    "print_section_header(\"Email Spam Filter Example\", \"üìß\")\n",
    "\n",
    "# Load the data\n",
    "spam_data = load_json_data('spam_email_dataset.json')\n",
    "\n",
    "print(\"üìä Dataset Description:\")\n",
    "print(f\"   {spam_data['description']}\\n\")\n",
    "\n",
    "print(\"üîç Attributes (Features):\")\n",
    "for attr, values in spam_data['attributes'].items():\n",
    "    print(f\"   ‚Ä¢ {attr}: {values}\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "train_emails = pd.DataFrame(spam_data['training_data'])\n",
    "test_emails = pd.DataFrame(spam_data['test_data'])\n",
    "\n",
    "print(f\"\\nüìà Training Data: {len(train_emails)} emails\")\n",
    "print(f\"üìà Test Data: {len(test_emails)} emails\\n\")\n",
    "\n",
    "# Display sample emails\n",
    "print(\"üì¨ Sample Training Emails:\\n\")\n",
    "display_cols = ['email_id', 'subject', 'has_money_words', 'has_urgent_words', \n",
    "                'from_known_sender', 'is_spam']\n",
    "print(train_emails[display_cols].head(6).to_string(index=False))\n",
    "\n",
    "# Analyze spam vs non-spam distribution\n",
    "spam_counts = train_emails['is_spam'].value_counts()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Pie chart\n",
    "axes[0].pie(spam_counts.values, labels=['Not Spam', 'Spam'], autopct='%1.1f%%',\n",
    "            colors=['#2ecc71', '#e74c3c'], startangle=90, textprops={'fontsize': 12})\n",
    "axes[0].set_title('Email Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Feature correlation with spam\n",
    "feature_cols = ['has_money_words', 'has_urgent_words', 'has_links', \n",
    "                'from_known_sender', 'has_attachments', 'proper_grammar']\n",
    "\n",
    "# Calculate spam correlation for each feature\n",
    "spam_correlation = {}\n",
    "for feature in feature_cols:\n",
    "    spam_yes = train_emails[train_emails['is_spam'] == 'Yes'][feature].value_counts()\n",
    "    if 'Yes' in spam_yes:\n",
    "        spam_correlation[feature] = spam_yes['Yes'] / len(train_emails[train_emails['is_spam'] == 'Yes']) * 100\n",
    "    else:\n",
    "        spam_correlation[feature] = 0\n",
    "\n",
    "axes[1].barh(list(spam_correlation.keys()), list(spam_correlation.values()), \n",
    "             color='#e74c3c', alpha=0.7)\n",
    "axes[1].set_xlabel('% of Spam Emails with Feature', fontsize=11)\n",
    "axes[1].set_title('Features in Spam Emails', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ Well-Posed Problem Definition:\")\n",
    "print(\"   Task (T): Classify emails as spam or not spam\")\n",
    "print(\"   Performance (P): Accuracy % (correctly classified)\")\n",
    "print(\"   Experience (E): Database of labeled emails with features\")\n",
    "print(\"\\nüíº Business Impact:\")\n",
    "print(\"   ‚Ä¢ Gmail blocks 99.9% of spam and phishing attempts\")\n",
    "print(\"   ‚Ä¢ Processes 100+ billion spam attempts daily\")\n",
    "print(\"   ‚Ä¢ False positive rate < 0.05% (critical for user trust)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Designing a Learning System\n",
    "\n",
    "## ‚ôüÔ∏è Slide 7-8: The Checkers Game Example\n",
    "\n",
    "### Four Key Design Choices\n",
    "\n",
    "Let's design a learning system for playing checkers. This classic example illustrates the fundamental design decisions in any ML system.\n",
    "\n",
    "**The Four Design Choices:**\n",
    "1. **Determine the Training Experience**\n",
    "2. **Choose the Target Function**\n",
    "3. **Choose Representation**\n",
    "4. **Choose Learning Algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkers training data\n",
    "print_section_header(\"Checkers Learning System Design\", \"‚ôüÔ∏è\")\n",
    "\n",
    "checkers_data = load_json_data('checkers_training_data.json')\n",
    "\n",
    "print(\"üéÆ Checkers Learning Problem:\")\n",
    "print(\"   Task (T): Playing checkers\")\n",
    "print(\"   Performance (P): % of games won in world tournament\")\n",
    "print(\"   Experience (E): Games played against itself\\n\")\n",
    "\n",
    "print(\"üìä Board Features (Input):\")\n",
    "for feature, description in checkers_data['features'].items():\n",
    "    print(f\"   {feature}: {description}\")\n",
    "\n",
    "print(f\"\\nüéØ Target Function:\")\n",
    "print(f\"   {checkers_data['target_function']}\")\n",
    "print(\"\\n   Where:\")\n",
    "print(\"   ‚Ä¢ w0 = bias term (constant)\")\n",
    "print(\"   ‚Ä¢ w1...w6 = weights to be learned\")\n",
    "print(\"   ‚Ä¢ V(b) = evaluation score for board state b\")\n",
    "print(\"   ‚Ä¢ V(b) = +100 if win, -100 if loss, 0 if draw\\n\")\n",
    "\n",
    "# Load and display training examples\n",
    "train_boards = pd.DataFrame(checkers_data['training_examples'])\n",
    "\n",
    "print(\"üìã Sample Training Examples:\\n\")\n",
    "display_cols = ['board_id', 'x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'V_train', 'description']\n",
    "print(train_boards[display_cols].head(8).to_string(index=False))\n",
    "\n",
    "# Visualize board evaluations\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Scatter plot: Piece advantage vs evaluation\n",
    "train_boards['piece_advantage'] = train_boards['x1'] - train_boards['x2']\n",
    "train_boards['king_advantage'] = train_boards['x3'] - train_boards['x4']\n",
    "\n",
    "scatter = axes[0].scatter(train_boards['piece_advantage'], train_boards['V_train'],\n",
    "                          c=train_boards['V_train'], cmap='RdYlGn', s=100, alpha=0.6,\n",
    "                          edgecolors='black', linewidth=1.5)\n",
    "axes[0].axhline(y=0, color='black', linestyle='--', alpha=0.3)\n",
    "axes[0].axvline(x=0, color='black', linestyle='--', alpha=0.3)\n",
    "axes[0].set_xlabel('Piece Advantage (Black - Red)', fontsize=11)\n",
    "axes[0].set_ylabel('Board Evaluation V(b)', fontsize=11)\n",
    "axes[0].set_title('Piece Advantage vs Board Value', fontsize=13, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter, ax=axes[0], label='Evaluation')\n",
    "\n",
    "# Bar chart: Feature importance visualization\n",
    "feature_names = ['Black\\nPieces', 'Red\\nPieces', 'Black\\nKings', \n",
    "                 'Red\\nKings', 'Black\\nThreatened', 'Red\\nThreatened']\n",
    "avg_values = [train_boards['x1'].mean(), train_boards['x2'].mean(),\n",
    "              train_boards['x3'].mean(), train_boards['x4'].mean(),\n",
    "              train_boards['x5'].mean(), train_boards['x6'].mean()]\n",
    "\n",
    "colors_bar = ['#2c3e50', '#e74c3c', '#34495e', '#c0392b', '#7f8c8d', '#95a5a6']\n",
    "axes[1].bar(feature_names, avg_values, color=colors_bar, alpha=0.7, edgecolor='black')\n",
    "axes[1].set_ylabel('Average Value', fontsize=11)\n",
    "axes[1].set_title('Average Feature Values in Training Data', fontsize=13, fontweight='bold')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Design Decisions:\")\n",
    "print(\"   1. Training Experience: Self-play (no external trainer needed)\")\n",
    "print(\"   2. Target Function: V(b) - board evaluation function\")\n",
    "print(\"   3. Representation: Linear combination of 6 board features\")\n",
    "print(\"   4. Learning Algorithm: LMS (Least Mean Squares) weight update\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the LMS Weight Update Rule\n",
    "\n",
    "The **Least Mean Squares (LMS)** algorithm adjusts weights to minimize the squared error between predicted and actual board evaluations.\n",
    "\n",
    "**LMS Update Rule:**\n",
    "```\n",
    "For each training example (b, V_train(b)):\n",
    "  1. Calculate V(b) using current weights\n",
    "  2. For each weight wi:\n",
    "       wi ‚Üê wi + Œ∑ * (V_train(b) - V(b)) * xi\n",
    "```\n",
    "\n",
    "Where:\n",
    "- Œ∑ (eta) = learning rate (e.g., 0.1)\n",
    "- V_train(b) = target value\n",
    "- V(b) = predicted value\n",
    "- xi = feature value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement LMS algorithm for checkers\n",
    "print_section_header(\"LMS Algorithm Implementation\", \"üîß\")\n",
    "\n",
    "class CheckersLearner:\n",
    "    \"\"\"Simple checkers board evaluator using LMS algorithm\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        \"\"\"Initialize with random small weights\"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        # Initialize weights: w0 (bias), w1-w6 (features)\n",
    "        self.weights = np.random.randn(7) * 0.1\n",
    "        self.training_history = []\n",
    "    \n",
    "    def evaluate_board(self, features):\n",
    "        \"\"\"\n",
    "        Evaluate board position using current weights.\n",
    "        V(b) = w0 + w1*x1 + w2*x2 + ... + w6*x6\n",
    "        \"\"\"\n",
    "        # Add bias term (1.0) to features\n",
    "        features_with_bias = np.concatenate([[1.0], features])\n",
    "        return np.dot(self.weights, features_with_bias)\n",
    "    \n",
    "    def train_step(self, features, target_value):\n",
    "        \"\"\"\n",
    "        Perform one LMS weight update step.\n",
    "        \"\"\"\n",
    "        # Calculate prediction\n",
    "        predicted_value = self.evaluate_board(features)\n",
    "        \n",
    "        # Calculate error\n",
    "        error = target_value - predicted_value\n",
    "        \n",
    "        # Update weights: wi ‚Üê wi + Œ∑ * error * xi\n",
    "        features_with_bias = np.concatenate([[1.0], features])\n",
    "        self.weights += self.learning_rate * error * features_with_bias\n",
    "        \n",
    "        return predicted_value, error\n",
    "    \n",
    "    def train(self, training_data, epochs=100):\n",
    "        \"\"\"\n",
    "        Train on multiple epochs through the data.\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            total_error = 0\n",
    "            \n",
    "            for example in training_data:\n",
    "                features = np.array([example['x1'], example['x2'], example['x3'],\n",
    "                                   example['x4'], example['x5'], example['x6']])\n",
    "                target = example['V_train']\n",
    "                \n",
    "                predicted, error = self.train_step(features, target)\n",
    "                total_error += error ** 2\n",
    "            \n",
    "            # Calculate mean squared error\n",
    "            mse = total_error / len(training_data)\n",
    "            self.training_history.append(mse)\n",
    "            \n",
    "            if (epoch + 1) % 20 == 0:\n",
    "                print(f\"Epoch {epoch+1:3d}: MSE = {mse:8.2f}\")\n",
    "\n",
    "# Train the model\n",
    "learner = CheckersLearner(learning_rate=0.01)\n",
    "\n",
    "print(\"üéì Training Checkers Board Evaluator...\\n\")\n",
    "print(\"Initial weights (random):\")\n",
    "print(f\"  w0={learner.weights[0]:.3f}, w1={learner.weights[1]:.3f}, w2={learner.weights[2]:.3f},\")\n",
    "print(f\"  w3={learner.weights[3]:.3f}, w4={learner.weights[4]:.3f}, w5={learner.weights[5]:.3f}, w6={learner.weights[6]:.3f}\\n\")\n",
    "\n",
    "learner.train(checkers_data['training_examples'], epochs=100)\n",
    "\n",
    "print(\"\\n‚úÖ Training Complete!\\n\")\n",
    "print(\"Learned weights:\")\n",
    "print(f\"  w0={learner.weights[0]:.3f} (bias)\")\n",
    "print(f\"  w1={learner.weights[1]:.3f} (black pieces)\")\n",
    "print(f\"  w2={learner.weights[2]:.3f} (red pieces)\")\n",
    "print(f\"  w3={learner.weights[3]:.3f} (black kings)\")\n",
    "print(f\"  w4={learner.weights[4]:.3f} (red kings)\")\n",
    "print(f\"  w5={learner.weights[5]:.3f} (black threatened)\")\n",
    "print(f\"  w6={learner.weights[6]:.3f} (red threatened)\")\n",
    "\n",
    "# Plot learning curve\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(learner.training_history, linewidth=2, color='#3498db')\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Mean Squared Error', fontsize=12)\n",
    "plt.title('Learning Curve: Error Reduction Over Time', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Test on new board positions\n",
    "print(\"\\nüß™ Testing on New Board Positions:\\n\")\n",
    "for test_board in checkers_data['test_examples']:\n",
    "    features = np.array([test_board['x1'], test_board['x2'], test_board['x3'],\n",
    "                        test_board['x4'], test_board['x5'], test_board['x6']])\n",
    "    prediction = learner.evaluate_board(features)\n",
    "    \n",
    "    print(f\"Board {test_board['board_id']}: {test_board['description']}\")\n",
    "    print(f\"  Features: x1={test_board['x1']}, x2={test_board['x2']}, x3={test_board['x3']}, \"\n",
    "          f\"x4={test_board['x4']}, x5={test_board['x5']}, x6={test_board['x6']}\")\n",
    "    print(f\"  Predicted V(b) = {prediction:.2f}\")\n",
    "    \n",
    "    if prediction > 20:\n",
    "        print(f\"  ‚Üí Black has advantage! üéØ\\n\")\n",
    "    elif prediction < -20:\n",
    "        print(f\"  ‚Üí Red has advantage! ‚ö†Ô∏è\\n\")\n",
    "    else:\n",
    "        print(f\"  ‚Üí Balanced position ‚öñÔ∏è\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Concept Learning Fundamentals\n",
    "\n",
    "## üåä Slide 10: Concept Learning - The Core Problem\n",
    "\n",
    "### Learning Boolean-Valued Functions\n",
    "\n",
    "**What is Concept Learning?**\n",
    "Inferring a boolean-valued function from training examples of its input and output.\n",
    "\n",
    "**Classic Example:** Learning \"days Aldo enjoys water sports\"\n",
    "\n",
    "Let's load and explore the EnjoySport dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load EnjoySport dataset\n",
    "print_section_header(\"Concept Learning: EnjoySport Dataset\", \"üåä\")\n",
    "\n",
    "enjoysport_data = load_json_data('enjoysport_dataset.json')\n",
    "\n",
    "print(\"üìä Dataset Description:\")\n",
    "print(f\"   {enjoysport_data['description']}\\n\")\n",
    "\n",
    "print(\"üîç Attributes (Features):\")\n",
    "for attr, values in enjoysport_data['attributes'].items():\n",
    "    print(f\"   ‚Ä¢ {attr}: {values}\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "train_enjoysport = pd.DataFrame(enjoysport_data['training_data'])\n",
    "test_enjoysport = pd.DataFrame(enjoysport_data['test_data'])\n",
    "\n",
    "print(f\"\\nüìà Training Examples: {len(train_enjoysport)}\")\n",
    "print(f\"üìà Test Examples: {len(test_enjoysport)}\\n\")\n",
    "\n",
    "# Display training data\n",
    "print(\"üìã Training Data:\\n\")\n",
    "display_cols = ['example_id', 'Sky', 'AirTemp', 'Humidity', 'Wind', 'Water', 'Forecast', 'EnjoySport']\n",
    "print(train_enjoysport[display_cols].to_string(index=False))\n",
    "\n",
    "# Visualize the data\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "attributes = ['Sky', 'AirTemp', 'Humidity', 'Wind', 'Water', 'Forecast']\n",
    "colors_map = {'Yes': '#2ecc71', 'No': '#e74c3c'}\n",
    "\n",
    "for idx, attr in enumerate(attributes):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Count occurrences for each value\n",
    "    yes_data = train_enjoysport[train_enjoysport['EnjoySport'] == 'Yes'][attr].value_counts()\n",
    "    no_data = train_enjoysport[train_enjoysport['EnjoySport'] == 'No'][attr].value_counts()\n",
    "    \n",
    "    # Create grouped bar chart\n",
    "    x = np.arange(len(enjoysport_data['attributes'][attr]))\n",
    "    width = 0.35\n",
    "    \n",
    "    yes_counts = [yes_data.get(val, 0) for val in enjoysport_data['attributes'][attr]]\n",
    "    no_counts = [no_data.get(val, 0) for val in enjoysport_data['attributes'][attr]]\n",
    "    \n",
    "    ax.bar(x - width/2, yes_counts, width, label='EnjoySport=Yes', color='#2ecc71', alpha=0.7)\n",
    "    ax.bar(x + width/2, no_counts, width, label='EnjoySport=No', color='#e74c3c', alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel(attr, fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel('Count', fontsize=10)\n",
    "    ax.set_title(f'{attr} Distribution', fontsize=12, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(enjoysport_data['attributes'][attr], rotation=45, ha='right')\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ The Challenge: Can you predict if Aldo will enjoy water sports on a new day?\")\n",
    "print(\"   We need to learn a hypothesis that generalizes from these examples!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Slide 11: Hypothesis Representation\n",
    "\n",
    "### Describing Concepts with Constraints\n",
    "\n",
    "Each hypothesis is a conjunction of constraints on attributes:\n",
    "\n",
    "**Three Types of Constraints:**\n",
    "1. **\"?\" (Any value)** - This attribute can be anything\n",
    "2. **Specific value** - Must match exactly (e.g., \"Warm\")\n",
    "3. **\"‚àÖ\" (No value)** - Impossible to satisfy (all negative)\n",
    "\n",
    "**Example Hypotheses:**\n",
    "- h‚ÇÅ = ‚ü®Sunny, ?, ?, Strong, ?, ?‚ü© ‚Üí Sky=Sunny AND Wind=Strong\n",
    "- h‚ÇÇ = ‚ü®?, Warm, Normal, Strong, Warm, Same‚ü© ‚Üí All attributes must match\n",
    "- Most General: ‚ü®?, ?, ?, ?, ?, ?‚ü© ‚Üí All instances positive\n",
    "- Most Specific: ‚ü®‚àÖ, ‚àÖ, ‚àÖ, ‚àÖ, ‚àÖ, ‚àÖ‚ü© ‚Üí All instances negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis representation and matching\n",
    "print_section_header(\"Hypothesis Representation\", \"üìù\")\n",
    "\n",
    "class Hypothesis:\n",
    "    \"\"\"Represents a hypothesis as a list of attribute constraints\"\"\"\n",
    "    \n",
    "    def __init__(self, constraints):\n",
    "        \"\"\"\n",
    "        Initialize hypothesis with constraints.\n",
    "        \n",
    "        Args:\n",
    "            constraints: List of constraints, one per attribute\n",
    "                        '?' = any value\n",
    "                        '‚àÖ' = no value (empty set)\n",
    "                        specific value = must match exactly\n",
    "        \"\"\"\n",
    "        self.constraints = constraints\n",
    "    \n",
    "    def matches(self, example):\n",
    "        \"\"\"\n",
    "        Check if hypothesis matches (classifies as positive) an example.\n",
    "        \n",
    "        Args:\n",
    "            example: Dictionary with attribute values\n",
    "        \n",
    "        Returns:\n",
    "            True if hypothesis matches, False otherwise\n",
    "        \"\"\"\n",
    "        attr_names = ['Sky', 'AirTemp', 'Humidity', 'Wind', 'Water', 'Forecast']\n",
    "        \n",
    "        for i, attr_name in enumerate(attr_names):\n",
    "            constraint = self.constraints[i]\n",
    "            \n",
    "            # Empty set constraint - never matches\n",
    "            if constraint == '‚àÖ':\n",
    "                return False\n",
    "            \n",
    "            # '?' matches any value\n",
    "            if constraint == '?':\n",
    "                continue\n",
    "            \n",
    "            # Specific value must match exactly\n",
    "            if constraint != example[attr_name]:\n",
    "                return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def is_more_general_than(self, other):\n",
    "        \"\"\"\n",
    "        Check if this hypothesis is more general than another.\n",
    "        h1 >= h2 if h1 classifies all instances that h2 classifies as positive.\n",
    "        \"\"\"\n",
    "        for i in range(len(self.constraints)):\n",
    "            c1, c2 = self.constraints[i], other.constraints[i]\n",
    "            \n",
    "            # If c1 is specific and c2 is '?', c1 is not more general\n",
    "            if c1 != '?' and c2 == '?':\n",
    "                return False\n",
    "            \n",
    "            # If c1 is '‚àÖ' and c2 is not, c1 is not more general\n",
    "            if c1 == '‚àÖ' and c2 != '‚àÖ':\n",
    "                return False\n",
    "            \n",
    "            # If c1 and c2 are both specific but different, c1 is not more general\n",
    "            if c1 != '?' and c2 != '?' and c1 != '‚àÖ' and c2 != '‚àÖ' and c1 != c2:\n",
    "                return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def __str__(self):\n",
    "        return '‚ü®' + ', '.join(self.constraints) + '‚ü©'\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "# Example hypotheses\n",
    "h1 = Hypothesis(['Sunny', '?', '?', 'Strong', '?', '?'])\n",
    "h2 = Hypothesis(['Sunny', 'Warm', '?', 'Strong', '?', '?'])\n",
    "h3 = Hypothesis(['?', '?', '?', '?', '?', '?'])  # Most general\n",
    "h4 = Hypothesis(['‚àÖ', '‚àÖ', '‚àÖ', '‚àÖ', '‚àÖ', '‚àÖ'])  # Most specific\n",
    "\n",
    "print(\"üìã Example Hypotheses:\\n\")\n",
    "print(f\"h1 = {h1}\")\n",
    "print(f\"   Interpretation: Sky=Sunny AND Wind=Strong (other attributes don't matter)\\n\")\n",
    "\n",
    "print(f\"h2 = {h2}\")\n",
    "print(f\"   Interpretation: Sky=Sunny AND AirTemp=Warm AND Wind=Strong\\n\")\n",
    "\n",
    "print(f\"h3 = {h3}\")\n",
    "print(f\"   Interpretation: Most General Hypothesis (all instances positive)\\n\")\n",
    "\n",
    "print(f\"h4 = {h4}\")\n",
    "print(f\"   Interpretation: Most Specific Hypothesis (all instances negative)\\n\")\n",
    "\n",
    "# Test hypothesis matching\n",
    "test_example = train_enjoysport.iloc[0].to_dict()\n",
    "\n",
    "print(f\"\\nüß™ Testing Hypotheses on Example 1:\")\n",
    "print(f\"   Example: Sky={test_example['Sky']}, AirTemp={test_example['AirTemp']}, \"\n",
    "      f\"Humidity={test_example['Humidity']}, Wind={test_example['Wind']}, \"\n",
    "      f\"Water={test_example['Water']}, Forecast={test_example['Forecast']}\")\n",
    "print(f\"   Actual Label: {test_example['EnjoySport']}\\n\")\n",
    "\n",
    "print(f\"   h1 {h1} matches? {h1.matches(test_example)}\")\n",
    "print(f\"   h2 {h2} matches? {h2.matches(test_example)}\")\n",
    "print(f\"   h3 {h3} matches? {h3.matches(test_example)}\")\n",
    "print(f\"   h4 {h4} matches? {h4.matches(test_example)}\")\n",
    "\n",
    "# Test general-to-specific ordering\n",
    "print(f\"\\nüìä General-to-Specific Ordering:\\n\")\n",
    "print(f\"   h3 {h3} is more general than h1 {h1}? {h3.is_more_general_than(h1)}\")\n",
    "print(f\"   h1 {h1} is more general than h2 {h2}? {h1.is_more_general_than(h2)}\")\n",
    "print(f\"   h2 {h2} is more general than h4 {h4}? {h2.is_more_general_than(h4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: The Find-S Algorithm\n",
    "\n",
    "## üîç Slide 13-14: Find-S Algorithm\n",
    "\n",
    "### Finding the Maximally Specific Hypothesis\n",
    "\n",
    "**Algorithm Goal:** Find the most specific hypothesis that fits all positive examples\n",
    "\n",
    "**The Find-S Algorithm:**\n",
    "```\n",
    "1. Initialize h to the most specific hypothesis ‚ü®‚àÖ, ‚àÖ, ‚àÖ, ‚àÖ, ‚àÖ, ‚àÖ‚ü©\n",
    "\n",
    "2. For each positive training example x:\n",
    "   - For each attribute constraint ai in h:\n",
    "       If constraint ai is satisfied by x:\n",
    "           Do nothing\n",
    "       Else:\n",
    "           Replace ai in h by the next more general constraint\n",
    "           that is satisfied by x\n",
    "\n",
    "3. Output hypothesis h\n",
    "```\n",
    "\n",
    "**Key Properties:**\n",
    "- Only considers positive examples\n",
    "- Moves from specific to general\n",
    "- Guaranteed to find maximally specific hypothesis\n",
    "- Ignores negative examples (limitation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Find-S Algorithm\n",
    "print_section_header(\"Find-S Algorithm Implementation\", \"üîç\")\n",
    "\n",
    "def find_s_algorithm(training_data, attribute_names):\n",
    "    \"\"\"\n",
    "    Find-S algorithm: Find the maximally specific hypothesis.\n",
    "    \n",
    "    Args:\n",
    "        training_data: List of training examples (dictionaries)\n",
    "        attribute_names: List of attribute names\n",
    "    \n",
    "    Returns:\n",
    "        Hypothesis object representing the learned hypothesis\n",
    "        List of hypothesis evolution steps\n",
    "    \"\"\"\n",
    "    # Initialize to most specific hypothesis\n",
    "    h = ['‚àÖ'] * len(attribute_names)\n",
    "    history = []\n",
    "    \n",
    "    print(\"üöÄ Starting Find-S Algorithm...\\n\")\n",
    "    print(f\"Initial hypothesis h0 = ‚ü®{', '.join(h)}‚ü©\\n\")\n",
    "    history.append({\n",
    "        'step': 0,\n",
    "        'example': 'Initial',\n",
    "        'hypothesis': h.copy(),\n",
    "        'action': 'Initialize to most specific'\n",
    "    })\n",
    "    \n",
    "    step = 1\n",
    "    for example in training_data:\n",
    "        # Only process positive examples\n",
    "        if example['EnjoySport'] == 'Yes':\n",
    "            print(f\"üìù Processing Example {example['example_id']} (Positive):\")\n",
    "            print(f\"   {', '.join([f'{attr}={example[attr]}' for attr in attribute_names])}\")\n",
    "            \n",
    "            changes = []\n",
    "            for i, attr in enumerate(attribute_names):\n",
    "                # If constraint is '‚àÖ', replace with example value\n",
    "                if h[i] == '‚àÖ':\n",
    "                    h[i] = example[attr]\n",
    "                    changes.append(f\"{attr}: ‚àÖ ‚Üí {example[attr]}\")\n",
    "                # If constraint doesn't match, generalize to '?'\n",
    "                elif h[i] != example[attr]:\n",
    "                    old_val = h[i]\n",
    "                    h[i] = '?'\n",
    "                    changes.append(f\"{attr}: {old_val} ‚Üí ?\")\n",
    "            \n",
    "            if changes:\n",
    "                print(f\"   Changes: {', '.join(changes)}\")\n",
    "            else:\n",
    "                print(f\"   No changes (hypothesis already covers this example)\")\n",
    "            \n",
    "            print(f\"   h{step} = ‚ü®{', '.join(h)}‚ü©\\n\")\n",
    "            \n",
    "            history.append({\n",
    "                'step': step,\n",
    "                'example': f\"Example {example['example_id']} (+)\",\n",
    "                'hypothesis': h.copy(),\n",
    "                'action': ', '.join(changes) if changes else 'No change'\n",
    "            })\n",
    "            step += 1\n",
    "        else:\n",
    "            print(f\"‚è≠Ô∏è  Skipping Example {example['example_id']} (Negative) - Find-S ignores negatives\\n\")\n",
    "    \n",
    "    return Hypothesis(h), history\n",
    "\n",
    "# Run Find-S on EnjoySport data\n",
    "attr_names = ['Sky', 'AirTemp', 'Humidity', 'Wind', 'Water', 'Forecast']\n",
    "final_hypothesis, evolution = find_s_algorithm(enjoysport_data['training_data'], attr_names)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"‚úÖ Final Hypothesis: {final_hypothesis}\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nüìä Interpretation:\")\n",
    "for i, attr in enumerate(attr_names):\n",
    "    constraint = final_hypothesis.constraints[i]\n",
    "    if constraint == '?':\n",
    "        print(f\"   ‚Ä¢ {attr}: Any value (doesn't matter)\")\n",
    "    elif constraint == '‚àÖ':\n",
    "        print(f\"   ‚Ä¢ {attr}: No value (impossible)\")\n",
    "    else:\n",
    "        print(f\"   ‚Ä¢ {attr}: Must be '{constraint}'\")\n",
    "\n",
    "# Visualize hypothesis evolution\n",
    "evolution_df = pd.DataFrame(evolution)\n",
    "print(\"\\nüìà Hypothesis Evolution:\\n\")\n",
    "for _, row in evolution_df.iterrows():\n",
    "    h_str = '‚ü®' + ', '.join(row['hypothesis']) + '‚ü©'\n",
    "    print(f\"Step {row['step']}: {row['example']:20s} ‚Üí {h_str:40s} | {row['action']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the learned hypothesis\n",
    "print_section_header(\"Testing Find-S Hypothesis\", \"üß™\")\n",
    "\n",
    "print(f\"Learned Hypothesis: {final_hypothesis}\\n\")\n",
    "\n",
    "# Test on training data\n",
    "print(\"üìä Performance on Training Data:\\n\")\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for example in enjoysport_data['training_data']:\n",
    "    prediction = 'Yes' if final_hypothesis.matches(example) else 'No'\n",
    "    actual = example['EnjoySport']\n",
    "    match = '‚úì' if prediction == actual else '‚úó'\n",
    "    \n",
    "    print(f\"Example {example['example_id']}: Predicted={prediction:3s}, Actual={actual:3s} {match}\")\n",
    "    \n",
    "    if prediction == actual:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "accuracy = (correct / total) * 100\n",
    "print(f\"\\nüìà Training Accuracy: {correct}/{total} = {accuracy:.1f}%\")\n",
    "\n",
    "# Test on test data\n",
    "print(\"\\nüîÆ Predictions on Test Data:\\n\")\n",
    "for example in enjoysport_data['test_data']:\n",
    "    prediction = 'Yes' if final_hypothesis.matches(example) else 'No'\n",
    "    \n",
    "    print(f\"Example {example['example_id']}:\")\n",
    "    print(f\"   {', '.join([f'{attr}={example[attr]}' for attr in attr_names])}\")\n",
    "    print(f\"   Prediction: EnjoySport = {prediction}\\n\")\n",
    "\n",
    "# Visualize predictions\n",
    "train_results = []\n",
    "for example in enjoysport_data['training_data']:\n",
    "    prediction = 'Yes' if final_hypothesis.matches(example) else 'No'\n",
    "    actual = example['EnjoySport']\n",
    "    train_results.append({\n",
    "        'Example': example['example_id'],\n",
    "        'Predicted': prediction,\n",
    "        'Actual': actual,\n",
    "        'Correct': prediction == actual\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(train_results)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(results_df['Actual'], results_df['Predicted'], labels=['No', 'Yes'])\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])\n",
    "axes[0].set_xlabel('Predicted', fontsize=12)\n",
    "axes[0].set_ylabel('Actual', fontsize=12)\n",
    "axes[0].set_title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Accuracy visualization\n",
    "correct_count = results_df['Correct'].sum()\n",
    "incorrect_count = len(results_df) - correct_count\n",
    "\n",
    "axes[1].bar(['Correct', 'Incorrect'], [correct_count, incorrect_count],\n",
    "            color=['#2ecc71', '#e74c3c'], alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[1].set_ylabel('Count', fontsize=12)\n",
    "axes[1].set_title(f'Find-S Performance: {accuracy:.1f}% Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "for i, v in enumerate([correct_count, incorrect_count]):\n",
    "    axes[1].text(i, v + 0.1, str(v), ha='center', va='bottom', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  Limitations of Find-S:\")\n",
    "print(\"   1. Ignores negative examples (can't detect when hypothesis is too general)\")\n",
    "print(\"   2. No way to know if we've converged to the correct concept\")\n",
    "print(\"   3. Can't handle inconsistent data (noise/errors)\")\n",
    "print(\"   4. Only finds ONE hypothesis (what if there are multiple valid ones?)\")\n",
    "print(\"\\nüí° Solution: Version Spaces and Candidate-Elimination Algorithm!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 6: Version Spaces and Candidate-Elimination\n",
    "\n",
    "## üéØ Slide 15-16: Version Spaces\n",
    "\n",
    "### The Power of Maintaining ALL Consistent Hypotheses\n",
    "\n",
    "**Version Space Definition:**\n",
    "The subset of all hypotheses from H that are consistent with the observed training examples D.\n",
    "\n",
    "**Mathematical Notation:**\n",
    "```\n",
    "VS(H,D) = {h ‚àà H | Consistent(h, D)}\n",
    "```\n",
    "\n",
    "**Key Insight:** Instead of listing all consistent hypotheses (could be thousands!), we represent the version space by:\n",
    "- **S (Specific boundary):** Most specific consistent hypotheses\n",
    "- **G (General boundary):** Most general consistent hypotheses\n",
    "\n",
    "**The version space is completely characterized by S and G!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Candidate-Elimination Algorithm\n",
    "print_section_header(\"Candidate-Elimination Algorithm\", \"üéØ\")\n",
    "\n",
    "class CandidateElimination:\n",
    "    \"\"\"Candidate-Elimination algorithm for concept learning\"\"\"\n",
    "    \n",
    "    def __init__(self, attribute_names, attribute_values):\n",
    "        \"\"\"\n",
    "        Initialize with most general and most specific hypotheses.\n",
    "        \n",
    "        Args:\n",
    "            attribute_names: List of attribute names\n",
    "            attribute_values: Dictionary mapping attribute names to possible values\n",
    "        \"\"\"\n",
    "        self.attribute_names = attribute_names\n",
    "        self.attribute_values = attribute_values\n",
    "        self.num_attributes = len(attribute_names)\n",
    "        \n",
    "        # Initialize S to most specific hypothesis\n",
    "        self.S = [Hypothesis(['‚àÖ'] * self.num_attributes)]\n",
    "        \n",
    "        # Initialize G to most general hypothesis\n",
    "        self.G = [Hypothesis(['?'] * self.num_attributes)]\n",
    "        \n",
    "        self.history = []\n",
    "    \n",
    "    def generalize_hypothesis(self, h, example):\n",
    "        \"\"\"\n",
    "        Minimally generalize hypothesis h to cover example.\n",
    "        \"\"\"\n",
    "        new_constraints = []\n",
    "        for i, attr in enumerate(self.attribute_names):\n",
    "            if h.constraints[i] == '‚àÖ':\n",
    "                new_constraints.append(example[attr])\n",
    "            elif h.constraints[i] != example[attr]:\n",
    "                new_constraints.append('?')\n",
    "            else:\n",
    "                new_constraints.append(h.constraints[i])\n",
    "        return Hypothesis(new_constraints)\n",
    "    \n",
    "    def specialize_hypothesis(self, h, example):\n",
    "        \"\"\"\n",
    "        Minimally specialize hypothesis h to exclude example.\n",
    "        Returns list of specialized hypotheses.\n",
    "        \"\"\"\n",
    "        specialized = []\n",
    "        \n",
    "        for i, attr in enumerate(self.attribute_names):\n",
    "            if h.constraints[i] == '?':\n",
    "                # Try all possible values except the one in the example\n",
    "                for value in self.attribute_values[attr]:\n",
    "                    if value != example[attr]:\n",
    "                        new_constraints = h.constraints.copy()\n",
    "                        new_constraints[i] = value\n",
    "                        specialized.append(Hypothesis(new_constraints))\n",
    "        \n",
    "        return specialized\n",
    "    \n",
    "    def train(self, training_data):\n",
    "        \"\"\"\n",
    "        Train using Candidate-Elimination algorithm.\n",
    "        \"\"\"\n",
    "        print(\"üöÄ Starting Candidate-Elimination Algorithm...\\n\")\n",
    "        print(f\"Initial State:\")\n",
    "        print(f\"  S = {{{', '.join([str(h) for h in self.S])}}}\")\n",
    "        print(f\"  G = {{{', '.join([str(h) for h in self.G])}}}\\n\")\n",
    "        \n",
    "        self.history.append({\n",
    "            'step': 0,\n",
    "            'example': 'Initial',\n",
    "            'S': [h.constraints.copy() for h in self.S],\n",
    "            'G': [h.constraints.copy() for h in self.G]\n",
    "        })\n",
    "        \n",
    "        for idx, example in enumerate(training_data):\n",
    "            step = idx + 1\n",
    "            label = example['EnjoySport']\n",
    "            \n",
    "            print(f\"{'='*80}\")\n",
    "            print(f\"üìù Step {step}: Processing Example {example['example_id']} ({label})\")\n",
    "            print(f\"{'='*80}\")\n",
    "            print(f\"   {', '.join([f'{attr}={example[attr]}' for attr in self.attribute_names])}\\n\")\n",
    "            \n",
    "            if label == 'Yes':\n",
    "                # Positive example\n",
    "                print(\"   ‚úÖ Positive Example - Update S and G:\\n\")\n",
    "                \n",
    "                # Remove from G any hypothesis inconsistent with example\n",
    "                self.G = [g for g in self.G if g.matches(example)]\n",
    "                print(f\"   1. Remove from G hypotheses that don't match\")\n",
    "                \n",
    "                # Generalize S if needed\n",
    "                new_S = []\n",
    "                for s in self.S:\n",
    "                    if not s.matches(example):\n",
    "                        # Generalize s\n",
    "                        s_new = self.generalize_hypothesis(s, example)\n",
    "                        # Only keep if some member of G is more general\n",
    "                        if any(g.is_more_general_than(s_new) for g in self.G):\n",
    "                            new_S.append(s_new)\n",
    "                        print(f\"   2. Generalize S: {s} ‚Üí {s_new}\")\n",
    "                    else:\n",
    "                        new_S.append(s)\n",
    "                \n",
    "                # Remove from S any hypothesis more general than another in S\n",
    "                self.S = []\n",
    "                for s in new_S:\n",
    "                    if not any(s != s2 and s.is_more_general_than(s2) for s2 in new_S):\n",
    "                        self.S.append(s)\n",
    "                \n",
    "            else:\n",
    "                # Negative example\n",
    "                print(\"   ‚ùå Negative Example - Update G and S:\\n\")\n",
    "                \n",
    "                # Remove from S any hypothesis inconsistent with example\n",
    "                self.S = [s for s in self.S if not s.matches(example)]\n",
    "                print(f\"   1. Remove from S hypotheses that match (should be none)\")\n",
    "                \n",
    "                # Specialize G if needed\n",
    "                new_G = []\n",
    "                for g in self.G:\n",
    "                    if g.matches(example):\n",
    "                        # Specialize g\n",
    "                        specialized = self.specialize_hypothesis(g, example)\n",
    "                        print(f\"   2. Specialize G: {g} ‚Üí\")\n",
    "                        for spec in specialized:\n",
    "                            # Only keep if some member of S is more specific\n",
    "                            if any(spec.is_more_general_than(s) for s in self.S):\n",
    "                                new_G.append(spec)\n",
    "                                print(f\"      {spec}\")\n",
    "                    else:\n",
    "                        new_G.append(g)\n",
    "                \n",
    "                # Remove from G any hypothesis less general than another in G\n",
    "                self.G = []\n",
    "                for g in new_G:\n",
    "                    if not any(g != g2 and g2.is_more_general_than(g) for g2 in new_G):\n",
    "                        self.G.append(g)\n",
    "            \n",
    "            print(f\"\\n   Updated Version Space:\")\n",
    "            print(f\"   S = {{{', '.join([str(h) for h in self.S])}}}\")\n",
    "            print(f\"   G = {{{', '.join([str(h) for h in self.G])}}}\\n\")\n",
    "            \n",
    "            self.history.append({\n",
    "                'step': step,\n",
    "                'example': f\"Example {example['example_id']} ({label})\",\n",
    "                'S': [h.constraints.copy() for h in self.S],\n",
    "                'G': [h.constraints.copy() for h in self.G]\n",
    "            })\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "        print(\"‚úÖ Candidate-Elimination Complete!\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"\\nFinal Version Space:\")\n",
    "        print(f\"  S (Specific Boundary) = {{{', '.join([str(h) for h in self.S])}}}\")\n",
    "        print(f\"  G (General Boundary) = {{{', '.join([str(h) for h in self.G])}}}\")\n",
    "\n",
    "# Run Candidate-Elimination\n",
    "ce = CandidateElimination(attr_names, enjoysport_data['attributes'])\n",
    "ce.train(enjoysport_data['training_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Version Space Evolution\n",
    "print_section_header(\"Version Space Evolution Visualization\", \"üìä\")\n",
    "\n",
    "# Create visualization of S and G boundaries over time\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Plot S boundary evolution\n",
    "ax1 = axes[0]\n",
    "steps = [h['step'] for h in ce.history]\n",
    "s_sizes = [len(h['S']) for h in ce.history]\n",
    "\n",
    "ax1.plot(steps, s_sizes, marker='o', linewidth=2.5, markersize=10, \n",
    "         color='#2ecc71', label='S Boundary Size')\n",
    "ax1.set_xlabel('Training Step', fontsize=12)\n",
    "ax1.set_ylabel('Number of Hypotheses in S', fontsize=12)\n",
    "ax1.set_title('S Boundary Evolution', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend(fontsize=11)\n",
    "\n",
    "# Annotate key points\n",
    "for i, (step, size) in enumerate(zip(steps, s_sizes)):\n",
    "    if i == 0 or i == len(steps) - 1 or (i > 0 and size != s_sizes[i-1]):\n",
    "        ax1.annotate(f'{size}', xy=(step, size), xytext=(5, 5), \n",
    "                    textcoords='offset points', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Plot G boundary evolution\n",
    "ax2 = axes[1]\n",
    "g_sizes = [len(h['G']) for h in ce.history]\n",
    "\n",
    "ax2.plot(steps, g_sizes, marker='s', linewidth=2.5, markersize=10, \n",
    "         color='#e74c3c', label='G Boundary Size')\n",
    "ax2.set_xlabel('Training Step', fontsize=12)\n",
    "ax2.set_ylabel('Number of Hypotheses in G', fontsize=12)\n",
    "ax2.set_title('G Boundary Evolution', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend(fontsize=11)\n",
    "\n",
    "# Annotate key points\n",
    "for i, (step, size) in enumerate(zip(steps, g_sizes)):\n",
    "    if i == 0 or i == len(steps) - 1 or (i > 0 and size != g_sizes[i-1]):\n",
    "        ax2.annotate(f'{size}', xy=(step, size), xytext=(5, 5), \n",
    "                    textcoords='offset points', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display evolution table\n",
    "print(\"\\nüìã Version Space Evolution Summary:\\n\")\n",
    "evolution_summary = []\n",
    "for h in ce.history:\n",
    "    s_str = ', '.join(['‚ü®' + ', '.join(s) + '‚ü©' for s in h['S']])\n",
    "    g_str = ', '.join(['‚ü®' + ', '.join(g) + '‚ü©' for g in h['G']])\n",
    "    evolution_summary.append({\n",
    "        'Step': h['step'],\n",
    "        'Example': h['example'],\n",
    "        '|S|': len(h['S']),\n",
    "        '|G|': len(h['G']),\n",
    "        'S': s_str[:50] + '...' if len(s_str) > 50 else s_str,\n",
    "        'G': g_str[:50] + '...' if len(g_str) > 50 else g_str\n",
    "    })\n",
    "\n",
    "evolution_df = pd.DataFrame(evolution_summary)\n",
    "print(evolution_df[['Step', 'Example', '|S|', '|G|']].to_string(index=False))\n",
    "\n",
    "print(\"\\nüí° Key Observations:\")\n",
    "print(\"   ‚Ä¢ S boundary starts specific and becomes more general\")\n",
    "print(\"   ‚Ä¢ G boundary starts general and becomes more specific\")\n",
    "print(\"   ‚Ä¢ Version space size = all hypotheses between S and G\")\n",
    "print(\"   ‚Ä¢ Converges when S = G (single hypothesis remains)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Candidate-Elimination predictions\n",
    "print_section_header(\"Testing Candidate-Elimination\", \"üß™\")\n",
    "\n",
    "def classify_with_version_space(example, S, G):\n",
    "    \"\"\"\n",
    "    Classify an example using version space.\n",
    "    Returns: 'Yes', 'No', or 'Unknown'\n",
    "    \"\"\"\n",
    "    # Check if all hypotheses in S match\n",
    "    s_matches = [s.matches(example) for s in S]\n",
    "    \n",
    "    # Check if all hypotheses in G match\n",
    "    g_matches = [g.matches(example) for g in G]\n",
    "    \n",
    "    if all(s_matches) and all(g_matches):\n",
    "        return 'Yes'\n",
    "    elif not any(g_matches):\n",
    "        return 'No'\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "print(f\"Final Version Space:\")\n",
    "print(f\"  S = {{{', '.join([str(h) for h in ce.S])}}}\")\n",
    "print(f\"  G = {{{', '.join([str(h) for h in ce.G])}}}\\n\")\n",
    "\n",
    "# Test on training data\n",
    "print(\"üìä Performance on Training Data:\\n\")\n",
    "train_correct = 0\n",
    "train_total = 0\n",
    "train_unknown = 0\n",
    "\n",
    "for example in enjoysport_data['training_data']:\n",
    "    prediction = classify_with_version_space(example, ce.S, ce.G)\n",
    "    actual = example['EnjoySport']\n",
    "    \n",
    "    if prediction == 'Unknown':\n",
    "        match = '?'\n",
    "        train_unknown += 1\n",
    "    elif prediction == actual:\n",
    "        match = '‚úì'\n",
    "        train_correct += 1\n",
    "    else:\n",
    "        match = '‚úó'\n",
    "    \n",
    "    print(f\"Example {example['example_id']}: Predicted={prediction:7s}, Actual={actual:3s} {match}\")\n",
    "    train_total += 1\n",
    "\n",
    "train_accuracy = (train_correct / train_total) * 100\n",
    "print(f\"\\nüìà Training Accuracy: {train_correct}/{train_total} = {train_accuracy:.1f}%\")\n",
    "print(f\"   Unknown predictions: {train_unknown}\")\n",
    "\n",
    "# Test on test data\n",
    "print(\"\\nüîÆ Predictions on Test Data:\\n\")\n",
    "test_predictions = []\n",
    "\n",
    "for example in enjoysport_data['test_data']:\n",
    "    prediction = classify_with_version_space(example, ce.S, ce.G)\n",
    "    \n",
    "    print(f\"Example {example['example_id']}:\")\n",
    "    print(f\"   {', '.join([f'{attr}={example[attr]}' for attr in attr_names])}\")\n",
    "    print(f\"   Prediction: EnjoySport = {prediction}\")\n",
    "    \n",
    "    # Show which hypotheses match\n",
    "    s_match = [str(s) for s in ce.S if s.matches(example)]\n",
    "    g_match = [str(g) for g in ce.G if g.matches(example)]\n",
    "    \n",
    "    print(f\"   S hypotheses that match: {s_match if s_match else 'None'}\")\n",
    "    print(f\"   G hypotheses that match: {g_match if g_match else 'None'}\\n\")\n",
    "    \n",
    "    test_predictions.append({\n",
    "        'Example': example['example_id'],\n",
    "        'Prediction': prediction,\n",
    "        'S_matches': len(s_match),\n",
    "        'G_matches': len(g_match)\n",
    "    })\n",
    "\n",
    "# Visualize comparison: Find-S vs Candidate-Elimination\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Find-S accuracy\n",
    "finds_acc = accuracy  # From previous Find-S test\n",
    "ce_acc = train_accuracy\n",
    "\n",
    "algorithms = ['Find-S', 'Candidate-\\nElimination']\n",
    "accuracies = [finds_acc, ce_acc]\n",
    "colors_alg = ['#3498db', '#2ecc71']\n",
    "\n",
    "axes[0].bar(algorithms, accuracies, color=colors_alg, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[0].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "axes[0].set_title('Algorithm Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylim(0, 110)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "for i, v in enumerate(accuracies):\n",
    "    axes[0].text(i, v + 2, f'{v:.1f}%', ha='center', va='bottom', \n",
    "                fontsize=12, fontweight='bold')\n",
    "\n",
    "# Version space size over time\n",
    "axes[1].plot(steps, s_sizes, marker='o', linewidth=2, markersize=8, \n",
    "             color='#2ecc71', label='|S|')\n",
    "axes[1].plot(steps, g_sizes, marker='s', linewidth=2, markersize=8, \n",
    "             color='#e74c3c', label='|G|')\n",
    "axes[1].set_xlabel('Training Step', fontsize=12)\n",
    "axes[1].set_ylabel('Boundary Size', fontsize=12)\n",
    "axes[1].set_title('Version Space Convergence', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Advantages of Candidate-Elimination over Find-S:\")\n",
    "print(\"   1. Uses both positive AND negative examples\")\n",
    "print(\"   2. Maintains ALL consistent hypotheses (via S and G boundaries)\")\n",
    "print(\"   3. Can detect when more data is needed (Unknown predictions)\")\n",
    "print(\"   4. Can detect inconsistent training data\")\n",
    "print(\"   5. Provides confidence in predictions based on version space\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 7: Inductive Bias\n",
    "\n",
    "## üß¨ Slide 19: Inductive Bias - The Necessity of Assumptions\n",
    "\n",
    "### Why Learning Requires Bias\n",
    "\n",
    "**The Fundamental Question:** Can an unbiased learner generalize beyond training data?\n",
    "\n",
    "**Answer:** **NO!** An unbiased learner cannot make inductive leaps.\n",
    "\n",
    "**Inductive Bias Definition:**\n",
    "The set of assumptions a learner uses to predict outputs for inputs it has not encountered.\n",
    "\n",
    "**Candidate-Elimination Inductive Bias:**\n",
    "*\"The target concept can be represented as a conjunction of attribute constraints.\"*\n",
    "\n",
    "**Key Insight:**\n",
    "- Stronger bias ‚Üí more assumptions ‚Üí better generalization (if assumptions correct)\n",
    "- Weaker bias ‚Üí fewer assumptions ‚Üí less generalization ability\n",
    "- No bias ‚Üí no generalization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the necessity of inductive bias\n",
    "print_section_header(\"Inductive Bias Demonstration\", \"üß¨\")\n",
    "\n",
    "print(\"ü§î The Futility of Bias-Free Learning\\n\")\n",
    "\n",
    "print(\"Scenario: Unbiased Learner with Power Set Hypothesis Space\\n\")\n",
    "print(\"Given 3 training examples:\")\n",
    "print(\"  1. ‚ü®Sunny, Warm, Normal, Strong, Warm, Same‚ü© ‚Üí Yes\")\n",
    "print(\"  2. ‚ü®Sunny, Warm, High, Strong, Warm, Same‚ü© ‚Üí Yes\")\n",
    "print(\"  3. ‚ü®Rainy, Cold, High, Strong, Cool, Change‚ü© ‚Üí No\\n\")\n",
    "\n",
    "print(\"Question: Will ‚ü®Sunny, Warm, Normal, Weak, Warm, Same‚ü© be positive?\\n\")\n",
    "\n",
    "print(\"With Power Set Hypothesis Space (no bias):\")\n",
    "print(\"  ‚Ä¢ Hypothesis h1: Only examples 1 and 2 are positive ‚Üí Predicts: No\")\n",
    "print(\"  ‚Ä¢ Hypothesis h2: All examples with Sunny are positive ‚Üí Predicts: Yes\")\n",
    "print(\"  ‚Ä¢ Hypothesis h3: All examples except 3 are positive ‚Üí Predicts: Yes\")\n",
    "print(\"  ‚Ä¢ ... infinitely many hypotheses ...\\n\")\n",
    "\n",
    "print(\"Result: Exactly HALF of consistent hypotheses say Yes, half say No!\")\n",
    "print(\"        ‚Üí Cannot make a confident prediction!\\n\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"With Candidate-Elimination Bias (conjunctive hypotheses):\")\n",
    "print(\"  ‚Ä¢ Restricts to hypotheses like ‚ü®Sunny, Warm, ?, Strong, ?, ?‚ü©\")\n",
    "print(\"  ‚Ä¢ Can make confident predictions on new examples\")\n",
    "print(\"  ‚Ä¢ Trade-off: Might miss concepts that aren't conjunctive\\n\")\n",
    "\n",
    "# Visualize different types of bias\n",
    "bias_types = [\n",
    "    {\n",
    "        'Algorithm': 'Candidate-Elimination',\n",
    "        'Bias': 'Target is conjunctive concept',\n",
    "        'Strength': 'Strong',\n",
    "        'Generalization': 'Good (if assumption holds)',\n",
    "        'Expressiveness': 'Limited'\n",
    "    },\n",
    "    {\n",
    "        'Algorithm': 'Decision Trees',\n",
    "        'Bias': 'Prefer shorter trees',\n",
    "        'Strength': 'Medium',\n",
    "        'Generalization': 'Good',\n",
    "        'Expressiveness': 'High'\n",
    "    },\n",
    "    {\n",
    "        'Algorithm': 'Neural Networks',\n",
    "        'Bias': 'Smooth decision boundaries',\n",
    "        'Strength': 'Weak',\n",
    "        'Generalization': 'Depends on architecture',\n",
    "        'Expressiveness': 'Very High'\n",
    "    },\n",
    "    {\n",
    "        'Algorithm': 'Unbiased Learner',\n",
    "        'Bias': 'None (power set)',\n",
    "        'Strength': 'None',\n",
    "        'Generalization': 'Impossible',\n",
    "        'Expressiveness': 'Complete'\n",
    "    }\n",
    "]\n",
    "\n",
    "bias_df = pd.DataFrame(bias_types)\n",
    "print(\"\\nüìä Comparison of Inductive Biases:\\n\")\n",
    "print(bias_df.to_string(index=False))\n",
    "\n",
    "# Visualize bias-generalization trade-off\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "algorithms = ['Unbiased\\nLearner', 'Neural\\nNetworks', 'Decision\\nTrees', 'Candidate-\\nElimination']\n",
    "bias_strength = [0, 3, 6, 9]  # Arbitrary scale\n",
    "generalization = [0, 6, 7, 8]  # Arbitrary scale\n",
    "expressiveness = [10, 9, 7, 4]  # Arbitrary scale\n",
    "\n",
    "x = np.arange(len(algorithms))\n",
    "width = 0.25\n",
    "\n",
    "ax.bar(x - width, bias_strength, width, label='Bias Strength', color='#3498db', alpha=0.7)\n",
    "ax.bar(x, generalization, width, label='Generalization Ability', color='#2ecc71', alpha=0.7)\n",
    "ax.bar(x + width, expressiveness, width, label='Expressiveness', color='#e74c3c', alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Learning Algorithm', fontsize=12)\n",
    "ax.set_ylabel('Relative Strength (0-10)', fontsize=12)\n",
    "ax.set_title('Inductive Bias Trade-offs', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(algorithms)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Takeaways on Inductive Bias:\")\n",
    "print(\"   1. All learning algorithms have inductive bias (assumptions)\")\n",
    "print(\"   2. Bias is NECESSARY for generalization\")\n",
    "print(\"   3. Stronger bias ‚Üí better generalization (if assumptions are correct)\")\n",
    "print(\"   4. Weaker bias ‚Üí more expressive but needs more data\")\n",
    "print(\"   5. Choose bias based on domain knowledge and data availability\")\n",
    "print(\"\\n‚ö†Ô∏è  The No Free Lunch Theorem:\")\n",
    "print(\"   No single learning algorithm is best for all problems!\")\n",
    "print(\"   The right bias depends on the problem domain.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 8: Self-Assessment and Practice\n",
    "\n",
    "## üìù Self-Assessment Questions\n",
    "\n",
    "Test your understanding of the concepts covered in this module!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive self-assessment\n",
    "print_section_header(\"Self-Assessment Questions\", \"üìù\")\n",
    "\n",
    "questions = [\n",
    "    {\n",
    "        'id': 1,\n",
    "        'type': 'Multiple Choice',\n",
    "        'question': 'Which component determines \"how we measure success\" in a learning problem?',\n",
    "        'options': ['A) Task', 'B) Performance Measure', 'C) Experience', 'D) Hypothesis'],\n",
    "        'answer': 'B',\n",
    "        'explanation': 'Performance Measure (P) defines how we quantify success (e.g., accuracy %, error rate).'\n",
    "    },\n",
    "    {\n",
    "        'id': 2,\n",
    "        'type': 'Multiple Choice',\n",
    "        'question': 'What is the main limitation of the Find-S algorithm?',\n",
    "        'options': ['A) Too slow', 'B) Ignores negative examples', 'C) Requires too much memory', 'D) Cannot handle numeric data'],\n",
    "        'answer': 'B',\n",
    "        'explanation': 'Find-S only uses positive examples to generalize, ignoring negative examples entirely.'\n",
    "    },\n",
    "    {\n",
    "        'id': 3,\n",
    "        'type': 'Multiple Choice',\n",
    "        'question': 'In the hypothesis ‚ü®Sunny, ?, ?, Strong, ?, ?‚ü©, what does \"?\" mean?',\n",
    "        'options': ['A) Unknown value', 'B) Any value acceptable', 'C) No value', 'D) Error'],\n",
    "        'answer': 'B',\n",
    "        'explanation': '\"?\" means the attribute can have any value - it doesn\\'t matter for classification.'\n",
    "    },\n",
    "    {\n",
    "        'id': 4,\n",
    "        'type': 'Multiple Choice',\n",
    "        'question': 'What does the version space represent?',\n",
    "        'options': ['A) All possible hypotheses', 'B) Only the most general hypothesis', \n",
    "                   'C) All hypotheses consistent with training data', 'D) The final learned hypothesis'],\n",
    "        'answer': 'C',\n",
    "        'explanation': 'Version space = all hypotheses from H that are consistent with observed training examples.'\n",
    "    },\n",
    "    {\n",
    "        'id': 5,\n",
    "        'type': 'Multiple Choice',\n",
    "        'question': 'Can the version space size increase as we see more training examples?',\n",
    "        'options': ['A) Yes, always', 'B) Yes, sometimes', 'C) No, it can only shrink or stay same', 'D) Depends on the data'],\n",
    "        'answer': 'C',\n",
    "        'explanation': 'Version space is monotonically decreasing - each example eliminates hypotheses, never adds them back.'\n",
    "    },\n",
    "    {\n",
    "        'id': 6,\n",
    "        'type': 'True/False',\n",
    "        'question': 'An unbiased learner (with no inductive bias) can generalize better than a biased learner.',\n",
    "        'answer': 'False',\n",
    "        'explanation': 'FALSE. An unbiased learner cannot generalize at all! Inductive bias is necessary for generalization.'\n",
    "    },\n",
    "    {\n",
    "        'id': 7,\n",
    "        'type': 'True/False',\n",
    "        'question': 'The Candidate-Elimination algorithm uses both positive and negative examples.',\n",
    "        'answer': 'True',\n",
    "        'explanation': 'TRUE. Positive examples generalize S, negative examples specialize G.'\n",
    "    },\n",
    "    {\n",
    "        'id': 8,\n",
    "        'type': 'True/False',\n",
    "        'question': 'The LMS algorithm is guaranteed to find the global minimum of the error function.',\n",
    "        'answer': 'True',\n",
    "        'explanation': 'TRUE for linear functions. The error surface is convex, so gradient descent finds the global minimum.'\n",
    "    },\n",
    "    {\n",
    "        'id': 9,\n",
    "        'type': 'Conceptual',\n",
    "        'question': 'Explain why inductive bias is necessary for machine learning.',\n",
    "        'answer': 'Inductive bias provides assumptions that allow a learner to generalize beyond training data. '\n",
    "                 'Without bias, a learner has no basis to prefer one hypothesis over another for unseen examples, '\n",
    "                 'making generalization impossible. The bias restricts the hypothesis space, enabling learning.'\n",
    "    },\n",
    "    {\n",
    "        'id': 10,\n",
    "        'type': 'Conceptual',\n",
    "        'question': 'What is the difference between the S and G boundaries in version space?',\n",
    "        'answer': 'S (Specific boundary) contains the most specific hypotheses consistent with training data. '\n",
    "                 'G (General boundary) contains the most general hypotheses consistent with training data. '\n",
    "                 'Together, they completely characterize the version space - all consistent hypotheses lie between S and G.'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Display questions\n",
    "for q in questions:\n",
    "    print(f\"\\nQuestion {q['id']} ({q['type']}):\")\n",
    "    print(f\"  {q['question']}\")\n",
    "    \n",
    "    if 'options' in q:\n",
    "        for opt in q['options']:\n",
    "            print(f\"    {opt}\")\n",
    "    \n",
    "    print(f\"\\n  ‚úÖ Answer: {q['answer']}\")\n",
    "    print(f\"  üí° Explanation: {q['explanation']}\")\n",
    "    print(\"  \" + \"-\"*70)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä How did you do? Review any concepts you found challenging!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 9: Summary and Key Takeaways\n",
    "\n",
    "## üéØ Module Summary\n",
    "\n",
    "Congratulations! You've completed Module 1: Introduction & Concept Learning. Let's review what we've learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary visualization\n",
    "print_section_header(\"Module 1 Summary\", \"üéØ\")\n",
    "\n",
    "print(\"üìö Key Concepts Covered:\\n\")\n",
    "\n",
    "concepts = [\n",
    "    {\n",
    "        'Topic': 'Well-Posed Learning Problems',\n",
    "        'Key Points': [\n",
    "            'Task (T): What we\\'re trying to do',\n",
    "            'Performance (P): How we measure success',\n",
    "            'Experience (E): What data we learn from',\n",
    "            'All three must be clearly defined'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'Topic': 'Designing Learning Systems',\n",
    "        'Key Points': [\n",
    "            '1. Choose training experience',\n",
    "            '2. Choose target function',\n",
    "            '3. Choose representation',\n",
    "            '4. Choose learning algorithm'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'Topic': 'Concept Learning',\n",
    "        'Key Points': [\n",
    "            'Learning boolean-valued functions',\n",
    "            'Hypothesis = conjunction of constraints',\n",
    "            'General-to-specific ordering',\n",
    "            'Search through hypothesis space'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'Topic': 'Find-S Algorithm',\n",
    "        'Key Points': [\n",
    "            'Finds maximally specific hypothesis',\n",
    "            'Only uses positive examples',\n",
    "            'Starts specific, generalizes as needed',\n",
    "            'Limitation: Ignores negative examples'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'Topic': 'Candidate-Elimination',\n",
    "        'Key Points': [\n",
    "            'Maintains version space (all consistent hypotheses)',\n",
    "            'S boundary: Most specific hypotheses',\n",
    "            'G boundary: Most general hypotheses',\n",
    "            'Uses both positive and negative examples'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'Topic': 'Inductive Bias',\n",
    "        'Key Points': [\n",
    "            'Assumptions needed for generalization',\n",
    "            'No bias = no generalization',\n",
    "            'Stronger bias = better generalization (if correct)',\n",
    "            'Trade-off: bias vs expressiveness'\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, concept in enumerate(concepts, 1):\n",
    "    print(f\"{i}. {concept['Topic']}\")\n",
    "    for point in concept['Key Points']:\n",
    "        print(f\"   ‚Ä¢ {point}\")\n",
    "    print()\n",
    "\n",
    "# Create a visual summary\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# 1. Well-Posed Problems\n",
    "ax = axes[0]\n",
    "components = ['Task\\n(T)', 'Performance\\n(P)', 'Experience\\n(E)']\n",
    "values = [1, 1, 1]\n",
    "colors_comp = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "ax.bar(components, values, color=colors_comp, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "ax.set_title('Well-Posed Learning Problem', fontweight='bold', fontsize=11)\n",
    "ax.set_ylim(0, 1.5)\n",
    "ax.set_yticks([])\n",
    "\n",
    "# 2. Design Choices\n",
    "ax = axes[1]\n",
    "choices = ['Training\\nExperience', 'Target\\nFunction', 'Representation', 'Learning\\nAlgorithm']\n",
    "ax.barh(choices, [1]*4, color='#9b59b6', alpha=0.7, edgecolor='black', linewidth=2)\n",
    "ax.set_title('Four Design Choices', fontweight='bold', fontsize=11)\n",
    "ax.set_xlim(0, 1.5)\n",
    "ax.set_xticks([])\n",
    "\n",
    "# 3. Hypothesis Space\n",
    "ax = axes[2]\n",
    "ax.text(0.5, 0.7, '‚ü®?, ?, ?, ?, ?, ?‚ü©', ha='center', fontsize=12, \n",
    "           bbox=dict(boxstyle='round', facecolor='#2ecc71', alpha=0.3))\n",
    "ax.text(0.5, 0.5, '‚Üì More Specific ‚Üì', ha='center', fontsize=10, style='italic')\n",
    "ax.text(0.5, 0.3, '‚ü®‚àÖ, ‚àÖ, ‚àÖ, ‚àÖ, ‚àÖ, ‚àÖ‚ü©', ha='center', fontsize=12,\n",
    "           bbox=dict(boxstyle='round', facecolor='#e74c3c', alpha=0.3))\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.axis('off')\n",
    "ax.set_title('General-to-Specific Ordering', fontweight='bold', fontsize=11)\n",
    "\n",
    "# 4. Find-S\n",
    "ax = axes[3]\n",
    "ax.text(0.5, 0.6, 'Find-S Algorithm', ha='center', fontsize=13, fontweight='bold')\n",
    "ax.text(0.5, 0.45, '‚úì Simple & Fast', ha='center', fontsize=10, color='green')\n",
    "ax.text(0.5, 0.35, '‚úì Finds max specific', ha='center', fontsize=10, color='green')\n",
    "ax.text(0.5, 0.25, '‚úó Ignores negatives', ha='center', fontsize=10, color='red')\n",
    "ax.text(0.5, 0.15, '‚úó Single hypothesis', ha='center', fontsize=10, color='red')\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.axis('off')\n",
    "\n",
    "# 5. Candidate-Elimination\n",
    "ax = axes[4]\n",
    "ax.text(0.5, 0.7, 'Version Space', ha='center', fontsize=13, fontweight='bold')\n",
    "ax.text(0.5, 0.55, 'G: {‚ü®?, ?, ...‚ü©}', ha='center', fontsize=10,\n",
    "           bbox=dict(boxstyle='round', facecolor='#e74c3c', alpha=0.3))\n",
    "ax.text(0.5, 0.4, '‚Üï', ha='center', fontsize=14)\n",
    "ax.text(0.5, 0.25, 'S: {‚ü®Sunny, Warm, ...‚ü©}', ha='center', fontsize=10,\n",
    "           bbox=dict(boxstyle='round', facecolor='#2ecc71', alpha=0.3))\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.axis('off')\n",
    "\n",
    "# 6. Inductive Bias\n",
    "ax = axes[5]\n",
    "bias_levels = ['No Bias', 'Weak Bias', 'Strong Bias']\n",
    "generalization = [0, 5, 9]\n",
    "colors_bias = ['#e74c3c', '#f39c12', '#2ecc71']\n",
    "ax.barh(bias_levels, generalization, color=colors_bias, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "ax.set_xlabel('Generalization Ability', fontsize=10)\n",
    "ax.set_title('Inductive Bias Trade-off', fontweight='bold', fontsize=11)\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéì CONGRATULATIONS! You've completed Module 1!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíº Real-World Applications\n",
    "\n",
    "### Where These Concepts Are Used Today\n",
    "\n",
    "1. **Email Spam Filtering**\n",
    "   - Gmail, Outlook use concept learning principles\n",
    "   - Learns patterns from labeled spam/not-spam examples\n",
    "   - Continuously updates as new spam tactics emerge\n",
    "\n",
    "2. **Medical Diagnosis Systems**\n",
    "   - Learn disease patterns from symptoms\n",
    "   - Version spaces help identify when more tests are needed\n",
    "   - Inductive bias based on medical knowledge\n",
    "\n",
    "3. **Fraud Detection**\n",
    "   - Credit card companies detect fraudulent transactions\n",
    "   - Learn from historical fraud patterns\n",
    "   - Balance false positives vs false negatives\n",
    "\n",
    "4. **Recommendation Systems**\n",
    "   - Netflix, Amazon learn user preferences\n",
    "   - Generalize from past behavior to new items\n",
    "   - Bias: Users with similar history have similar preferences\n",
    "\n",
    "5. **Quality Control in Manufacturing**\n",
    "   - Learn defect patterns from inspection data\n",
    "   - Automated visual inspection systems\n",
    "   - Reduce human error and inspection time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Additional Resources\n",
    "\n",
    "### Recommended Reading\n",
    "\n",
    "1. **Tom Mitchell - Machine Learning (Chapter 1-2)**\n",
    "   - The definitive textbook on concept learning\n",
    "   - Detailed mathematical proofs and analysis\n",
    "\n",
    "2. **Research Papers**\n",
    "   - Mitchell, T. (1982). \"Generalization as Search\" - Original version space paper\n",
    "   - Haussler, D. (1988). \"Quantifying Inductive Bias\" - Theoretical foundations\n",
    "\n",
    "3. **Online Resources**\n",
    "   - Coursera: Machine Learning by Andrew Ng\n",
    "   - MIT OpenCourseWare: Introduction to Machine Learning\n",
    "   - Scikit-learn documentation and tutorials\n",
    "\n",
    "### Practice Exercises\n",
    "\n",
    "1. **Implement Find-S for a new domain**\n",
    "   - Choose a different concept (e.g., \"good movies to watch\")\n",
    "   - Define attributes and collect training data\n",
    "   - Run Find-S and analyze results\n",
    "\n",
    "2. **Extend Candidate-Elimination**\n",
    "   - Add noise handling (contradictory examples)\n",
    "   - Implement confidence scoring for predictions\n",
    "   - Visualize the version space graphically\n",
    "\n",
    "3. **Compare Different Biases**\n",
    "   - Implement a disjunctive concept learner\n",
    "   - Compare with conjunctive (Candidate-Elimination)\n",
    "   - Analyze which works better for different problems\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "**Module 2: Decision Tree Learning**\n",
    "- Learn how to build decision trees\n",
    "- Understand entropy and information gain\n",
    "- Handle overfitting with pruning\n",
    "- Apply to real-world classification problems\n",
    "\n",
    "**Prepare by:**\n",
    "- Reviewing probability basics (conditional probability)\n",
    "- Understanding entropy and information theory\n",
    "- Practicing with tree-structured data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Final Reflection\n",
    "\n",
    "### Discussion Questions\n",
    "\n",
    "1. **Conceptual Understanding**\n",
    "   - Why is it impossible to learn without inductive bias?\n",
    "   - How would you explain version spaces to a non-technical person?\n",
    "   - What are the trade-offs between Find-S and Candidate-Elimination?\n",
    "\n",
    "2. **Practical Application**\n",
    "   - Think of a problem in your domain that could use concept learning\n",
    "   - What would be the attributes? The target concept?\n",
    "   - What inductive bias would be appropriate?\n",
    "\n",
    "3. **Critical Thinking**\n",
    "   - When would Candidate-Elimination fail or perform poorly?\n",
    "   - How could you modify these algorithms for continuous-valued attributes?\n",
    "   - What happens if the target concept is not in the hypothesis space?\n",
    "\n",
    "4. **Future Exploration**\n",
    "   - How do modern ML algorithms (neural networks) relate to these concepts?\n",
    "   - What role does inductive bias play in deep learning?\n",
    "   - How can we automatically learn the right bias for a problem?\n",
    "\n",
    "---\n",
    "\n",
    "## üôè Thank You!\n",
    "\n",
    "Thank you for completing Module 1! You now have a solid foundation in:\n",
    "- Defining well-posed learning problems\n",
    "- Designing learning systems\n",
    "- Understanding concept learning algorithms\n",
    "- Appreciating the role of inductive bias\n",
    "\n",
    "These fundamentals will serve you well as you progress through more advanced machine learning topics.\n",
    "\n",
    "**Keep Learning! Keep Building! Keep Innovating!** üöÄ\n",
    "\n",
    "---\n",
    "\n",
    "*Arivu AI Machine Learning Course - Module 1*  \n",
    "*Created with ‚ù§Ô∏è for aspiring ML practitioners*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

