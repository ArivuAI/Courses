{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "332c3937",
   "metadata": {},
   "source": [
    "# Ensemble Learning\n",
    "\n",
    "Ensemble learning is a machine learning technique that combines multiple individual models (called \"base learners\" or \"weak learners\") to create a stronger, more robust predictive model. The core idea is that by aggregating predictions from several models, we can achieve better performance than any single model alone.\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "**Base Learners**: Individual models that are combined in the ensemble. These can be different algorithms or the same algorithm trained on different subsets of data.\n",
    "\n",
    "**Aggregation Methods**: Techniques used to combine predictions from base learners, such as:\n",
    "- **Voting**: For classification (majority vote or weighted vote)\n",
    "- **Averaging**: For regression (simple or weighted average)\n",
    "- **Stacking**: Using a meta-model to learn how to combine predictions\n",
    "\n",
    "## Common Ensemble Methods\n",
    "\n",
    "### 1. Bagging (Bootstrap Aggregating)\n",
    "- Trains multiple models on different bootstrap samples of the training data\n",
    "- Reduces variance and helps prevent overfitting\n",
    "- **Example**: Random Forest\n",
    "\n",
    "### 2. Boosting\n",
    "- Trains models sequentially, where each model learns from the mistakes of previous ones\n",
    "- Focuses on reducing bias\n",
    "- **Examples**: AdaBoost, Gradient Boosting, XGBoost\n",
    "\n",
    "### 3. Stacking\n",
    "- Uses a meta-learner to combine predictions from multiple base models\n",
    "- Can capture complex relationships between base model predictions\n",
    "\n",
    "## Benefits\n",
    "\n",
    "- **Improved Accuracy**: Often achieves better performance than individual models\n",
    "- **Reduced Overfitting**: Especially with bagging methods\n",
    "- **Increased Robustness**: Less sensitive to outliers and noise\n",
    "- **Better Generalization**: Combines strengths of different algorithms\n",
    "\n",
    "## Applications\n",
    "\n",
    "Ensemble learning is widely used in:\n",
    "- Competitions (Kaggle, etc.)\n",
    "- Real-world production systems\n",
    "- Complex prediction tasks where high accuracy is crucial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46ee8c1",
   "metadata": {},
   "source": [
    "## Ensemble Learning Example\n",
    "\n",
    "Let's demonstrate ensemble learning with a practical example using the famous Iris dataset. We'll create multiple base learners and combine them to improve prediction accuracy.\n",
    "\n",
    "### Example: Predicting Iris Species\n",
    "\n",
    "\n",
    "### Key Concepts Explained\n",
    "\n",
    "**Diversity**: Each base learner uses a different approach:\n",
    "- **Random Forest**: Tree-based ensemble method\n",
    "- **Logistic Regression**: Linear probabilistic model\n",
    "- **SVM**: Margin-based classifier\n",
    "\n",
    "**Voting Strategy**: The `VotingClassifier` combines predictions using:\n",
    "- **Hard Voting**: Uses majority class prediction\n",
    "- **Soft Voting**: Uses averaged predicted probabilities (often better)\n",
    "\n",
    "**Why It Works**: Different models make different types of errors. By combining them, we can:\n",
    "- Reduce the impact of individual model weaknesses\n",
    "- Leverage the strengths of each approach\n",
    "- Achieve more stable and reliable predictions\n",
    "\n",
    "The ensemble typically outperforms individual models because it reduces both bias and variance in the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43975fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create individual base learners\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "lr_classifier = LogisticRegression(random_state=42)\n",
    "svm_classifier = SVC(probability=True, random_state=42)\n",
    "\n",
    "# Create an ensemble using Voting Classifier\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('random_forest', rf_classifier),\n",
    "        ('logistic_regression', lr_classifier),\n",
    "        ('svm', svm_classifier)\n",
    "    ],\n",
    "    voting='soft'  # Uses predicted probabilities\n",
    ")\n",
    "\n",
    "# Train individual models and ensemble\n",
    "models = {\n",
    "    'Random Forest': rf_classifier,\n",
    "    'Logistic Regression': lr_classifier,\n",
    "    'SVM': svm_classifier,\n",
    "    'Ensemble': ensemble\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(f\"{name} Accuracy: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29deec0",
   "metadata": {},
   "source": [
    "## Ensemble Aggregation Methods\n",
    "\n",
    "### Voting\n",
    "\n",
    "**Voting** is used for classification tasks and combines predictions from multiple classifiers.\n",
    "\n",
    "**Hard Voting (Majority Vote)**:\n",
    "- Each classifier makes a prediction (class label)\n",
    "- The final prediction is the class that receives the most votes\n",
    "- Example: If 3 classifiers predict [A, B, A], the final prediction is A\n",
    "\n",
    "**Soft Voting (Probability-based)**:\n",
    "- Each classifier outputs class probabilities\n",
    "- Probabilities are averaged across all classifiers\n",
    "- Final prediction is the class with highest average probability\n",
    "- Generally performs better than hard voting when classifiers can output probabilities\n",
    "\n",
    "### Averaging\n",
    "\n",
    "**Averaging** is used for regression tasks and combines numerical predictions.\n",
    "\n",
    "**Simple Averaging**:\n",
    "- Takes the arithmetic mean of all model predictions\n",
    "- Each model contributes equally to the final prediction\n",
    "- Formula: `prediction = (model1 + model2 + ... + modelN) / N`\n",
    "\n",
    "**Weighted Averaging**:\n",
    "- Assigns different weights to each model based on their performance\n",
    "- Better-performing models get higher weights\n",
    "- Formula: `prediction = (w1×model1 + w2×model2 + ... + wN×modelN) / (w1+w2+...+wN)`\n",
    "\n",
    "### Stacking (Stacked Generalization)\n",
    "\n",
    "**Stacking** uses a meta-learner to learn how to best combine base model predictions.\n",
    "\n",
    "**Process**:\n",
    "1. **Level 0**: Train multiple base models on the training data\n",
    "2. **Level 1**: Use base model predictions as features to train a meta-model\n",
    "3. **Prediction**: Base models make predictions, meta-model combines them\n",
    "\n",
    "**Advantages**:\n",
    "- Can learn complex, non-linear combinations of base predictions\n",
    "- Often achieves better performance than simple voting/averaging\n",
    "- Can handle different types of base models effectively\n",
    "\n",
    "**Example Structure**:\n",
    "- Base models: Random Forest, SVM, Neural Network\n",
    "- Meta-model: Logistic Regression or another algorithm\n",
    "- The meta-model learns which base model to trust more for different types of inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0bce1a",
   "metadata": {},
   "source": [
    "## Real-World Applications of Ensemble Learning\n",
    "\n",
    "### Netflix Movie Recommendations\n",
    "Netflix uses ensemble methods to combine multiple recommendation algorithms:\n",
    "- **Collaborative Filtering**: Recommends based on similar users' preferences\n",
    "- **Content-Based Filtering**: Recommends based on movie features (genre, actors, director)\n",
    "- **Matrix Factorization**: Discovers latent factors in user-movie interactions\n",
    "- **Final Ensemble**: Combines all approaches using weighted averaging to provide personalized recommendations\n",
    "\n",
    "### Kaggle Competition Winners\n",
    "Most Kaggle competition winners use ensemble strategies:\n",
    "- **House Prices Competition**: Winners typically combine XGBoost, LightGBM, and Neural Networks\n",
    "- **Titanic Survival**: Top solutions ensemble Decision Trees, Random Forest, and Logistic Regression\n",
    "- **Strategy**: Use stacking with 5-10 diverse base models and a simple meta-learner like Linear Regression\n",
    "\n",
    "### Medical Diagnosis Systems\n",
    "Ensemble learning improves diagnostic accuracy in healthcare:\n",
    "- **Cancer Detection**: Combines CNN for image analysis, Random Forest for clinical data, and SVM for genetic markers\n",
    "- **COVID-19 Screening**: Ensembles chest X-ray analysis, symptom checkers, and lab test results\n",
    "- **Benefits**: Reduces false positives/negatives by leveraging multiple data sources and algorithms\n",
    "\n",
    "### Financial Credit Scoring\n",
    "Banks use ensemble methods for loan approval decisions:\n",
    "- **Base Models**: \n",
    "    - Logistic Regression (interpretable baseline)\n",
    "    - Random Forest (handles non-linear relationships)\n",
    "    - Gradient Boosting (captures complex patterns)\n",
    "- **Combination**: Soft voting weighted by each model's historical performance\n",
    "- **Result**: More accurate risk assessment than any single model\n",
    "\n",
    "### Autonomous Vehicles\n",
    "Self-driving cars use ensemble learning for object detection:\n",
    "- **Camera-based CNN**: Detects objects from visual data\n",
    "- **LiDAR Random Forest**: Identifies objects using 3D point cloud data\n",
    "- **Radar SVM**: Detects objects in various weather conditions\n",
    "- **Sensor Fusion**: Ensemble combines all inputs for robust object recognition\n",
    "\n",
    "### Fraud Detection\n",
    "Credit card companies use ensemble methods to detect fraudulent transactions:\n",
    "- **Real-time Models**: Fast decision trees for immediate approval/decline\n",
    "- **Deep Analysis**: Neural networks for complex pattern recognition\n",
    "- **Rule-based Systems**: Expert-defined rules for known fraud patterns\n",
    "- **Ensemble Strategy**: Voting system that flags transactions when multiple models agree on suspicious activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1725fc29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
